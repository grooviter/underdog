{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Underdog","text":""},{"location":"#about","title":"About","text":"<p>Underdog is a set of Groovy libraries for data analysis.</p> <p>It doesn't expect to be the best data analysis tool, but it combines Groovy's expressiveness, and some of the best Java data analysis libraries to at least, make data analysis fun.</p>"},{"location":"#modules","title":"Modules","text":"<p>Underdog project covers several data analysis fields with the following subprojects:</p>"},{"location":"#underdog-dataframe","title":"underdog-dataframe","text":"<p>Combines tools for working with dataframes and series. You can use it adding the dependency:</p> GradleMavenGrapes <pre><code>implementation \"com.github.grooviter:underdog-dataframe:VERSION\"\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-dataframe&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grab(\"com.github.grooviter:underdog-dataframe:VERSION\")\n</code></pre> <p>More information in the DataFrame section</p>"},{"location":"#underdog-graphs","title":"underdog-graphs","text":"<p>It helps working on graph theory data structures and algorithms. You can use it adding the following dependency:</p> GradleMavenGrapes <pre><code>implementation \"com.github.grooviter:underdog-graphs:VERSION\"\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-graphs&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grab(\"com.github.grooviter:underdog-graphs:VERSION\")\n</code></pre> <p>More information in the Graphs section</p>"},{"location":"#underdog-ml","title":"underdog-ml","text":"<p>Contains machine learning algorithms and evaluation mechanisms. You can use it adding the following dependency:</p> GradleMavenGrapes <pre><code>implementation \"com.github.grooviter:underdog-ml:VERSION\"\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-ml&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grab(\"com.github.grooviter:underdog-ml:VERSION\")\n</code></pre> <p>More information in the ML section</p>"},{"location":"#underdog-plots","title":"underdog-plots","text":"<p>Creates different types of charts using the Apache Echarts library underneath. You can use it adding the following dependency:</p> GradleMavenGrapes <pre><code>implementation \"com.github.grooviter:underdog-plots:VERSION\"\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grab(\"com.github.grooviter:underdog-plots:VERSION\")\n</code></pre> <p>More information in the Plots section</p>"},{"location":"#underdog-ta","title":"underdog-ta","text":"<p>The technical analysis module is a wrapper over the Ta4j library. It adds some extension modules to the existent classes so that it makes easier to play with technical indicators and rules and integrate with Underdog's dataframes and series. You can use it adding the following dependency:</p> GradleMavenGrapes <pre><code>implementation \"com.github.grooviter:underdog-ta:VERSION\"\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-ta&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grab(\"com.github.grooviter:underdog-ta:VERSION\")\n</code></pre> <p>More information in the Technical Analysis section</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/12/20/classifying-food/","title":"Classifying food","text":"<p>In this article I\u2019m using Supervised Learning Classification to classify food into three categories: green, orange or red depending on whether they\u2019re healthy or not. </p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/classifying-food/#prerequisites","title":"Prerequisites","text":"<p>For this entry you should need the following dependencies:</p> GradleMavenGrapes <pre><code>implementation \"com.github.grooviter:underdog-ml:VERSION\"\nimplementation \"com.github.grooviter:underdog-plots:VERSION\"\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-ml&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grapes([\n    @Grab(\"com.github.grooviter:underdog-ml:VERSION\"),\n    @Grab(\"com.github.grooviter:underdog-plots:VERSION\")\n]) \n</code></pre> <p>Note</p> <p>ml and plots modules already have underdog-dataframe dependency as transitive dependency so you don't have to explicitly declare it.</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/classifying-food/#representation-phase","title":"Representation Phase","text":"<p>In the representation phase we should find a representative dataset and a suitable algorithm for the problem at hand. Then we use both, dataset and algorithm, to train a software model to make predictions.</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/classifying-food/#dataset-preparation","title":"Dataset preparation","text":"<p>A representative dataset, in this case, would be a large dataset of food, labeled as good/bad/not-so-good food. I created a csv file with more than three thousand food entries, collected from this site (Spanish).</p> loading data<pre><code>import underdog.Underdog\nimport underdog.plots.Plots\n\n\ndef food = Underdog.df()\n    .read_csv(\"src/test/resources/data/blog/2024/12/classifying_food.csv\", sep: ';')\n    .fillna(0)\n\nprintln(food.head())\n</code></pre> output<pre><code>  ID   |               NAME                |  BRAND ID  |    BRAND     |  GROUP ID  |        GROUP NAME        |  SUBGROUP ID  |     SUBGROUP NAME      |  SPECIAL  |  TRAFFICLIGHT VALUE  |  PYRAMID VALUE  |  CARBS  |  SUGAR  |  ENERGY  |  PROTEINS  |  SATURATED FAT  |  FAT  |  SALT  |  SODIUM  |  FIBER  |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  891  |       Aceitunas negras sin hueso  |        76  |      Ybarra  |         8  |                  Frutas  |           24  |            Procesados  |       No  |                   3  |             2A  |      0  |      0  |     129  |       0.5  |            2.2  |   13  |   1.5  |     0.6  |      0  |\n  848  |                          Acelgas  |        78  |  Verdifresh  |         9  |   Verduras y hortalizas  |           24  |            Procesados  |       No  |                   1  |             2A  |    3.7  |      0  |      21  |      1.68  |              0  |  0.2  |  0.53  |       0  |    1.6  |\n 1118  |                          Acelgas  |        70  |    Florette  |         9  |   Verduras y hortalizas  |           24  |            Procesados  |       No  |                   1  |             2A  |    2.1  |    1.1  |      21  |       1.8  |              0  |  0.2  |  0.53  |       0  |    1.6  |\n 2856  |       Acelgas primera al natural  |       150  |     Helio's  |         9  |   Verduras y hortalizas  |           12  |              Conserva  |       No  |                   1  |             2A  |    4.5  |    3.5  |      25  |       1.2  |              0  |    0  |   0.2  |    0.08  |      1  |\n 2903  |  Acelgas troceadas calidad extra  |       130  |     Gvtarra  |         9  |   Verduras y hortalizas  |           12  |              Conserva  |       No  |                   3  |             2A  |    1.2  |      0  |      12  |       0.9  |              0  |    0  |  0.88  |   0.352  |    1.7  |\n 2911  |    Acelgas, patatas y zanahorias  |       130  |     Gvtarra  |         9  |   Verduras y hortalizas  |           12  |              Conserva  |       No  |                   2  |             2A  |    3.6  |    0.5  |      20  |       0.7  |              0  |    0  |   0.7  |    0.28  |    1.3  |\n 2715  |                    Agua de lim\u00f3n  |       175  |  Font Vella  |         5  |  Bebidas no alcoh\u00f3licas  |            6  |  Bebidas refrescantes  |       No  |                   3  |              0  |    4.2  |    4.2  |      17  |         0  |              0  |    0  |  0.02  |   0.008  |      0  |\n 2713  |                  Agua de manzana  |       175  |  Font Vella  |         5  |  Bebidas no alcoh\u00f3licas  |            6  |  Bebidas refrescantes  |       No  |                   3  |              0  |    4.1  |    4.1  |      17  |         0  |              0  |    0  |  0.01  |   0.004  |      0  |\n 2718  |                  Agua de naranja  |       175  |  Font Vella  |         5  |  Bebidas no alcoh\u00f3licas  |            6  |  Bebidas refrescantes  |       No  |                   3  |              0  |    4.2  |    4.2  |      19  |         0  |              0  |    0  |  0.01  |   0.004  |      0  |\n 2719  |                     Agua de pi\u00f1a  |       175  |  Font Vella  |         5  |  Bebidas no alcoh\u00f3licas  |            6  |  Bebidas refrescantes  |       No  |                   3  |              0  |    4.2  |    4.2  |      17  |         0  |              0  |    0  |  0.01  |   0.004  |      0  |\n</code></pre> <p>Each entry has a series of possible features and it\u2019s labeled with a color value (TRAFFICLIGH VALUE) which depends on whether it is a healthy food, not so healthy food or junk food. Next I should choose every column that can be eligible as a feature, for example the name of the food is not a good feature if you\u2019d like to generalize the results.</p> choosing possible features<pre><code>def COLUMNS_OF_INTEREST = [\n    \"TRAFFICLIGHT VALUE\",\n    \"CARBS\",\n    \"SUGAR\",\n    \"ENERGY\",\n    \"PROTEINS\",\n    \"SATURATED FAT\",\n    \"FAT\",\n    \"SODIUM\",\n    \"FIBER\",\n    \"SALT\",\n]\n\ndef df = food.copy()\n    .loc[__, COLUMNS_OF_INTEREST]\n    .dropna()\n\nprintln(df.head())\n</code></pre> output<pre><code> TRAFFICLIGHT VALUE  |  CARBS  |  SUGAR  |  ENERGY  |  PROTEINS  |  SATURATED FAT  |  FAT  |  SODIUM  |  FIBER  |  SALT  |\n--------------------------------------------------------------------------------------------------------------------------\n                  3  |      0  |      0  |     129  |       0.5  |            2.2  |   13  |     0.6  |      0  |   1.5  |\n                  1  |    3.7  |      0  |      21  |      1.68  |              0  |  0.2  |       0  |    1.6  |  0.53  |\n                  1  |    2.1  |    1.1  |      21  |       1.8  |              0  |  0.2  |       0  |    1.6  |  0.53  |\n                  1  |    4.5  |    3.5  |      25  |       1.2  |              0  |    0  |    0.08  |      1  |   0.2  |\n                  3  |    1.2  |      0  |      12  |       0.9  |              0  |    0  |   0.352  |    1.7  |  0.88  |\n                  2  |    3.6  |    0.5  |      20  |       0.7  |              0  |    0  |    0.28  |    1.3  |   0.7  |\n                  3  |    4.2  |    4.2  |      17  |         0  |              0  |    0  |   0.008  |      0  |  0.02  |\n                  3  |    4.1  |    4.1  |      17  |         0  |              0  |    0  |   0.004  |      0  |  0.01  |\n                  3  |    4.2  |    4.2  |      19  |         0  |              0  |    0  |   0.004  |      0  |  0.01  |\n                  3  |    4.2  |    4.2  |      17  |         0  |              0  |    0  |   0.004  |      0  |  0.01  |\n</code></pre> <p>However the goal is to choose the minimum set of features that maximizes the classification. Too many could classify well but it would become too hard to use, too few would not classify well enough. I need to find the balance between the two. Once I\u2019ve found the balance I can use both, features and labels to create a training and test datasets. For that I use the trainTestSplit function.</p> minimum set of features and creating training and test datasets<pre><code>def feats = df['CARBS', 'SUGAR', 'PROTEINS', 'FAT', 'SALT', 'FIBER'] as double[][]\ndef label = df['TRAFFICLIGHT VALUE'] as int[]\n\ndef (X_train, X_test, y_train, y_test) = Underdog.ml()\n    .utils\n    .trainTestSplit(feats, label, random_state: 0)\n</code></pre> <p>Drawing a scatter matrix sometimes could help you to spot features that are particularly good on classifying samples. I did the matrix, but I recognize that although some of the feature-pairs are clearly better than others (e.g. proteins/carbs) many of them are inconclusive to me (e.g. salt/fat). I invite you to open the plot in a new window, full size, and take a look for yourself:</p> scatter matrix<pre><code>def plot = Underdog.plots()\n    .scatterMatrix(\n        X_train,\n        labels: [\"CARBS\", \"SUGAR\", \"PROTEINS\", \"FAT\", \"SALT\", \"FIBER\"],\n        histogramBins: 15)\n\nplot.show()\n</code></pre>","tags":["ml","classification"]},{"location":"blog/2024/12/20/classifying-food/#algorithm-selection","title":"Algorithm selection","text":"<p>In order to choose the algorithm, I needed to identify first the type of problem I was facing. Why I though this would fit as a Supervised Learning Classification Problem ?</p> <ul> <li>First, I\u2019ve got a labeled dataset, so it looked like I could use the labeled data to train a supervised learning model. </li> <li>Second, I was looking for different types of discrete target values (values for green, orange, red), therefore it seemed to be a classification problem.</li> </ul> <p>Once I confirmed it was a classification problem I picked the k-nearest neighbors algorithm.</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/classifying-food/#evaluation-phase","title":"Evaluation Phase","text":"<p>Then we use both, dataset and algorithm, to train a software model to make predictions. Afterwards the model performance is evaluated with testing datasets. Training and testing are part of the evaluation phase.</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/classifying-food/#model-creation-training","title":"Model creation &amp; training","text":"<p>The k-nearest neighbors algorithm tries to establish to which type the element belongs by checking the closest neighborg elements around. You can customize the K parameter which sets how many neighbors does the algorithm have to check before emmiting its veredict.</p> <p>Here I\u2019m initializing the algorithm with k=5. Then I\u2019m training the model using the fit function and finally I\u2019m checking how well the model is going to perform by passing the testing dataset (X_test, y_test) to the score function. I was able to get more than 80% of accuracy by using 6 features.</p> model training and getting accuracy score with the testing dataset<pre><code>def ml = Underdog.ml()\n\n// creates and trains the model\ndef knn = ml.classification.knn(X_train, y_train, k: 5)\n\n// creating predictions with the test feature set\ndef predictions = knn.predict(X_test)\n\n// getting the accuracy of the model when tested against the test set\ndef score = ml.metrics.accuracy(y_test, predictions)\n</code></pre> accuracy check<pre><code>assert score &gt; 0.80\n</code></pre>","tags":["ml","classification"]},{"location":"blog/2024/12/20/classifying-food/#model-testing","title":"Model testing","text":"<p>To get a prediction I need to provide the following measurements to the model:</p> <ul> <li>CARBS </li> <li>SUGAR </li> <li>PROTEINS </li> <li>FAT </li> <li>SALT </li> <li>FIBER</li> </ul> <p>I\u2019m passing as many samples as I want to the KNeighborsClassifier\u2019s predict function and for every sample I\u2019m getting a prediction. Every sample is passed as an array with the required feature values. In this case I\u2019m using another online supermarket dataset to test the model with other datasets than the training and testing datasets.</p> prediction<pre><code>// def sample      = [CARBS, SUGAR, PROTEINS, FAT, SALT, FIBER]\ndef tuna_olive_oil = [0, 0, 20, 33, 0.88, 0]    // expected 3\ndef beer_one_liter = [3.4, 0.1, 0.3, 0, 0, 0]   // expected 2\ndef coke           = [10.6, 10.6, 0, 0, 0, 0]   // expected 3\ndef croissants     = [46, 4.5, 8.7, 26, 1.3, 0] // expected 3\n\ndef sample_predictions = knn.predict([\n    tuna_olive_oil,\n    beer_one_liter,\n    coke,\n    croissants] as double[][])\n</code></pre> <p>With the expected output of:</p> sample predictions<pre><code>sample_predictions == [3, 2, 3, 3]\n</code></pre> <p>Which matches the initial expectations over these samples.</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/classifying-food/#optimization","title":"Optimization","text":"<p>The optimization was completely hand-crafted. But I can comment on two tools that I think helped trying to optimize the whole process:</p> <ul> <li>Scatter matrix. It helped me to see some features that I though for sure they were not going to work well. </li> <li>Model score: I used it as a brute force mechanism. I chose the optimal set of features by running the model score until I got what I though it was a compromised between a high score and a reasonable number of features.</li> </ul>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/","title":"Linear regression notes","text":"<p>Classification is a great method to predict discrete values from a given dataset, but sometimes you need to predict a continuous value, e.g: height, weight, prices... And that\u2019s when linear regression techniques come handy.</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/#what-is-linear-regression","title":"What is linear regression ?","text":"<p>The definition that I read in the Wikipedia didn\u2019t help me at all. Instead when I related it with a line, it started to make sense to me. If we\u2019ve got a linear function, that is, a function describing a line, where \u0175 is the slope of the line and b is called the intercept which is a constant value:</p> <p>For every x value a new point will be drawn and eventually altogether will form a line. So, if you think about it visually, given a set of input values, a simple linear regression algorithm will try to come up with a line trying to pass as close as possible to the majority of the input dataset points. So if you try to predict an output value from the input values, the machine learning process will pick up a value from that line.</p> <p>There are differences between the types of linear regression techniques depending on the presence of regularization (Ridge and Lasso), or the lack of it (Simple Linear Regression). It's also worth mentioning the use of normalization, feature generation (polynomial transformation) and feature compression (PCA).</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/#prerequisites","title":"Prerequisites","text":"<p>For this entry you should need the following dependencies:</p> GradleMavenGrapes <pre><code>implementation \"com.github.grooviter:underdog-ml:VERSION\"\nimplementation \"com.github.grooviter:underdog-plots:VERSION\"\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-ml&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grapes([\n@Grab(\"com.github.grooviter:underdog-ml:VERSION\"),\n@Grab(\"com.github.grooviter:underdog-plots:VERSION\")\n])\n</code></pre> <p>Note</p> <p>ml and plots modules already have underdog-dataframe dependency as transitive dependency so you don't have to explicitly declare it.</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/#simple-linear-regression","title":"Simple linear regression","text":"<p>The most popular linear regression uses the least squares technique. It tries to find a slope (w) and constant value (b) that minimizes the mean squared error of the model. It doesn\u2019t have parameters to control model complexity, everything it needs is estimated from training data.</p> <p>UC Irvine Dataset</p> <p>The dataset used for this entry is a Bike sharing  dataset from the UCI Dataset repository for machine learning. </p> <p>First of all I'm loading the Bike sharing daily dataset. We are removing non-numerical series and rows with missing values:</p> loading data<pre><code>import underdog.Underdog\n\ndef data = Underdog.df()\n    .read_csv(FILE_PATH, dateFormat: 'yyyy-MM-dd')\n    .dropna()  // removing missing values\n    .sort_values(by: ['dteday'])\n    .drop('instant', 'yr', 'casual', 'cnt', 'dteday') // removing some columns\n</code></pre> <p>Which outputs:</p> <pre><code>                                            day.csv\n season  |  mnth  |  holiday  |  weekday  |  workingday  |  weathersit  |    temp    |   atemp    |    hum     |  windspeed  |  registered  |\n---------------------------------------------------------------------------------------------------------------------------------------------\n      1  |     1  |        0  |        6  |           0  |           2  |  0.344167  |  0.363625  |  0.805833  |   0.160446  |         654  |\n      1  |     1  |        0  |        0  |           0  |           2  |  0.363478  |  0.353739  |  0.696087  |   0.248539  |         670  |\n      1  |     1  |        0  |        1  |           1  |           1  |  0.196364  |  0.189405  |  0.437273  |   0.248309  |        1229  |\n      1  |     1  |        0  |        2  |           1  |           1  |       0.2  |  0.212122  |  0.590435  |   0.160296  |        1454  |\n      1  |     1  |        0  |        3  |           1  |           1  |  0.226957  |   0.22927  |  0.436957  |     0.1869  |        1518  |\n</code></pre> <p>First, I\u2019d like to see how features could be related to each other using a correlation matrix:</p> correlation matrix<pre><code>def plot = Underdog\n    .plots()\n    .correlationMatrix(df)\n\nplot.show()\n</code></pre> <p>There are a lot of features, but I\u2019m focusing on just choosing one, temp which is the normalized temperature in Celsius  the day of the rental. I\u2019d like to see how it looks like visually the relationship between registered number of rentals  (registered variable) and the temperature feature I\u2019ve chosen:</p> scatter matrix: temp vs registered<pre><code>def plot = Underdog\n    .plots()\n    .scatterMatrix(df['temp', 'registered'])\n\nplot.show()\n</code></pre> <p>What I\u2019m looking for at this point in the scatter plot, is tendencies. In this case it seems that points tend to go in diagonal from the bottom left to the upper right part of the graph. So far, the more tendency I see the better it seems to work. Now I'm creating a linear regression model using the Ordinary least square algorithm (OLS):</p> linear regression<pre><code>// features (X) and labels (y)\ndef X = df['temp'] as double[][]\ndef y = df['registered'] as double[]\n\ndef ml = Underdog.ml()\n\n// train test split (0.75 training, 0.25 test)\ndef (\n    xTrain,\n    xTest,\n    yTrain,\n    yTest\n) = ml.utils.trainTestSplit(X, y)\n\n// model creation and training\ndef model = ml.regression.ols(xTrain, yTrain)\n\n// predicting and getting r2_score for training and test sets\ndef scoreTrain = model.score(xTrain, yTrain).round(6)\ndef scoreTest = model.score(xTest, yTest).round(6)\n\nprint(\"train: ${scoreTrain}, test: ${scoreTest}\")\n</code></pre> output<pre><code>train: 0.307325, test: 0.223381\n</code></pre> <p>If we draw the regression line we\u2019ve got:</p> scatter &amp; line plot<pre><code>def plt = Options.create {\n    title {\n        text('Linear Regression (Least Squares - No Polynomial)')\n        left('center')\n        top('5%')\n    }\n    xAxis {\n        name('temp')\n        nameGap(25)\n        nameLocation('center')\n        show(true)\n    }\n    yAxis {\n        name('registered')\n        nameGap(50)\n        nameLocation('center')\n        show(true)\n    }\n    // SCATTER PLOT\n    series(ScatterSeries){\n        data(\n            toData(\n                xTest, // x1, x2,...xn\n                yTest  // y1, y2,...yn\n            ) // [[x1,y1], [x2,y2],...[xn, yn]]\n        )\n    }\n    // REGRESSION LINE\n    series(LineSeries) {\n        data(\n            toData(\n                xTest,\n                model.predict(xTest)\n            )\n        )\n    }\n}\n\nplt.show()\n</code></pre> <p>A straight line won\u2019t be able to do good predictions. A way of helping the linear transformation to adapt  better to the shape of the model is to use a polynomial transformation.</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/#polynomial-transformation","title":"Polynomial transformation","text":"<p>When the problem doesn't fit easily a straight line or there are many features, it could become complicated to find a  good relationship between them, specially with a simple line. The polynomial transformation helps finding those  relationships. Applying a polynomial transformation to our problem can help the linear regression to adapt better to the  shape of the data. This is the same linear regression example, but this time applying the polynomialFeatures function prior  to the linear regression fit.</p> applying polynomial transformation<pre><code>// transforming X adding new generated features\ndef xPoly = ml.features.polynomialFeatures(X)\n\n// train test split (more data for training)\ndef (\n    xTrain,\n    xTest,\n    yTrain,\n    yTest\n) = ml.utils.trainTestSplit(xPoly, y)\n\n// creating and training model\ndef model = ml.regression.ols(xTrain, yTrain)\n\n// predicting and getting r2_score for training and test sets\ndef scoreTrain = model.score(xTrain, yTrain).round(6)\ndef scoreTest = model.score(xTest, yTest).round(6)\n\nprint(\"train: ${scoreTrain}, test: ${scoreTest}\")\n</code></pre> output<pre><code>train: 0.355879, test: 0.296751\n</code></pre> <p>Because the polynomial transformation is creating more features, they cover a wider spectrum of the data, therefore more  likely to do improve accuracy, at least in the training dataset. If we draw now the result:</p> linear regression polynomial plot<pre><code>// building chart\ndef plt = Options.create {\n    title {\n        text('Linear Regression (Least Squares - Polynomial)')\n        left('center')\n        top('5%')\n    }\n    xAxis {\n        name('temp')\n        nameGap(25)\n        nameLocation('center')\n        show(true)\n    }\n    yAxis {\n        name('registered')\n        nameGap(50)\n        nameLocation('center')\n        show(true)\n    }\n    // SCATTER PLOT\n    series(ScatterSeries){\n        data(toData(X, y))\n    }\n    // REGRESSION LINES\n    def colors = ['gray', 'green', 'brown'].indexed()\n\n    (0..&lt;xTest.shape().cols).each {feature -&gt;\n        series(LineSeries) {\n            smooth(true)\n            itemStyle { color(colors[feature]) }\n            data(\n                toData(\n                    xTest.collect { it[feature] },\n                    model.predict(xTest)\n                )\n            )\n        }\n\n    }\n}\n\n// showing chart\nplt.show()\n</code></pre> <p>Which covers much more than the previous example. However there are a couple of things to keep in mind when applying  the polynomial transformation:</p> <ul> <li>Polynomial transformation with a high degree value could overfit the model</li> <li>It\u2019s better to combine it with a regularized regression method like Ridge.</li> </ul> <p>However so far it's clear that with just one feature we don't go anywhere as the models we've got so far barely work for training set and are useless for test sets. In regularization and normalization we will be using more features to try to create a viable model.</p> <p>References</p> <ul> <li>Polynomial interpolation</li> <li>PolynomialFeatures in scikit-learn</li> <li>Understanding Polynomial Features (Medium)</li> </ul>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/#feature-selection","title":"Feature selection","text":"<p>So far I\u2019ve been working with just one feature temp to predict a possible outcome. I chose this feature by using the correlation table as a guide. When looking for just one variable to work with, it could be enough, but when looking for many possible features it could be cumbersome. </p> <p>Lasso regression could be one method for telling me which features do perform and which don\u2019t. How ? Well according to how the L1 regulation method works, keeping it short, those features that are not so important, Lasso makes its coefficient equal to 0, therefore, those features having a coefficient greater than 0 are worth using them to train the model (the higher the better). Lets use this knowledge to know which features could be useful to train the model.</p> <p>Lasso to get feature coefficients<pre><code>import underdog.Underdog\n\nimport static groovy.json.JsonOutput.toJson\nimport static groovy.json.JsonOutput.prettyPrint\n\n// taking all available features but the one for labelling\ndef featureNames = df.columns - \"registered\"\n\ndef X = df[featureNames] as double[][]\ndef y = df['registered'] as double[]\n\n// creating and training model\ndef model = Underdog.ml().regression.lasso(X, y)\n\n// extracting coefficients\ndef coefficients = model.coefficients()\ndef featNamesAndCoefficients = [featureNames, coefficients].transpose().collectEntries()\n\nprintln(prettyPrint(toJson(featNamesAndCoefficients)))\n</code></pre> Which shows the following map:</p> <p>features along with their coefficients<pre><code>{\n    \"season\": 424.11017152754937,\n    \"mnth\": -14.868338993584615,\n    \"holiday\": -211.17143310755654,\n    \"weekday\": 36.06114879750305,\n    \"workingday\": 941.7383145447468,\n    \"weathersit\": -397.9241830877648,\n    \"temp\": 1136.7307642112696,\n    \"atemp\": 2730.771127273043,\n    \"hum\": -1670.0999227731259,\n    \"windspeed\": -2200.526972819417\n}\n</code></pre> Now as the theory stated, we can discard those features with 0 value, and maybe those which are negatively correlated. For this example, where I\u2019m only interested in one feature to validate whether I chose the most significant feature or not. In this case I\u2019m getting the feature with the highest possitive coefficient:</p> <pre><code>def bestFeatures = featNamesAndCoefficients\n    .findAll { it.value &gt; 0 } // filtering all features w/ coefficient &gt; 0\n    .sort { -it.value } // sorting by coefficient (desc)\n    *.key as List&lt;String&gt; // getting feature names\n\nprintln(bestFeatures)\n</code></pre> output<pre><code>['atemp', 'temp', 'workingday', 'season', 'weekday']\n</code></pre>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/#regularization-and-normalization","title":"Regularization and normalization","text":"<p>Regularization is a technique used to reduce the model complexity and thus it helps dealing with over-fitting:</p> <ul> <li>It reduces the model size by shrinking the number of parameters the model has to learn </li> <li>It adds weight to the values so that it tries to favor smaller values</li> </ul> <p>Regularization penalizes certain values by using a loss function with a cost. This cost could be of type:</p> <ul> <li>L1: The cost is proportional to the absolute value of the weight coefficients (Lasso)</li> <li>L2: The cost is proportional to the square of the value of the weight coefficients (Ridge)</li> </ul> <p>Tip</p> <p>Regularization really shines when there is a high dimensionality, meaning there\u2019re multiple features. So in these  examples it won\u2019t make a huge impact with the scores.</p> <p>Data normalization is the process of rescaling one or more features to a common scale. It\u2019s normally used when features used to create the model have different scales. There are a few advantages of using normalization is such scenario:</p> <ul> <li>It could improve the numerical stability of your model </li> <li>It could speed up the training process</li> </ul> <p>Normalization is specially important when applying certain regression techniques, as regression is sensitive to  model feature adjustments.</p> <p>Tip</p> <p>When using only using ONE feature, normalization doesn't make much difference but, when  using multiple features, and each of them in different scales, then we should use normalization.</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/#regularization-baseline","title":"Regularization Baseline","text":"<p>Lets do the simple linear regression again with the best features obtained from lasso regression coefficients to set the baseline for the regularization &amp; normalization examples:</p> Baseline<pre><code>def X = df['atemp', 'temp', 'workingday', 'season', 'weekday'] as double[][]\ndef y = df['registered'] as double[]\ndef ml = Underdog.ml()\n\n// train test split\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\n\n// creating and training model\ndef model = ml.regression.ols(xTrain, yTrain)\n\n// getting scores\ndef scoreTrain = model.score(xTrain, yTrain).round(6)\ndef scoreTest = model.score(xTest, yTest).round(6)\n\nprint(\"train: ${scoreTrain}, test: ${scoreTest}\")\n</code></pre> <p>Obtaining the scores:</p> baseline scores<pre><code>train: 0.460528, test: 0.311946\n</code></pre>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/#ridge","title":"Ridge","text":"<ul> <li>Follows the least-squares criterion but it uses regularization as a penalty for large variations in w parameters. </li> <li>Regularization prevents over-fitting by restricting the model, it normally reduces its complexity </li> <li>Regularization is controlled by the alpha parameter </li> <li>The higher the value of alpha the simpler the model, that is, the model is less likely to over-fit</li> </ul> <p>Now I\u2019m using Ridge class with the same dataset:</p> using Ridge regression<pre><code>def X = df['atemp', 'temp', 'workingday', 'season', 'weekday'] as double[][]\ndef y = df['registered'] as double[]\ndef ml = Underdog.ml()\n\n// train test split (more data for training)\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\n\n// creating and training model (RIDGE)\ndef model = ml.regression.ridge(xTrain, yTrain, alpha: 40)\n\n// predicting and getting r2_score for training and test sets\ndef scoreTrain = model.score(xTrain, yTrain).round(6)\ndef scoreTest = model.score(xTest, yTest).round(6)\n\nprint(\"train: ${scoreTrain}, test: ${scoreTest}\")\n</code></pre> <p>Giving me the following scores:</p> output<pre><code>train: 0.459691, test: 0.321293\n</code></pre> <p>It looks better than the simple OLS example, the takeaway idea here is that the Ridge regression along with a  high value of alpha is going to reduce the complexity of the model and make the generalization better (or at least more stable).</p> <p>Ridge regression score can be improved by applying normalization to the source dataset. Is important for some ML methods  that all features are on the same scale. In this case we\u2019re applying a MinMax normalization. However there\u2019re some  basic tips to be aware of:</p> <ul> <li>Fit the scaler with the training set and then apply the same scaler to transform the training and test feature sets</li> <li>Don\u2019t use the test dataset to fit the scaler. That could lead to data leakage.</li> </ul> Ridge with scaled set<pre><code>def (\n    xTrain,\n    xTest,\n    yTrain,\n    yTest\n) = ml.utils.trainTestSplit(X, y)\n\n// NORMALIZATION: training scaler with training set\ndef minMaxTransformation = ml.features.minMaxScaler(xTrain)\n\n// NORMALIZATION: transforming training set\ndef xTrainScaled = minMaxTransformation.apply(xTrain)\n\n// NORMALIZATION: transforming testing set\ndef xTestScaled = minMaxTransformation.apply(xTest)\n\n// creating and training model\ndef model = ml.regression.ridge(xTrainScaled, yTrain, alpha: 40)\n\n// predicting and getting r2_score for training and test sets\ndef scoreTrain = model.score(xTrainScaled, yTrain).round(6)\ndef scoreTest = model.score(xTestScaled, yTest).round(6)\n\nprint(\"train: ${scoreTrain}, test: ${scoreTest}\")\n</code></pre> output<pre><code>train: 0.459691, test: 0.321293\n</code></pre> <p>Well it didn't change a bit. I'm not sure whether is because the model is not that complex or the features don't change much and the regularization + normalization doesn't add much either.</p>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/#lasso","title":"Lasso","text":"<ul> <li>It uses a L1 type regularization penalty, meaning it minimizes the sum of the absolute values of the coefficients</li> <li>It works as a kind of feature selection</li> <li>It also has an alpha parameter to control regularization</li> </ul> using lasso regression<pre><code>def model = ml.regression.lasso(xTrain, yTrain, alpha: 40)\n\n// predicting and getting r2_score for training and test sets\ndef scoreTrain = model.score(xTrain, yTrain).round(6)\ndef scoreTest = model.score(xTest, yTest).round(6)\n\nprint(\"train: ${scoreTrain}, test: ${scoreTest}\")\n</code></pre> output<pre><code>train: 0.460528, test: 0.31195\n</code></pre> <p>And finally using min-max scaler to try to improve regression scoring:</p> lasso with scaled features<pre><code>// train test split\ndef (\n        xTrain,\n        xTest,\n        yTrain,\n        yTest\n) = ml.utils.trainTestSplit(X, y)\n\n// normalization\ndef minMaxTransformation = ml.features.minMaxScaler(xTrain)\ndef xTrainScaled = minMaxTransformation.apply(xTrain)\ndef xTestScaled = minMaxTransformation.apply(xTest)\n\n// creating and training model\ndef model = ml.regression.lasso(xTrainScaled, yTrain, alpha: 40)\n\ndef scoreTrain = model.score(xTrainScaled, yTrain).round(6)\ndef scoreTest = model.score(xTestScaled, yTest).round(6)\n\nprint(\"train: ${scoreTrain}, test: ${scoreTest}\")\n</code></pre> output<pre><code>train: 0.460528, test: 0.311954&gt;\n</code></pre> <p>Unfortunately this didn't improve the result either.</p> <p>Ridge vs Lasso</p> <p>In this case we\u2019ve used both algorithms with the same dataset, but there\u2019re situations where one or the other fit best:</p> <ul> <li>Ridge: Many small/medium sized effects</li> <li>Lasso: Few medium/large sized effects</li> </ul>","tags":["ml","classification"]},{"location":"blog/2024/12/20/linear-regression-notes/#pca","title":"PCA","text":"<p>Principal component analysis (PCA) is an orthogonal linear transformation that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables called principal components. Long story short tries to do the same model with less features involved doing a type of data compression. In this example we are trying to obtain similar results with one feature less using PCA:</p> pca<pre><code>// train test split\ndef (\n    xTrain,\n    xTest,\n    yTrain,\n    yTest\n) = ml.utils.trainTestSplit(X, y)\n\n// normalization\ndef normalization = ml.features.standardizeScaler(xTrain)\n//      |\n//      v\ndef xTrainScaled = normalization.apply(xTrain)\ndef xTestScaled = normalization.apply(xTest)\n\n// compression\ndef compression = ml.features.pca(xTrainScaled, 4)\n//      |\n//      v\ndef xTrainScaledReduced = compression.apply(xTrainScaled)\ndef xTestScaledReduced = compression.apply(xTestScaled)\n\n// creating and training model\ndef model = ml.regression.ols(xTrainScaledReduced, yTrain)\n\n// predicting and getting r2_score for training and test sets\ndef scoreTrain = model.score(xTrainScaledReduced, yTrain).round(6)\ndef scoreTest = model.score(xTestScaledReduced, yTest).round(6)\n\nprint(\"train: ${scoreTrain}, test: ${scoreTest}\")\n</code></pre> <p>Which shows pretty the same results but reducing the data needed to train the data, which, may not be relevant in this example but it could be a huge amount of data for some models.</p> output<pre><code>train: 0.460506, test: 0.311325\n</code></pre>","tags":["ml","classification"]},{"location":"dataframe/","title":"DataFrame","text":"<p>Underdog's DataFrame combines tools for working with columnar data as tables (dataframes) and columns (series). And also has extra features such statistical functions and visualizations via Underdog's plots module.</p>"},{"location":"dataframe/#tutorial","title":"Tutorial","text":""},{"location":"dataframe/#prerequisites","title":"Prerequisites","text":""},{"location":"dataframe/#dependencies","title":"Dependencies","text":"<p>To be able to follow the tutorial you need the <code>underdog-dataframe</code> module. If you're using Gradle in your project:</p> GradleMavenGrapes <pre><code>implementation \"com.github.grooviter:underdog-dataframe:VERSION\"\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-dataframe&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grab(\"com.github.grooviter:underdog-dataframe:VERSION\")\n</code></pre>"},{"location":"dataframe/#data","title":"Data","text":"<p>You can find the data used in this tutorial here </p>"},{"location":"dataframe/#loading-data","title":"Loading data","text":"<p>First of all we need to load the csv file with the data. Underdog infers the column types by sampling the data.</p> reading<pre><code>import underdog.Underdog\n\ndef tornadoes = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> <p>Note the file, in this case, is addressed relative to the current working directory. In this case it must point to the csv file path whether it is a relative path of an absolute path.</p>"},{"location":"dataframe/#metadata","title":"Metadata","text":"<p>Often, the best way to start is to print the column names for reference:</p> list dataframe column names<pre><code>println tornadoes.columns\n</code></pre> output<pre><code>[Date, Time, State, State No, Scale, Injuries, Fatalities, Start Lat, Start Lon, Length, Width]\n</code></pre> <p>The shape() method displays the row and column counts:</p> .shape of the dataframe<pre><code>def (int rows, int cols) = tornadoes.shape()\n</code></pre> shape of the dataframe<pre><code>println tornadoes.shape()\n</code></pre> output<pre><code>59945 rows X 11 cols\n</code></pre> <p>structure() shows the index, name and type of each column</p> tornadoes schemas<pre><code>// getting tornadoes schema\ndef schema = tornadoes.schema()\n</code></pre> output<pre><code>  Structure of tornadoes_1950-2014.csv\n Index  |  Column Name  |  Column Type  |\n-----------------------------------------\n     0  |         Date  |   LOCAL_DATE  |\n     1  |         Time  |   LOCAL_TIME  |\n     2  |        State  |       STRING  |\n     3  |     State No  |      INTEGER  |\n     4  |        Scale  |      INTEGER  |\n     5  |     Injuries  |      INTEGER  |\n     6  |   Fatalities  |      INTEGER  |\n     7  |    Start Lat  |       DOUBLE  |\n     8  |    Start Lon  |       DOUBLE  |\n     9  |       Length  |       DOUBLE  |\n    10  |        Width  |      INTEGER  |\n</code></pre> <p>Like many DataFrame methods, schema() returns another DataFrame. You can then produce a string representation for display. To display the DataFrame then, you can simply call.</p> <pre><code>println tornadoes\n</code></pre> <p>You can also perform other DataFrame operations on the schema. For example, the code below removes all columns whose type isn\u2019t <code>DOUBLE</code>:</p> using schema to change dataframe structure<pre><code>def customSchema = schema[schema['Column Type'] == 'DOUBLE']\n</code></pre> output<pre><code>  Structure of tornadoes_1950-2014.csv\n Index  |  Column Name  |  Column Type  |\n-----------------------------------------\n     7  |    Start Lat  |       DOUBLE  |\n     8  |    Start Lon  |       DOUBLE  |\n     9  |       Length  |       DOUBLE  |\n</code></pre> <p>Of course, that also returned another DataFrame. We\u2019ll cover selecting rows in more detail later.</p>"},{"location":"dataframe/#previewing","title":"Previewing","text":"<p>The first(n) method returns a new DataFrame containing the first n rows.</p> getting first 3 lines<pre><code>tornadoes.head(3)\n</code></pre> output<pre><code>                                tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |  ... |\n</code></pre>"},{"location":"dataframe/#transforming","title":"Transforming","text":"<p>Mapping operations take one or more series (columns) as inputs and produce a new column as output. The method below extracts the Month name from the date column into a new column.</p> creating a new series to dataframe<pre><code>def monthSeries = tornadoes[\"Date\"](LocalDate, String) {\n    it.format(\"MMMM\")\n}\n</code></pre> <p>Now that you have a new column, you can add it to the DataFrame:</p> adding month series to tornadoes dataframe<pre><code>tornadoes['month'] = monthSeries\n</code></pre> <p>Of course nothing prevents you from doing everything altogether.</p> <p>You can remove columns from DataFrames to save memory or reduce clutter:</p> remove series from dataframe<pre><code>tornadoes = tornadoes - tornadoes['Date']\n</code></pre>"},{"location":"dataframe/#sorting","title":"Sorting","text":"<p>Now lets sort the DataFrame in reverse order by the id column. The negative sign before the name indicates a descending sort.</p> sort asc<pre><code>def df1 = tornadoes.sort_values(by: 'Fatalities')\n</code></pre> <p>You can also sort in descending order:</p> sort desc<pre><code>def df2 = tornadoes.sort_values(by: '-Fatalities')\n</code></pre> <p>and even sorting by more than one field:</p> sort by n-fields<pre><code>def df3 = tornadoes.sort_values(by: ['Fatalities', 'State'])\n</code></pre>"},{"location":"dataframe/#descriptive-statistics","title":"Descriptive statistics","text":"<p>Descriptive statistics are calculated using the summary() method:</p> printing column insights<pre><code>def columnStats = tornadoes[\"Fatalities\"].describe()\n\nprintln(columnStats)\n</code></pre> <p>Showing the following output:</p> column describe output<pre><code>         Column: Fatalities\n Measure   |         Value         |\n------------------------------------\n        n  |                59945  |\n      sum  |                 6802  |\n     Mean  |  0.11347068145800349  |\n      Min  |                    0  |\n      Max  |                  158  |\n    Range  |                  158  |\n Variance  |    2.901978053261765  |\n Std. Dev  |   1.7035193140266314  |\n</code></pre>"},{"location":"dataframe/#filtering","title":"Filtering","text":"<p>The preferred way of filtering DataFrames in Underdog is to use the list notation. Look at the following example:</p> filtering dataframe by column expressions<pre><code>// reading tornadoes\ndef ts = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\n// adding a new series to dataframe with the name of the month\nts['month'] = ts[\"Date\"](LocalDate, String) { it.format(\"MMMM\") }\n\n// filtering\ndef result = ts[\n    ts['Fatalities'] &gt; 0 &amp;                   // at least 1 fatalities\n    ts['month'] == \"April\" &amp;                 // in the month of April\n    (ts['Width'] &gt; 300 | ts['Length'] &gt; 10)  // a tornado with a\n]\n\n// selecting only two columns\ndef stateAndDate = result['State', 'Date']\n</code></pre> output<pre><code> tornadoes_1950-2014.csv\n State  |     Date     |\n------------------------\n    MO  |  1950-01-03  |\n    IL  |  1950-01-03  |\n    OH  |  1950-01-03  |\n</code></pre> <p>The last example filters the tornadoes DataFrame with predicates of type <code>dataFrame[seriesName] op (series | object)</code>. Where op can be a comparison operators such as <code>&gt;=,&lt;=,==</code> etc. Of course these expressions can be combined with <code>or</code> or <code>and</code> operators <code>|</code> and <code>&amp;</code>. </p>"},{"location":"dataframe/#grouping","title":"Grouping","text":"<p>Series metrics can be calculated using grouping methods like sum(), product(), mean(), max(), etc.  You can apply those methods to a DataFrame, calculating results on one column, grouped by the values in another.</p> <pre><code>def tornadoes = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\ndef injuriesByScale = tornadoes\n    .rename(\"Median Injuries by Tornado Scale\")\n    .agg(Injuries: \"median\")\n    .by(\"Scale\")\n    .sort_values(by: \"Scale\")\n</code></pre> <p>This produces the following DataFrame, in which Group represents the Tornado Scale and Median the median injures for that group:</p> output<pre><code>Median injuries by Tornado Scale\n Scale  |  Median [Injuries]  |\n-------------------------------\n    -9  |                  0  |\n     0  |                  0  |\n     1  |                  0  |\n     2  |                  0  |\n     3  |                  1  |\n     4  |                 12  |\n     5  |                107  |\n</code></pre>"},{"location":"dataframe/#cross-tabs","title":"Cross Tabs","text":"<p>Underdog lets you easily produce two-dimensional cross-tabulations (\u201ccross tabs\u201d) of counts and proportions with row and column subtotals. Here\u2019s a count example where we look at the interaction of tornado severity and US state:</p> crosstabs (count)<pre><code>def crossTab = tornadoes.xTabCounts(labels: 'State', values: 'Scale')\n\ncrossTab.head()\n</code></pre> output<pre><code>                       Crosstab Counts: State x Scale\n [labels]  |  -9  |   0    |   1   |   2   |   3   |  4   |  5   |  total  |\n----------------------------------------------------------------------------\n       AL  |   0  |   624  |  770  |  425  |  142  |  38  |  12  |   2011  |\n       AR  |   1  |   486  |  667  |  420  |  162  |  29  |   0  |   1765  |\n       AZ  |   1  |   146  |   71  |   16  |    3  |   0  |   0  |    237  |\n       CA  |   1  |   271  |  117  |   23  |    2  |   0  |   0  |    414  |\n       CO  |   3  |  1322  |  563  |  112  |   22  |   1  |   0  |   2023  |\n       CT  |   0  |    18  |   53  |   22  |    4  |   2  |   0  |     99  |\n       DC  |   0  |     2  |    0  |    0  |    0  |   0  |   0  |      2  |\n       DE  |   0  |    22  |   26  |   12  |    1  |   0  |   0  |     61  |\n       FL  |   2  |  1938  |  912  |  319  |   37  |   3  |   0  |   3211  |\n       GA  |   0  |   413  |  700  |  309  |   74  |  11  |   0  |   1507  |\n</code></pre> <p>Putting it all together</p> <p>Now that you\u2019ve seen the pieces, we can put them together to perform a more complex data analysis. Lets say we want to know how frequently Tornadoes occur in the summer. Here\u2019\u2019s one way to approach that:</p> <p>Let\u2019s start by getting only those tornadoes that occurred in the summer.</p> tornadoes in the summer<pre><code>def ts = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\n// adding some series to the dataframe to make filtering easier\nts['month']      = ts['Date'](Date, String) { it.format(\"MMMM\") }\nts['dayOfMonth'] = ts['Date'](Date, Integer) { it.format(\"dd\").toInteger() }\n\n// filtering\ndef summer = ts[\n    (ts['month'] == 'June' &amp; ts['dayOfMonth'] &gt; 21) |    // after June the 21st or...\n    (ts['month'] in ['July', 'August']) |                // in July or August or...\n    (ts['month'] == 'September' &amp; ts['dayOfMonth'] &lt; 22) // before September the 22nd\n]\n</code></pre> <p>Then to get the frequency:</p> <ul> <li>We calculate the difference in days between successive tornadoes. </li> <li>The lag() method creates a column where every value equals the previous value (the prior row) of the source column. </li> <li>Then we can simply get the difference in days between the two dates. DateColumn has a method daysUntil() that does this. It returns a NumberColumn that we\u2019ll call \u201cdelta\u201d.</li> </ul> lag and delta dates<pre><code>// sorting by Date and Time series\nsummer = summer.sort_values(by: ['Date', 'Time'])\n\n// creating a series with lagged dates\nsummer['Lagged'] = summer['Date'].lag(1)\n\n// creating a series with delta days between lagged dates and summer dates\nsummer['Delta'] = summer['Lagged'] - summer['Date']\n</code></pre> <p>Now we simply calculate the mean of the delta column. Splitting on year keeps us from inadvertently including the time between the last tornado of one summer and the first tornado of the next.</p> create summary<pre><code>// creating year series to be able to group by it\nsummer['year'] = summer['Date'](Date, String) { it.format(\"YYYY\") }\n\n// aggregating delta\ndef summary = summer.agg(Delta: [\"mean\", \"count\"]).by(\"year\")\n\n// print out summary\nprintln(summary)\n</code></pre> <p>Printing summary gives us the answer by year.</p> summary output<pre><code>                           tornadoes_1950-2014.csv summary\n Date year  |  Mean [Date lag(1) - Date[DAYS]]  |  Count [Date lag(1) - Date[DAYS]]  |\n--------------------------------------------------------------------------------------\n      1950  |               2.0555555555555545  |                               162  |\n      1951  |               1.7488584474885829  |                               219  |\n      1952  |               1.8673469387755088  |                               196  |\n      1953  |                0.983870967741935  |                               372  |\n      1954  |               0.8617283950617302  |                               405  |\n</code></pre> <p>To get a DOUBLE for the entire period, we can take the average of the annual means.</p> average second column<pre><code>def meanOfSeries = summary.iloc[__, 1].mean()\n</code></pre> Average days between tornadoes in the summer:<pre><code>0.5931137164104612\n</code></pre>"},{"location":"dataframe/#saving-your-data","title":"Saving your data","text":"<p>To save a DataFrame, you can write it as a CSV file:</p> saving csv file<pre><code>tornadoes.write().csv(\"rev_tornadoes_1950-2014.csv\");\n</code></pre> <p>And that\u2019s it for the introduction. Please see the User Guide for more information.</p>"},{"location":"dataframe/#dataframe","title":"DataFrame","text":"<p>Underdog's DataFrame combines tools for working with columnar data as tables (dataframes) and columns (series). And also has extra features such statistical functions and visualizations via  Underdog's plots module.</p>"},{"location":"dataframe/#creation","title":"Creation","text":"<p>The easiest way to create a Dataframe is using the Underdog extension method <code>Underdog.df()</code>. Here we're creating an empty DataFrame:</p> empty dataframe<pre><code>DataFrame emptyDataFrame = Underdog.df().empty()\n</code></pre> <p>We can create a dataframe with a series of map entries representing series. In this case the key entry is the name of the series and the value is a collection which will become the content of the series.</p> dataframe from a map<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from the map\nDataFrame map2DataFrame = Underdog.df().from(map, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Underdog dataframe library adds additional methods to collection types so that you can convert from collections to Dataframes. And example is invoking the <code>toDataFrame(...)</code> method from the map directly:</p> map extension<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from a map\nDataFrame map2DataFrame = map.toDataFrame(\"people-dataframe\")\n</code></pre> <p>You can also pass a list of maps to the <code>Underdog.df().from(col, name)</code> method. The method assumes all entries are maps with the same keys:</p> collection of maps<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = Underdog.df().from(list, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Here there is also an extension method for collections so that, IF your list complies to this structure you can call to the method <code>toDataFrame(name)</code> and create a DataFrame from that collection.</p> collection extension<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = list.toDataFrame(\"people-dataframe\")\n</code></pre>"},{"location":"dataframe/#filtering_1","title":"Filtering","text":"<p>In a dataframe you can filter data by any of the Series the dataframe has.</p>"},{"location":"dataframe/#numbers","title":"Numbers","text":"<p>The following example creates a hypothetical population progression in ten years:</p> numbers<pre><code>def df = [\n    years: (1991..2000),\n    population: (1..10).collect { 1000 * it }\n].toDataFrame(\"population increase\")\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n 1994  |        4000  |\n 1995  |        5000  |\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>If we wanted to take the records after year 1995:</p> greater than<pre><code>def last5Years = df[df['years'] &gt; 1995]\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>Or getting records with population less than 4000:</p> less than<pre><code>def yearsWithLessThan = df[df['population'] &lt; 4000]\n</code></pre> output<pre><code>population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n</code></pre>"},{"location":"dataframe/#string","title":"String","text":"<p>Of course we can filter by strings. Follow up we've got a dataframe with some employee data:</p> employees<pre><code>def df = [\n    employees: ['Udo', 'John', 'Albert', 'Ronda'],\n    department: ['sales', 'it', 'sales', 'it'],\n    payroll: [10_000, 12_000, 11_000, 13_000]\n].toDataFrame(\"employees\")\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n    Albert  |       sales  |    11000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>Getting employees from sales department:</p> sales<pre><code>def salesPeople = df[df['department'] == 'sales']\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n    Albert  |       sales  |    11000  |\n</code></pre> <p>You can also use to filter by a list of possible choices:</p> in list<pre><code>def employeesByName = df[df['employees'] in ['Ronda', 'Udo']]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>You can even try by a regular expression. Lets look for employees with an 'o' in their name:</p> regex<pre><code>def employeesWithAnO = df[df['employees'] ==~ /.*o.*/ ]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n     Ronda  |          it  |    13000  |\n</code></pre>"},{"location":"dataframe/#dates","title":"Dates","text":"<p>Of course in time series is crucial to allow searches by time frame.</p> dates<pre><code>// Using a given date as the beginning of our df dates series\ndef initialDate = LocalDate.parse('01/01/2000', 'dd/MM/yyyy')\n\n// a dataframe containing the simulation of bicycles rented through 2000\ndef df = [\n    dates: (1..365).collect(initialDate::plusDays),\n    rented: (1..365).collect { new Random().nextInt(200) }\n].toDataFrame(\"rented bicycles 2000\")\n</code></pre> output<pre><code>rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-01-02  |      41  |\n2000-01-03  |      47  |\n2000-01-04  |      27  |\n2000-01-05  |      95  |\n2000-01-06  |      30  |\n2000-01-07  |     162  |\n2000-01-08  |      52  |\n2000-01-09  |     197  |\n2000-01-10  |     125  |\n2000-01-11  |      15  |\n       ...  |     ...  |\n</code></pre> <p>What if we'd like to get only those records of december 2000 ?</p> after<pre><code>def lastMonth = df[df['dates'] &gt;= LocalDate.parse('01/12/2000', 'dd/MM/yyyy')]\n</code></pre> output<pre><code>  rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-12-01  |     104  |\n2000-12-02  |     193  |\n2000-12-03  |     107  |\n2000-12-04  |     108  |\n2000-12-05  |     193  |\n2000-12-06  |     165  |\n2000-12-07  |      82  |\n2000-12-08  |      77  |\n2000-12-09  |     176  |\n2000-12-10  |     158  |\n       ...  |     ...  |\n2000-12-31  |     150  |\n</code></pre>"},{"location":"dataframe/#and","title":"&amp; and |","text":"<p>You can combine any predicate with the operators <code>&amp;</code> and <code>|</code>. For example, lets filter by all years after 1995 AND entries with population less or equals 8000:</p> AND<pre><code>def result1 = df[df['years'] &gt; 1995 &amp; df['population'] &lt;= 8000]\n</code></pre> <p>Now filter by years less or equals to 1995 OR entries with population greater than 9000.</p> OR<pre><code>def result2 = df[df['years'] &lt;= 1998 | df['population'] &gt; 9000]\n</code></pre>"},{"location":"dataframe/#summary","title":"Summary","text":"<p>Here you have the tables with the supported operators:</p> <p>Arithmetic</p> Left Right Operator Example Status Series Series + <code>df['a'] + df['b']</code> Yes Series Series - <code>df['a'] - df['b]</code> Yes Series Object + <code>df['a'] + 1</code> Yes Series Object - <code>df['a'] - 1</code> Yes Series Object * <code>df['a'] * 2</code> Yes Series Object / <code>df['a'] / 2</code> Yes <p>filtering operators</p> Type Operator Example Status String == <code>df['a'] == 'x'</code> Yes String != <code>df['a'] != 'x'</code> Yes String ==~ <code>df['a'] ==~ /.*/</code> Yes String in <code>df['a'] in ['x']</code> Yes Number == <code>df['a'] == 1</code> Yes Number != <code>df['a'] != 1</code> Yes Number &gt; <code>df['a'] &gt; 1</code> Yes Number &gt;= <code>df['a'] &gt;= 1</code> Yes Number &lt; <code>df['a'] &lt; 1</code> Yes Number &lt;= <code>df['a'] &lt;= 1</code> Yes LocalDate &gt; <code>df['a'] &gt; date</code> Yes LocalDate &gt;= <code>df['a'] &gt;= date</code> Yes LocalDate &lt; <code>df['a'] &lt; date</code> Yes LocalDate &lt;= <code>df['a'] &lt;= date</code> Yes"},{"location":"dataframe/#sorting_1","title":"Sorting","text":"<p>TODO</p>"},{"location":"dataframe/#mapping","title":"Mapping","text":"<p>TODO</p>"},{"location":"dataframe/#import-export","title":"Import / Export","text":""},{"location":"dataframe/#csv","title":"CSV","text":""},{"location":"dataframe/#reading","title":"Reading","text":"<p>You can read csv files via <code>Underdog.df().read_csv(...)</code> method. Here we are importing a csv files containing tornado incidents in the USA:</p> import csv<pre><code>DataFrame dataframe = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> output<pre><code>                               tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |      |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |      |\n1950-01-03  |  11:55:00  |     IL  |         2  |      3  |         3  |      |\n1950-01-03  |  16:00:00  |     OH  |         1  |      1  |         1  |      |\n1950-01-13  |  05:25:00  |     AR  |         1  |      3  |         1  |      |\n1950-01-25  |  19:30:00  |     MO  |         2  |      2  |         5  |      |\n1950-01-25  |  21:00:00  |     IL  |         3  |      2  |         0  |      |\n1950-01-26  |  18:00:00  |     TX  |         1  |      2  |         2  |      |\n1950-02-11  |  13:10:00  |     TX  |         2  |      2  |         0  |      |\n       ...  |       ...  |    ...  |       ...  |    ...  |       ...  |  ... |\n</code></pre>"},{"location":"dataframe/#separator","title":"Separator","text":"<p>By default the csv reader assumes the csv file is using comma (,) as the separator character, but you can provide a custom separator. For example the following csv file content:</p> separator<pre><code>name;age\nLorna;34\nYule;63\nTom;28\n</code></pre> <p>Can be read by using the sep argument:</p> custom separator<pre><code>def dataframe = Underdog.df().read_csv(filePath, sep: \";\")\n</code></pre>"},{"location":"dataframe/#duplicated-names","title":"Duplicated names","text":"<p>Sometimes you can find a csv where columns are repeated, by default if you don't specify you allow repeated columns the import process will fail. Imagine we've got the following csv:</p> csv with repeated cols<pre><code>bronze,silver,gold,summer_total,bronze,silver,gold,winter_total\n1,2,1,4,1,1,1,3\n</code></pre> <p>To allow repeated columns you should set the <code>allowDuplicatedNames</code> flag to true.</p> allow repeated cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, allowedDuplicatedNames: true)\n</code></pre> <p>Then all repeated names will be prefixed in order with a number to avoid collisions:</p> output<pre><code>                                        io_repeated_cols.csv\n bronze |  silver |  gold |  summer_total |  bronze-2  |  silver-2  |  gold-2  |  winter_total|\n-----------------------------------------------------------------------------------------------\n      1 |       2 |     1 |             4 |         1  |         1  |       1  |           3  |\n</code></pre>"},{"location":"dataframe/#missing-values","title":"Missing values","text":"<p>If a csv file contains values which should be considered as well as missing values, we can pass this information before reading the csv file.</p> csv file with missing data<pre><code>from,to,id\nN/C,Madrid,123\nMadrid,Paris,124\nParis,London,125\nLondon,NONE,126\n</code></pre> <p>Here we're considering missing data the values N/C and NONE:</p> considering missing data<pre><code>def dataframe = Underdog.df().read_csv(filePath, nanValues: ['NONE', 'N/C'])\n</code></pre> <p>That will inform the reader to consider cells containing that value as missing values:</p> output<pre><code>io_custom_missing_data.csv\n  from   |    to    |  id   |\n-----------------------------\n         |  Madrid  |  123  |\n Madrid  |   Paris  |  124  |\n  Paris  |  London  |  125  |\n London  |          |  126  |\n</code></pre>"},{"location":"dataframe/#date-format","title":"Date format","text":"<p>If your csv files have a custom date format you can provide the date pattern as a parameter. Here we have a file with a custom format:</p> custom date format<pre><code>Date,Close\n2014-12-05 00:00:00+00:00,0.5267500281333923\n2014-12-08 00:00:00+00:00,0.5199999809265137\n2014-12-09 00:00:00+00:00,0.5182499885559082\n</code></pre> <p>Passing the pattern as parameter:</p> custom date format<pre><code>def dataframe = Underdog.df().read_csv(filePath, dateFormat: \"yyyy-MM-dd HH:mm:ss+00:00\")\n</code></pre> <p>Gives the following output:</p> output<pre><code>      io_custom_date_format.csv\n   Date     |        Close         |\n-------------------------------------\n2014-12-05  |  0.5267500281333923  |\n2014-12-08  |  0.5199999809265137  |\n2014-12-09  |  0.5182499885559082  |\n</code></pre>"},{"location":"dataframe/#skip-rowsfooter","title":"Skip rows/footer","text":"<p>If you're sure that there is data you'd like to avoid parsing, like nonsense data, you can skip parsing those rows. Check the following example:</p> csv file with comments<pre><code># some information about the data\n# col1: city\n# col2: weight\ncol1,col2\nNC,0\nNC,0\nNC,0\nNC,0\nMadrid,1\nParis,2\nNC,0\nNC,0\nNC,0\nNC,0\n# another comment here\n</code></pre> <p>There are lines we don't want to consider when creating our dataframe:</p> <ul> <li>comments in the beginning of the file (lines 1-3)</li> <li>comments in the end of the file (line 15)</li> <li>rows we don't want to parse because they don't add any meaningful information (4-8 and 11-14)</li> </ul> <p>To avoid parsing any of these lines we can instruct the csv reader to skip lines in the header and/or in the footer of the file:</p> skipping rows<pre><code>def dataframe = Underdog.df()\n    .read_csv(filePath,\n        header: false,    // not using first row as header\n        skipRows: 8,      // skipping rows at the beginning of the file\n        skipFooter: 4     // skipping rows at the end of the file\n    ).renameSeries(columns: ['city', 'id']) // renaming series names with the list passed as parameter\n</code></pre> output<pre><code>io_skipping_rows.csv\n city   |  id  |\n-----------------\nMadrid  |   1  |\n Paris  |   2  |\n</code></pre>"},{"location":"dataframe/#max-chars-x-col","title":"Max chars x col","text":"<p>You can instruct the csv reader to avoid parsing columns with more than a number of characters.</p> limiting col chars<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxCharsPerColumn: 20)\n</code></pre> <p>Warning</p> <p>If a column exceeds the number of characters the process will throw an exception</p>"},{"location":"dataframe/#max-cols","title":"Max cols","text":"<p>You can instruct the csv reader to avoid parsing more than a given number of columns.</p> limiting number of cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxNumberOfColumns: 2)\n</code></pre> <p>Warning</p> <p>If the number of columns exceeds the number specified the process will throw an exception</p>"},{"location":"dataframe/#series","title":"Series","text":""},{"location":"dataframe/#intro","title":"Intro","text":"<p>A Series object represents a named one-dimensional array. It also supports operations and statistical methods. It also has operations to deal with missing values. You can create a Series object from different sources:</p> create<pre><code>// from a range of numbers\ndef numbers = (1..4).toSeries(\"numbers\")\n\n// from a range of letters\ndef letters = ('A'..'C').toSeries(\"letters\")\n\n// from a list\ndef stuff = [1, 2, null, 3, 4].toSeries(\"stuff\")\n</code></pre> <p>You can use operator symbols to apply simple operations over the Series object:</p> operations<pre><code>// multiplying a series by a number\ndef doubleSeries = numbers * 2\n\n// multiplying a series by another series\ndef rowProduct = numbers * stuff.dropna()\n\n// dividing a series\ndef halves = stuff / 2\n\n// using custom transformation to create a new series\ndef custom = letters(String, String) { \"letter-$it\".toString() }\n</code></pre> <p>Sometimes you may want to analyze a given Series object by using statistical methods:</p> statistics<pre><code>def mean = doubleSeries.mean()\ndef max = doubleSeries.max()\ndef min = doubleSeries.min()\ndef avg = doubleSeries.avg()\n</code></pre> <p>You can find all statistical available methods in the <code>underdog.impl.extensions.SeriesStatsExtensions</code> class.</p>"},{"location":"dataframe/#creating","title":"Creating","text":"<p>Series are meant to be created from collections or as a transformation from another Series.</p> <p>The only way to create a Series from a collection is invoking the extension method <code>toSeries()</code> from a list:</p> collection extension<pre><code>Series series = [1, 2, 3].toSeries(\"numbers\")\n</code></pre> <p>Most of the time we will be dealing with a Series creation inside the scope of a Dataframe. Sometimes as the result of the transformation of another series, sometimes because we would like to fill a series from a constant value.</p> <p>Lets say we have a DataFrame with some Series:</p> sample dataframe<pre><code>def numbers = Underdog.df().from([numbers: 1..10], \"numbers\")\n</code></pre> output<pre><code> numbers\n numbers  |\n-----------\n       1  |\n       2  |\n       3  |\n       4  |\n       5  |\n       6  |\n       7  |\n       8  |\n       9  |\n      10  |\n</code></pre> <p>And we want to create a new series named by_two with the result of multiplying all numbers in the numbers series:</p> new series<pre><code>numbers['by_two'] = numbers['numbers'] * 2\n</code></pre> output<pre><code>       numbers\n numbers  |  by_two  |\n----------------------\n       1  |       2  |\n       2  |       4  |\n       3  |       6  |\n       4  |       8  |\n       5  |      10  |\n       6  |      12  |\n       7  |      14  |\n       8  |      16  |\n       9  |      18  |\n      10  |      20  |\n</code></pre> <p>You can also create a new Series inside a dataframe filling all rows with the same value:</p> series from value<pre><code>numbers['one'] = 1\n</code></pre> output<pre><code>           numbers\n numbers  |  by_two  |  one  |\n------------------------------\n       1  |       2  |    1  |\n       2  |       4  |    1  |\n       3  |       6  |    1  |\n       4  |       8  |    1  |\n       5  |      10  |    1  |\n       6  |      12  |    1  |\n       7  |      14  |    1  |\n       8  |      16  |    1  |\n       9  |      18  |    1  |\n      10  |      20  |    1  |\n</code></pre>"},{"location":"dataframe/#statistics","title":"Statistics","text":"<p>TODO</p>"},{"location":"dataframe/tutorial/","title":"Tutorial","text":""},{"location":"dataframe/tutorial/#tutorial","title":"Tutorial","text":""},{"location":"dataframe/tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"dataframe/tutorial/#dependencies","title":"Dependencies","text":"<p>To be able to follow the tutorial you need the <code>underdog-dataframe</code> module. If you're using Gradle in your project:</p> GradleMavenGrapes <pre><code>implementation \"com.github.grooviter:underdog-dataframe:VERSION\"\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-dataframe&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grab(\"com.github.grooviter:underdog-dataframe:VERSION\")\n</code></pre>"},{"location":"dataframe/tutorial/#data","title":"Data","text":"<p>You can find the data used in this tutorial here </p>"},{"location":"dataframe/tutorial/#loading-data","title":"Loading data","text":"<p>First of all we need to load the csv file with the data. Underdog infers the column types by sampling the data.</p> reading<pre><code>import underdog.Underdog\n\ndef tornadoes = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> <p>Note the file, in this case, is addressed relative to the current working directory. In this case it must point to the csv file path whether it is a relative path of an absolute path.</p>"},{"location":"dataframe/tutorial/#metadata","title":"Metadata","text":"<p>Often, the best way to start is to print the column names for reference:</p> list dataframe column names<pre><code>println tornadoes.columns\n</code></pre> output<pre><code>[Date, Time, State, State No, Scale, Injuries, Fatalities, Start Lat, Start Lon, Length, Width]\n</code></pre> <p>The shape() method displays the row and column counts:</p> .shape of the dataframe<pre><code>def (int rows, int cols) = tornadoes.shape()\n</code></pre> shape of the dataframe<pre><code>println tornadoes.shape()\n</code></pre> output<pre><code>59945 rows X 11 cols\n</code></pre> <p>structure() shows the index, name and type of each column</p> tornadoes schemas<pre><code>// getting tornadoes schema\ndef schema = tornadoes.schema()\n</code></pre> output<pre><code>  Structure of tornadoes_1950-2014.csv\n Index  |  Column Name  |  Column Type  |\n-----------------------------------------\n     0  |         Date  |   LOCAL_DATE  |\n     1  |         Time  |   LOCAL_TIME  |\n     2  |        State  |       STRING  |\n     3  |     State No  |      INTEGER  |\n     4  |        Scale  |      INTEGER  |\n     5  |     Injuries  |      INTEGER  |\n     6  |   Fatalities  |      INTEGER  |\n     7  |    Start Lat  |       DOUBLE  |\n     8  |    Start Lon  |       DOUBLE  |\n     9  |       Length  |       DOUBLE  |\n    10  |        Width  |      INTEGER  |\n</code></pre> <p>Like many DataFrame methods, schema() returns another DataFrame. You can then produce a string representation for display. To display the DataFrame then, you can simply call.</p> <pre><code>println tornadoes\n</code></pre> <p>You can also perform other DataFrame operations on the schema. For example, the code below removes all columns whose type isn\u2019t <code>DOUBLE</code>:</p> using schema to change dataframe structure<pre><code>def customSchema = schema[schema['Column Type'] == 'DOUBLE']\n</code></pre> output<pre><code>  Structure of tornadoes_1950-2014.csv\n Index  |  Column Name  |  Column Type  |\n-----------------------------------------\n     7  |    Start Lat  |       DOUBLE  |\n     8  |    Start Lon  |       DOUBLE  |\n     9  |       Length  |       DOUBLE  |\n</code></pre> <p>Of course, that also returned another DataFrame. We\u2019ll cover selecting rows in more detail later.</p>"},{"location":"dataframe/tutorial/#previewing","title":"Previewing","text":"<p>The first(n) method returns a new DataFrame containing the first n rows.</p> getting first 3 lines<pre><code>tornadoes.head(3)\n</code></pre> output<pre><code>                                tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |  ... |\n</code></pre>"},{"location":"dataframe/tutorial/#transforming","title":"Transforming","text":"<p>Mapping operations take one or more series (columns) as inputs and produce a new column as output. The method below extracts the Month name from the date column into a new column.</p> creating a new series to dataframe<pre><code>def monthSeries = tornadoes[\"Date\"](LocalDate, String) {\n    it.format(\"MMMM\")\n}\n</code></pre> <p>Now that you have a new column, you can add it to the DataFrame:</p> adding month series to tornadoes dataframe<pre><code>tornadoes['month'] = monthSeries\n</code></pre> <p>Of course nothing prevents you from doing everything altogether.</p> <p>You can remove columns from DataFrames to save memory or reduce clutter:</p> remove series from dataframe<pre><code>tornadoes = tornadoes - tornadoes['Date']\n</code></pre>"},{"location":"dataframe/tutorial/#sorting","title":"Sorting","text":"<p>Now lets sort the DataFrame in reverse order by the id column. The negative sign before the name indicates a descending sort.</p> sort asc<pre><code>def df1 = tornadoes.sort_values(by: 'Fatalities')\n</code></pre> <p>You can also sort in descending order:</p> sort desc<pre><code>def df2 = tornadoes.sort_values(by: '-Fatalities')\n</code></pre> <p>and even sorting by more than one field:</p> sort by n-fields<pre><code>def df3 = tornadoes.sort_values(by: ['Fatalities', 'State'])\n</code></pre>"},{"location":"dataframe/tutorial/#descriptive-statistics","title":"Descriptive statistics","text":"<p>Descriptive statistics are calculated using the summary() method:</p> printing column insights<pre><code>def columnStats = tornadoes[\"Fatalities\"].describe()\n\nprintln(columnStats)\n</code></pre> <p>Showing the following output:</p> column describe output<pre><code>         Column: Fatalities\n Measure   |         Value         |\n------------------------------------\n        n  |                59945  |\n      sum  |                 6802  |\n     Mean  |  0.11347068145800349  |\n      Min  |                    0  |\n      Max  |                  158  |\n    Range  |                  158  |\n Variance  |    2.901978053261765  |\n Std. Dev  |   1.7035193140266314  |\n</code></pre>"},{"location":"dataframe/tutorial/#filtering","title":"Filtering","text":"<p>The preferred way of filtering DataFrames in Underdog is to use the list notation. Look at the following example:</p> filtering dataframe by column expressions<pre><code>// reading tornadoes\ndef ts = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\n// adding a new series to dataframe with the name of the month\nts['month'] = ts[\"Date\"](LocalDate, String) { it.format(\"MMMM\") }\n\n// filtering\ndef result = ts[\n    ts['Fatalities'] &gt; 0 &amp;                   // at least 1 fatalities\n    ts['month'] == \"April\" &amp;                 // in the month of April\n    (ts['Width'] &gt; 300 | ts['Length'] &gt; 10)  // a tornado with a\n]\n\n// selecting only two columns\ndef stateAndDate = result['State', 'Date']\n</code></pre> output<pre><code> tornadoes_1950-2014.csv\n State  |     Date     |\n------------------------\n    MO  |  1950-01-03  |\n    IL  |  1950-01-03  |\n    OH  |  1950-01-03  |\n</code></pre> <p>The last example filters the tornadoes DataFrame with predicates of type <code>dataFrame[seriesName] op (series | object)</code>. Where op can be a comparison operators such as <code>&gt;=,&lt;=,==</code> etc. Of course these expressions can be combined with <code>or</code> or <code>and</code> operators <code>|</code> and <code>&amp;</code>. </p>"},{"location":"dataframe/tutorial/#grouping","title":"Grouping","text":"<p>Series metrics can be calculated using grouping methods like sum(), product(), mean(), max(), etc.  You can apply those methods to a DataFrame, calculating results on one column, grouped by the values in another.</p> <pre><code>def tornadoes = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\ndef injuriesByScale = tornadoes\n    .rename(\"Median Injuries by Tornado Scale\")\n    .agg(Injuries: \"median\")\n    .by(\"Scale\")\n    .sort_values(by: \"Scale\")\n</code></pre> <p>This produces the following DataFrame, in which Group represents the Tornado Scale and Median the median injures for that group:</p> output<pre><code>Median injuries by Tornado Scale\n Scale  |  Median [Injuries]  |\n-------------------------------\n    -9  |                  0  |\n     0  |                  0  |\n     1  |                  0  |\n     2  |                  0  |\n     3  |                  1  |\n     4  |                 12  |\n     5  |                107  |\n</code></pre>"},{"location":"dataframe/tutorial/#cross-tabs","title":"Cross Tabs","text":"<p>Underdog lets you easily produce two-dimensional cross-tabulations (\u201ccross tabs\u201d) of counts and proportions with row and column subtotals. Here\u2019s a count example where we look at the interaction of tornado severity and US state:</p> crosstabs (count)<pre><code>def crossTab = tornadoes.xTabCounts(labels: 'State', values: 'Scale')\n\ncrossTab.head()\n</code></pre> output<pre><code>                       Crosstab Counts: State x Scale\n [labels]  |  -9  |   0    |   1   |   2   |   3   |  4   |  5   |  total  |\n----------------------------------------------------------------------------\n       AL  |   0  |   624  |  770  |  425  |  142  |  38  |  12  |   2011  |\n       AR  |   1  |   486  |  667  |  420  |  162  |  29  |   0  |   1765  |\n       AZ  |   1  |   146  |   71  |   16  |    3  |   0  |   0  |    237  |\n       CA  |   1  |   271  |  117  |   23  |    2  |   0  |   0  |    414  |\n       CO  |   3  |  1322  |  563  |  112  |   22  |   1  |   0  |   2023  |\n       CT  |   0  |    18  |   53  |   22  |    4  |   2  |   0  |     99  |\n       DC  |   0  |     2  |    0  |    0  |    0  |   0  |   0  |      2  |\n       DE  |   0  |    22  |   26  |   12  |    1  |   0  |   0  |     61  |\n       FL  |   2  |  1938  |  912  |  319  |   37  |   3  |   0  |   3211  |\n       GA  |   0  |   413  |  700  |  309  |   74  |  11  |   0  |   1507  |\n</code></pre> <p>Putting it all together</p> <p>Now that you\u2019ve seen the pieces, we can put them together to perform a more complex data analysis. Lets say we want to know how frequently Tornadoes occur in the summer. Here\u2019\u2019s one way to approach that:</p> <p>Let\u2019s start by getting only those tornadoes that occurred in the summer.</p> tornadoes in the summer<pre><code>def ts = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\n// adding some series to the dataframe to make filtering easier\nts['month']      = ts['Date'](Date, String) { it.format(\"MMMM\") }\nts['dayOfMonth'] = ts['Date'](Date, Integer) { it.format(\"dd\").toInteger() }\n\n// filtering\ndef summer = ts[\n    (ts['month'] == 'June' &amp; ts['dayOfMonth'] &gt; 21) |    // after June the 21st or...\n    (ts['month'] in ['July', 'August']) |                // in July or August or...\n    (ts['month'] == 'September' &amp; ts['dayOfMonth'] &lt; 22) // before September the 22nd\n]\n</code></pre> <p>Then to get the frequency:</p> <ul> <li>We calculate the difference in days between successive tornadoes. </li> <li>The lag() method creates a column where every value equals the previous value (the prior row) of the source column. </li> <li>Then we can simply get the difference in days between the two dates. DateColumn has a method daysUntil() that does this. It returns a NumberColumn that we\u2019ll call \u201cdelta\u201d.</li> </ul> lag and delta dates<pre><code>// sorting by Date and Time series\nsummer = summer.sort_values(by: ['Date', 'Time'])\n\n// creating a series with lagged dates\nsummer['Lagged'] = summer['Date'].lag(1)\n\n// creating a series with delta days between lagged dates and summer dates\nsummer['Delta'] = summer['Lagged'] - summer['Date']\n</code></pre> <p>Now we simply calculate the mean of the delta column. Splitting on year keeps us from inadvertently including the time between the last tornado of one summer and the first tornado of the next.</p> create summary<pre><code>// creating year series to be able to group by it\nsummer['year'] = summer['Date'](Date, String) { it.format(\"YYYY\") }\n\n// aggregating delta\ndef summary = summer.agg(Delta: [\"mean\", \"count\"]).by(\"year\")\n\n// print out summary\nprintln(summary)\n</code></pre> <p>Printing summary gives us the answer by year.</p> summary output<pre><code>                           tornadoes_1950-2014.csv summary\n Date year  |  Mean [Date lag(1) - Date[DAYS]]  |  Count [Date lag(1) - Date[DAYS]]  |\n--------------------------------------------------------------------------------------\n      1950  |               2.0555555555555545  |                               162  |\n      1951  |               1.7488584474885829  |                               219  |\n      1952  |               1.8673469387755088  |                               196  |\n      1953  |                0.983870967741935  |                               372  |\n      1954  |               0.8617283950617302  |                               405  |\n</code></pre> <p>To get a DOUBLE for the entire period, we can take the average of the annual means.</p> average second column<pre><code>def meanOfSeries = summary.iloc[__, 1].mean()\n</code></pre> Average days between tornadoes in the summer:<pre><code>0.5931137164104612\n</code></pre>"},{"location":"dataframe/tutorial/#saving-your-data","title":"Saving your data","text":"<p>To save a DataFrame, you can write it as a CSV file:</p> saving csv file<pre><code>tornadoes.write().csv(\"rev_tornadoes_1950-2014.csv\");\n</code></pre> <p>And that\u2019s it for the introduction. Please see the User Guide for more information.</p>"},{"location":"dataframe/dataframe/","title":"Index","text":""},{"location":"dataframe/dataframe/#dataframe","title":"DataFrame","text":"<p>Underdog's DataFrame combines tools for working with columnar data as tables (dataframes) and columns (series). And also has extra features such statistical functions and visualizations via  Underdog's plots module.</p>"},{"location":"dataframe/dataframe/#creation","title":"Creation","text":"<p>The easiest way to create a Dataframe is using the Underdog extension method <code>Underdog.df()</code>. Here we're creating an empty DataFrame:</p> empty dataframe<pre><code>DataFrame emptyDataFrame = Underdog.df().empty()\n</code></pre> <p>We can create a dataframe with a series of map entries representing series. In this case the key entry is the name of the series and the value is a collection which will become the content of the series.</p> dataframe from a map<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from the map\nDataFrame map2DataFrame = Underdog.df().from(map, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Underdog dataframe library adds additional methods to collection types so that you can convert from collections to Dataframes. And example is invoking the <code>toDataFrame(...)</code> method from the map directly:</p> map extension<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from a map\nDataFrame map2DataFrame = map.toDataFrame(\"people-dataframe\")\n</code></pre> <p>You can also pass a list of maps to the <code>Underdog.df().from(col, name)</code> method. The method assumes all entries are maps with the same keys:</p> collection of maps<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = Underdog.df().from(list, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Here there is also an extension method for collections so that, IF your list complies to this structure you can call to the method <code>toDataFrame(name)</code> and create a DataFrame from that collection.</p> collection extension<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = list.toDataFrame(\"people-dataframe\")\n</code></pre>"},{"location":"dataframe/dataframe/#filtering","title":"Filtering","text":"<p>In a dataframe you can filter data by any of the Series the dataframe has.</p>"},{"location":"dataframe/dataframe/#numbers","title":"Numbers","text":"<p>The following example creates a hypothetical population progression in ten years:</p> numbers<pre><code>def df = [\n    years: (1991..2000),\n    population: (1..10).collect { 1000 * it }\n].toDataFrame(\"population increase\")\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n 1994  |        4000  |\n 1995  |        5000  |\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>If we wanted to take the records after year 1995:</p> greater than<pre><code>def last5Years = df[df['years'] &gt; 1995]\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>Or getting records with population less than 4000:</p> less than<pre><code>def yearsWithLessThan = df[df['population'] &lt; 4000]\n</code></pre> output<pre><code>population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n</code></pre>"},{"location":"dataframe/dataframe/#string","title":"String","text":"<p>Of course we can filter by strings. Follow up we've got a dataframe with some employee data:</p> employees<pre><code>def df = [\n    employees: ['Udo', 'John', 'Albert', 'Ronda'],\n    department: ['sales', 'it', 'sales', 'it'],\n    payroll: [10_000, 12_000, 11_000, 13_000]\n].toDataFrame(\"employees\")\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n    Albert  |       sales  |    11000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>Getting employees from sales department:</p> sales<pre><code>def salesPeople = df[df['department'] == 'sales']\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n    Albert  |       sales  |    11000  |\n</code></pre> <p>You can also use to filter by a list of possible choices:</p> in list<pre><code>def employeesByName = df[df['employees'] in ['Ronda', 'Udo']]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>You can even try by a regular expression. Lets look for employees with an 'o' in their name:</p> regex<pre><code>def employeesWithAnO = df[df['employees'] ==~ /.*o.*/ ]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n     Ronda  |          it  |    13000  |\n</code></pre>"},{"location":"dataframe/dataframe/#dates","title":"Dates","text":"<p>Of course in time series is crucial to allow searches by time frame.</p> dates<pre><code>// Using a given date as the beginning of our df dates series\ndef initialDate = LocalDate.parse('01/01/2000', 'dd/MM/yyyy')\n\n// a dataframe containing the simulation of bicycles rented through 2000\ndef df = [\n    dates: (1..365).collect(initialDate::plusDays),\n    rented: (1..365).collect { new Random().nextInt(200) }\n].toDataFrame(\"rented bicycles 2000\")\n</code></pre> output<pre><code>rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-01-02  |      41  |\n2000-01-03  |      47  |\n2000-01-04  |      27  |\n2000-01-05  |      95  |\n2000-01-06  |      30  |\n2000-01-07  |     162  |\n2000-01-08  |      52  |\n2000-01-09  |     197  |\n2000-01-10  |     125  |\n2000-01-11  |      15  |\n       ...  |     ...  |\n</code></pre> <p>What if we'd like to get only those records of december 2000 ?</p> after<pre><code>def lastMonth = df[df['dates'] &gt;= LocalDate.parse('01/12/2000', 'dd/MM/yyyy')]\n</code></pre> output<pre><code>  rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-12-01  |     104  |\n2000-12-02  |     193  |\n2000-12-03  |     107  |\n2000-12-04  |     108  |\n2000-12-05  |     193  |\n2000-12-06  |     165  |\n2000-12-07  |      82  |\n2000-12-08  |      77  |\n2000-12-09  |     176  |\n2000-12-10  |     158  |\n       ...  |     ...  |\n2000-12-31  |     150  |\n</code></pre>"},{"location":"dataframe/dataframe/#and","title":"&amp; and |","text":"<p>You can combine any predicate with the operators <code>&amp;</code> and <code>|</code>. For example, lets filter by all years after 1995 AND entries with population less or equals 8000:</p> AND<pre><code>def result1 = df[df['years'] &gt; 1995 &amp; df['population'] &lt;= 8000]\n</code></pre> <p>Now filter by years less or equals to 1995 OR entries with population greater than 9000.</p> OR<pre><code>def result2 = df[df['years'] &lt;= 1998 | df['population'] &gt; 9000]\n</code></pre>"},{"location":"dataframe/dataframe/#summary","title":"Summary","text":"<p>Here you have the tables with the supported operators:</p> <p>Arithmetic</p> Left Right Operator Example Status Series Series + <code>df['a'] + df['b']</code> Yes Series Series - <code>df['a'] - df['b]</code> Yes Series Object + <code>df['a'] + 1</code> Yes Series Object - <code>df['a'] - 1</code> Yes Series Object * <code>df['a'] * 2</code> Yes Series Object / <code>df['a'] / 2</code> Yes <p>filtering operators</p> Type Operator Example Status String == <code>df['a'] == 'x'</code> Yes String != <code>df['a'] != 'x'</code> Yes String ==~ <code>df['a'] ==~ /.*/</code> Yes String in <code>df['a'] in ['x']</code> Yes Number == <code>df['a'] == 1</code> Yes Number != <code>df['a'] != 1</code> Yes Number &gt; <code>df['a'] &gt; 1</code> Yes Number &gt;= <code>df['a'] &gt;= 1</code> Yes Number &lt; <code>df['a'] &lt; 1</code> Yes Number &lt;= <code>df['a'] &lt;= 1</code> Yes LocalDate &gt; <code>df['a'] &gt; date</code> Yes LocalDate &gt;= <code>df['a'] &gt;= date</code> Yes LocalDate &lt; <code>df['a'] &lt; date</code> Yes LocalDate &lt;= <code>df['a'] &lt;= date</code> Yes"},{"location":"dataframe/dataframe/#sorting","title":"Sorting","text":"<p>TODO</p>"},{"location":"dataframe/dataframe/#mapping","title":"Mapping","text":"<p>TODO</p>"},{"location":"dataframe/dataframe/#import-export","title":"Import / Export","text":""},{"location":"dataframe/dataframe/#csv","title":"CSV","text":""},{"location":"dataframe/dataframe/#reading","title":"Reading","text":"<p>You can read csv files via <code>Underdog.df().read_csv(...)</code> method. Here we are importing a csv files containing tornado incidents in the USA:</p> import csv<pre><code>DataFrame dataframe = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> output<pre><code>                               tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |      |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |      |\n1950-01-03  |  11:55:00  |     IL  |         2  |      3  |         3  |      |\n1950-01-03  |  16:00:00  |     OH  |         1  |      1  |         1  |      |\n1950-01-13  |  05:25:00  |     AR  |         1  |      3  |         1  |      |\n1950-01-25  |  19:30:00  |     MO  |         2  |      2  |         5  |      |\n1950-01-25  |  21:00:00  |     IL  |         3  |      2  |         0  |      |\n1950-01-26  |  18:00:00  |     TX  |         1  |      2  |         2  |      |\n1950-02-11  |  13:10:00  |     TX  |         2  |      2  |         0  |      |\n       ...  |       ...  |    ...  |       ...  |    ...  |       ...  |  ... |\n</code></pre>"},{"location":"dataframe/dataframe/#separator","title":"Separator","text":"<p>By default the csv reader assumes the csv file is using comma (,) as the separator character, but you can provide a custom separator. For example the following csv file content:</p> separator<pre><code>name;age\nLorna;34\nYule;63\nTom;28\n</code></pre> <p>Can be read by using the sep argument:</p> custom separator<pre><code>def dataframe = Underdog.df().read_csv(filePath, sep: \";\")\n</code></pre>"},{"location":"dataframe/dataframe/#duplicated-names","title":"Duplicated names","text":"<p>Sometimes you can find a csv where columns are repeated, by default if you don't specify you allow repeated columns the import process will fail. Imagine we've got the following csv:</p> csv with repeated cols<pre><code>bronze,silver,gold,summer_total,bronze,silver,gold,winter_total\n1,2,1,4,1,1,1,3\n</code></pre> <p>To allow repeated columns you should set the <code>allowDuplicatedNames</code> flag to true.</p> allow repeated cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, allowedDuplicatedNames: true)\n</code></pre> <p>Then all repeated names will be prefixed in order with a number to avoid collisions:</p> output<pre><code>                                        io_repeated_cols.csv\n bronze |  silver |  gold |  summer_total |  bronze-2  |  silver-2  |  gold-2  |  winter_total|\n-----------------------------------------------------------------------------------------------\n      1 |       2 |     1 |             4 |         1  |         1  |       1  |           3  |\n</code></pre>"},{"location":"dataframe/dataframe/#missing-values","title":"Missing values","text":"<p>If a csv file contains values which should be considered as well as missing values, we can pass this information before reading the csv file.</p> csv file with missing data<pre><code>from,to,id\nN/C,Madrid,123\nMadrid,Paris,124\nParis,London,125\nLondon,NONE,126\n</code></pre> <p>Here we're considering missing data the values N/C and NONE:</p> considering missing data<pre><code>def dataframe = Underdog.df().read_csv(filePath, nanValues: ['NONE', 'N/C'])\n</code></pre> <p>That will inform the reader to consider cells containing that value as missing values:</p> output<pre><code>io_custom_missing_data.csv\n  from   |    to    |  id   |\n-----------------------------\n         |  Madrid  |  123  |\n Madrid  |   Paris  |  124  |\n  Paris  |  London  |  125  |\n London  |          |  126  |\n</code></pre>"},{"location":"dataframe/dataframe/#date-format","title":"Date format","text":"<p>If your csv files have a custom date format you can provide the date pattern as a parameter. Here we have a file with a custom format:</p> custom date format<pre><code>Date,Close\n2014-12-05 00:00:00+00:00,0.5267500281333923\n2014-12-08 00:00:00+00:00,0.5199999809265137\n2014-12-09 00:00:00+00:00,0.5182499885559082\n</code></pre> <p>Passing the pattern as parameter:</p> custom date format<pre><code>def dataframe = Underdog.df().read_csv(filePath, dateFormat: \"yyyy-MM-dd HH:mm:ss+00:00\")\n</code></pre> <p>Gives the following output:</p> output<pre><code>      io_custom_date_format.csv\n   Date     |        Close         |\n-------------------------------------\n2014-12-05  |  0.5267500281333923  |\n2014-12-08  |  0.5199999809265137  |\n2014-12-09  |  0.5182499885559082  |\n</code></pre>"},{"location":"dataframe/dataframe/#skip-rowsfooter","title":"Skip rows/footer","text":"<p>If you're sure that there is data you'd like to avoid parsing, like nonsense data, you can skip parsing those rows. Check the following example:</p> csv file with comments<pre><code># some information about the data\n# col1: city\n# col2: weight\ncol1,col2\nNC,0\nNC,0\nNC,0\nNC,0\nMadrid,1\nParis,2\nNC,0\nNC,0\nNC,0\nNC,0\n# another comment here\n</code></pre> <p>There are lines we don't want to consider when creating our dataframe:</p> <ul> <li>comments in the beginning of the file (lines 1-3)</li> <li>comments in the end of the file (line 15)</li> <li>rows we don't want to parse because they don't add any meaningful information (4-8 and 11-14)</li> </ul> <p>To avoid parsing any of these lines we can instruct the csv reader to skip lines in the header and/or in the footer of the file:</p> skipping rows<pre><code>def dataframe = Underdog.df()\n    .read_csv(filePath,\n        header: false,    // not using first row as header\n        skipRows: 8,      // skipping rows at the beginning of the file\n        skipFooter: 4     // skipping rows at the end of the file\n    ).renameSeries(columns: ['city', 'id']) // renaming series names with the list passed as parameter\n</code></pre> output<pre><code>io_skipping_rows.csv\n city   |  id  |\n-----------------\nMadrid  |   1  |\n Paris  |   2  |\n</code></pre>"},{"location":"dataframe/dataframe/#max-chars-x-col","title":"Max chars x col","text":"<p>You can instruct the csv reader to avoid parsing columns with more than a number of characters.</p> limiting col chars<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxCharsPerColumn: 20)\n</code></pre> <p>Warning</p> <p>If a column exceeds the number of characters the process will throw an exception</p>"},{"location":"dataframe/dataframe/#max-cols","title":"Max cols","text":"<p>You can instruct the csv reader to avoid parsing more than a given number of columns.</p> limiting number of cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxNumberOfColumns: 2)\n</code></pre> <p>Warning</p> <p>If the number of columns exceeds the number specified the process will throw an exception</p>"},{"location":"dataframe/dataframe/creation/","title":"Creation","text":""},{"location":"dataframe/dataframe/creation/#creation","title":"Creation","text":"<p>The easiest way to create a Dataframe is using the Underdog extension method <code>Underdog.df()</code>. Here we're creating an empty DataFrame:</p> empty dataframe<pre><code>DataFrame emptyDataFrame = Underdog.df().empty()\n</code></pre> <p>We can create a dataframe with a series of map entries representing series. In this case the key entry is the name of the series and the value is a collection which will become the content of the series.</p> dataframe from a map<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from the map\nDataFrame map2DataFrame = Underdog.df().from(map, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Underdog dataframe library adds additional methods to collection types so that you can convert from collections to Dataframes. And example is invoking the <code>toDataFrame(...)</code> method from the map directly:</p> map extension<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from a map\nDataFrame map2DataFrame = map.toDataFrame(\"people-dataframe\")\n</code></pre> <p>You can also pass a list of maps to the <code>Underdog.df().from(col, name)</code> method. The method assumes all entries are maps with the same keys:</p> collection of maps<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = Underdog.df().from(list, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Here there is also an extension method for collections so that, IF your list complies to this structure you can call to the method <code>toDataFrame(name)</code> and create a DataFrame from that collection.</p> collection extension<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = list.toDataFrame(\"people-dataframe\")\n</code></pre>"},{"location":"dataframe/dataframe/filtering/","title":"Filtering","text":""},{"location":"dataframe/dataframe/filtering/#filtering","title":"Filtering","text":"<p>In a dataframe you can filter data by any of the Series the dataframe has.</p>"},{"location":"dataframe/dataframe/filtering/#numbers","title":"Numbers","text":"<p>The following example creates a hypothetical population progression in ten years:</p> numbers<pre><code>def df = [\n    years: (1991..2000),\n    population: (1..10).collect { 1000 * it }\n].toDataFrame(\"population increase\")\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n 1994  |        4000  |\n 1995  |        5000  |\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>If we wanted to take the records after year 1995:</p> greater than<pre><code>def last5Years = df[df['years'] &gt; 1995]\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>Or getting records with population less than 4000:</p> less than<pre><code>def yearsWithLessThan = df[df['population'] &lt; 4000]\n</code></pre> output<pre><code>population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n</code></pre>"},{"location":"dataframe/dataframe/filtering/#string","title":"String","text":"<p>Of course we can filter by strings. Follow up we've got a dataframe with some employee data:</p> employees<pre><code>def df = [\n    employees: ['Udo', 'John', 'Albert', 'Ronda'],\n    department: ['sales', 'it', 'sales', 'it'],\n    payroll: [10_000, 12_000, 11_000, 13_000]\n].toDataFrame(\"employees\")\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n    Albert  |       sales  |    11000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>Getting employees from sales department:</p> sales<pre><code>def salesPeople = df[df['department'] == 'sales']\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n    Albert  |       sales  |    11000  |\n</code></pre> <p>You can also use to filter by a list of possible choices:</p> in list<pre><code>def employeesByName = df[df['employees'] in ['Ronda', 'Udo']]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>You can even try by a regular expression. Lets look for employees with an 'o' in their name:</p> regex<pre><code>def employeesWithAnO = df[df['employees'] ==~ /.*o.*/ ]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n     Ronda  |          it  |    13000  |\n</code></pre>"},{"location":"dataframe/dataframe/filtering/#dates","title":"Dates","text":"<p>Of course in time series is crucial to allow searches by time frame.</p> dates<pre><code>// Using a given date as the beginning of our df dates series\ndef initialDate = LocalDate.parse('01/01/2000', 'dd/MM/yyyy')\n\n// a dataframe containing the simulation of bicycles rented through 2000\ndef df = [\n    dates: (1..365).collect(initialDate::plusDays),\n    rented: (1..365).collect { new Random().nextInt(200) }\n].toDataFrame(\"rented bicycles 2000\")\n</code></pre> output<pre><code>rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-01-02  |      41  |\n2000-01-03  |      47  |\n2000-01-04  |      27  |\n2000-01-05  |      95  |\n2000-01-06  |      30  |\n2000-01-07  |     162  |\n2000-01-08  |      52  |\n2000-01-09  |     197  |\n2000-01-10  |     125  |\n2000-01-11  |      15  |\n       ...  |     ...  |\n</code></pre> <p>What if we'd like to get only those records of december 2000 ?</p> after<pre><code>def lastMonth = df[df['dates'] &gt;= LocalDate.parse('01/12/2000', 'dd/MM/yyyy')]\n</code></pre> output<pre><code>  rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-12-01  |     104  |\n2000-12-02  |     193  |\n2000-12-03  |     107  |\n2000-12-04  |     108  |\n2000-12-05  |     193  |\n2000-12-06  |     165  |\n2000-12-07  |      82  |\n2000-12-08  |      77  |\n2000-12-09  |     176  |\n2000-12-10  |     158  |\n       ...  |     ...  |\n2000-12-31  |     150  |\n</code></pre>"},{"location":"dataframe/dataframe/filtering/#and","title":"&amp; and |","text":"<p>You can combine any predicate with the operators <code>&amp;</code> and <code>|</code>. For example, lets filter by all years after 1995 AND entries with population less or equals 8000:</p> AND<pre><code>def result1 = df[df['years'] &gt; 1995 &amp; df['population'] &lt;= 8000]\n</code></pre> <p>Now filter by years less or equals to 1995 OR entries with population greater than 9000.</p> OR<pre><code>def result2 = df[df['years'] &lt;= 1998 | df['population'] &gt; 9000]\n</code></pre>"},{"location":"dataframe/dataframe/filtering/#summary","title":"Summary","text":"<p>Here you have the tables with the supported operators:</p> <p>Arithmetic</p> Left Right Operator Example Status Series Series + <code>df['a'] + df['b']</code> Yes Series Series - <code>df['a'] - df['b]</code> Yes Series Object + <code>df['a'] + 1</code> Yes Series Object - <code>df['a'] - 1</code> Yes Series Object * <code>df['a'] * 2</code> Yes Series Object / <code>df['a'] / 2</code> Yes <p>filtering operators</p> Type Operator Example Status String == <code>df['a'] == 'x'</code> Yes String != <code>df['a'] != 'x'</code> Yes String ==~ <code>df['a'] ==~ /.*/</code> Yes String in <code>df['a'] in ['x']</code> Yes Number == <code>df['a'] == 1</code> Yes Number != <code>df['a'] != 1</code> Yes Number &gt; <code>df['a'] &gt; 1</code> Yes Number &gt;= <code>df['a'] &gt;= 1</code> Yes Number &lt; <code>df['a'] &lt; 1</code> Yes Number &lt;= <code>df['a'] &lt;= 1</code> Yes LocalDate &gt; <code>df['a'] &gt; date</code> Yes LocalDate &gt;= <code>df['a'] &gt;= date</code> Yes LocalDate &lt; <code>df['a'] &lt; date</code> Yes LocalDate &lt;= <code>df['a'] &lt;= date</code> Yes"},{"location":"dataframe/dataframe/import_export/","title":"Import export","text":""},{"location":"dataframe/dataframe/import_export/#import-export","title":"Import / Export","text":""},{"location":"dataframe/dataframe/import_export/#csv","title":"CSV","text":""},{"location":"dataframe/dataframe/import_export/#reading","title":"Reading","text":"<p>You can read csv files via <code>Underdog.df().read_csv(...)</code> method. Here we are importing a csv files containing tornado incidents in the USA:</p> import csv<pre><code>DataFrame dataframe = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> output<pre><code>                               tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |      |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |      |\n1950-01-03  |  11:55:00  |     IL  |         2  |      3  |         3  |      |\n1950-01-03  |  16:00:00  |     OH  |         1  |      1  |         1  |      |\n1950-01-13  |  05:25:00  |     AR  |         1  |      3  |         1  |      |\n1950-01-25  |  19:30:00  |     MO  |         2  |      2  |         5  |      |\n1950-01-25  |  21:00:00  |     IL  |         3  |      2  |         0  |      |\n1950-01-26  |  18:00:00  |     TX  |         1  |      2  |         2  |      |\n1950-02-11  |  13:10:00  |     TX  |         2  |      2  |         0  |      |\n       ...  |       ...  |    ...  |       ...  |    ...  |       ...  |  ... |\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#separator","title":"Separator","text":"<p>By default the csv reader assumes the csv file is using comma (,) as the separator character, but you can provide a custom separator. For example the following csv file content:</p> separator<pre><code>name;age\nLorna;34\nYule;63\nTom;28\n</code></pre> <p>Can be read by using the sep argument:</p> custom separator<pre><code>def dataframe = Underdog.df().read_csv(filePath, sep: \";\")\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#duplicated-names","title":"Duplicated names","text":"<p>Sometimes you can find a csv where columns are repeated, by default if you don't specify you allow repeated columns the import process will fail. Imagine we've got the following csv:</p> csv with repeated cols<pre><code>bronze,silver,gold,summer_total,bronze,silver,gold,winter_total\n1,2,1,4,1,1,1,3\n</code></pre> <p>To allow repeated columns you should set the <code>allowDuplicatedNames</code> flag to true.</p> allow repeated cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, allowedDuplicatedNames: true)\n</code></pre> <p>Then all repeated names will be prefixed in order with a number to avoid collisions:</p> output<pre><code>                                        io_repeated_cols.csv\n bronze |  silver |  gold |  summer_total |  bronze-2  |  silver-2  |  gold-2  |  winter_total|\n-----------------------------------------------------------------------------------------------\n      1 |       2 |     1 |             4 |         1  |         1  |       1  |           3  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#missing-values","title":"Missing values","text":"<p>If a csv file contains values which should be considered as well as missing values, we can pass this information before reading the csv file.</p> csv file with missing data<pre><code>from,to,id\nN/C,Madrid,123\nMadrid,Paris,124\nParis,London,125\nLondon,NONE,126\n</code></pre> <p>Here we're considering missing data the values N/C and NONE:</p> considering missing data<pre><code>def dataframe = Underdog.df().read_csv(filePath, nanValues: ['NONE', 'N/C'])\n</code></pre> <p>That will inform the reader to consider cells containing that value as missing values:</p> output<pre><code>io_custom_missing_data.csv\n  from   |    to    |  id   |\n-----------------------------\n         |  Madrid  |  123  |\n Madrid  |   Paris  |  124  |\n  Paris  |  London  |  125  |\n London  |          |  126  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#date-format","title":"Date format","text":"<p>If your csv files have a custom date format you can provide the date pattern as a parameter. Here we have a file with a custom format:</p> custom date format<pre><code>Date,Close\n2014-12-05 00:00:00+00:00,0.5267500281333923\n2014-12-08 00:00:00+00:00,0.5199999809265137\n2014-12-09 00:00:00+00:00,0.5182499885559082\n</code></pre> <p>Passing the pattern as parameter:</p> custom date format<pre><code>def dataframe = Underdog.df().read_csv(filePath, dateFormat: \"yyyy-MM-dd HH:mm:ss+00:00\")\n</code></pre> <p>Gives the following output:</p> output<pre><code>      io_custom_date_format.csv\n   Date     |        Close         |\n-------------------------------------\n2014-12-05  |  0.5267500281333923  |\n2014-12-08  |  0.5199999809265137  |\n2014-12-09  |  0.5182499885559082  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#skip-rowsfooter","title":"Skip rows/footer","text":"<p>If you're sure that there is data you'd like to avoid parsing, like nonsense data, you can skip parsing those rows. Check the following example:</p> csv file with comments<pre><code># some information about the data\n# col1: city\n# col2: weight\ncol1,col2\nNC,0\nNC,0\nNC,0\nNC,0\nMadrid,1\nParis,2\nNC,0\nNC,0\nNC,0\nNC,0\n# another comment here\n</code></pre> <p>There are lines we don't want to consider when creating our dataframe:</p> <ul> <li>comments in the beginning of the file (lines 1-3)</li> <li>comments in the end of the file (line 15)</li> <li>rows we don't want to parse because they don't add any meaningful information (4-8 and 11-14)</li> </ul> <p>To avoid parsing any of these lines we can instruct the csv reader to skip lines in the header and/or in the footer of the file:</p> skipping rows<pre><code>def dataframe = Underdog.df()\n    .read_csv(filePath,\n        header: false,    // not using first row as header\n        skipRows: 8,      // skipping rows at the beginning of the file\n        skipFooter: 4     // skipping rows at the end of the file\n    ).renameSeries(columns: ['city', 'id']) // renaming series names with the list passed as parameter\n</code></pre> output<pre><code>io_skipping_rows.csv\n city   |  id  |\n-----------------\nMadrid  |   1  |\n Paris  |   2  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#max-chars-x-col","title":"Max chars x col","text":"<p>You can instruct the csv reader to avoid parsing columns with more than a number of characters.</p> limiting col chars<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxCharsPerColumn: 20)\n</code></pre> <p>Warning</p> <p>If a column exceeds the number of characters the process will throw an exception</p>"},{"location":"dataframe/dataframe/import_export/#max-cols","title":"Max cols","text":"<p>You can instruct the csv reader to avoid parsing more than a given number of columns.</p> limiting number of cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxNumberOfColumns: 2)\n</code></pre> <p>Warning</p> <p>If the number of columns exceeds the number specified the process will throw an exception</p>"},{"location":"dataframe/dataframe/import_export_csv/","title":"Import export csv","text":""},{"location":"dataframe/dataframe/import_export_csv/#csv","title":"CSV","text":""},{"location":"dataframe/dataframe/import_export_csv/#reading","title":"Reading","text":"<p>You can read csv files via <code>Underdog.df().read_csv(...)</code> method. Here we are importing a csv files containing tornado incidents in the USA:</p> import csv<pre><code>DataFrame dataframe = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> output<pre><code>                               tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |      |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |      |\n1950-01-03  |  11:55:00  |     IL  |         2  |      3  |         3  |      |\n1950-01-03  |  16:00:00  |     OH  |         1  |      1  |         1  |      |\n1950-01-13  |  05:25:00  |     AR  |         1  |      3  |         1  |      |\n1950-01-25  |  19:30:00  |     MO  |         2  |      2  |         5  |      |\n1950-01-25  |  21:00:00  |     IL  |         3  |      2  |         0  |      |\n1950-01-26  |  18:00:00  |     TX  |         1  |      2  |         2  |      |\n1950-02-11  |  13:10:00  |     TX  |         2  |      2  |         0  |      |\n       ...  |       ...  |    ...  |       ...  |    ...  |       ...  |  ... |\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#separator","title":"Separator","text":"<p>By default the csv reader assumes the csv file is using comma (,) as the separator character, but you can provide a custom separator. For example the following csv file content:</p> separator<pre><code>name;age\nLorna;34\nYule;63\nTom;28\n</code></pre> <p>Can be read by using the sep argument:</p> custom separator<pre><code>def dataframe = Underdog.df().read_csv(filePath, sep: \";\")\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#duplicated-names","title":"Duplicated names","text":"<p>Sometimes you can find a csv where columns are repeated, by default if you don't specify you allow repeated columns the import process will fail. Imagine we've got the following csv:</p> csv with repeated cols<pre><code>bronze,silver,gold,summer_total,bronze,silver,gold,winter_total\n1,2,1,4,1,1,1,3\n</code></pre> <p>To allow repeated columns you should set the <code>allowDuplicatedNames</code> flag to true.</p> allow repeated cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, allowedDuplicatedNames: true)\n</code></pre> <p>Then all repeated names will be prefixed in order with a number to avoid collisions:</p> output<pre><code>                                        io_repeated_cols.csv\n bronze |  silver |  gold |  summer_total |  bronze-2  |  silver-2  |  gold-2  |  winter_total|\n-----------------------------------------------------------------------------------------------\n      1 |       2 |     1 |             4 |         1  |         1  |       1  |           3  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#missing-values","title":"Missing values","text":"<p>If a csv file contains values which should be considered as well as missing values, we can pass this information before reading the csv file.</p> csv file with missing data<pre><code>from,to,id\nN/C,Madrid,123\nMadrid,Paris,124\nParis,London,125\nLondon,NONE,126\n</code></pre> <p>Here we're considering missing data the values N/C and NONE:</p> considering missing data<pre><code>def dataframe = Underdog.df().read_csv(filePath, nanValues: ['NONE', 'N/C'])\n</code></pre> <p>That will inform the reader to consider cells containing that value as missing values:</p> output<pre><code>io_custom_missing_data.csv\n  from   |    to    |  id   |\n-----------------------------\n         |  Madrid  |  123  |\n Madrid  |   Paris  |  124  |\n  Paris  |  London  |  125  |\n London  |          |  126  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#date-format","title":"Date format","text":"<p>If your csv files have a custom date format you can provide the date pattern as a parameter. Here we have a file with a custom format:</p> custom date format<pre><code>Date,Close\n2014-12-05 00:00:00+00:00,0.5267500281333923\n2014-12-08 00:00:00+00:00,0.5199999809265137\n2014-12-09 00:00:00+00:00,0.5182499885559082\n</code></pre> <p>Passing the pattern as parameter:</p> custom date format<pre><code>def dataframe = Underdog.df().read_csv(filePath, dateFormat: \"yyyy-MM-dd HH:mm:ss+00:00\")\n</code></pre> <p>Gives the following output:</p> output<pre><code>      io_custom_date_format.csv\n   Date     |        Close         |\n-------------------------------------\n2014-12-05  |  0.5267500281333923  |\n2014-12-08  |  0.5199999809265137  |\n2014-12-09  |  0.5182499885559082  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#skip-rowsfooter","title":"Skip rows/footer","text":"<p>If you're sure that there is data you'd like to avoid parsing, like nonsense data, you can skip parsing those rows. Check the following example:</p> csv file with comments<pre><code># some information about the data\n# col1: city\n# col2: weight\ncol1,col2\nNC,0\nNC,0\nNC,0\nNC,0\nMadrid,1\nParis,2\nNC,0\nNC,0\nNC,0\nNC,0\n# another comment here\n</code></pre> <p>There are lines we don't want to consider when creating our dataframe:</p> <ul> <li>comments in the beginning of the file (lines 1-3)</li> <li>comments in the end of the file (line 15)</li> <li>rows we don't want to parse because they don't add any meaningful information (4-8 and 11-14)</li> </ul> <p>To avoid parsing any of these lines we can instruct the csv reader to skip lines in the header and/or in the footer of the file:</p> skipping rows<pre><code>def dataframe = Underdog.df()\n    .read_csv(filePath,\n        header: false,    // not using first row as header\n        skipRows: 8,      // skipping rows at the beginning of the file\n        skipFooter: 4     // skipping rows at the end of the file\n    ).renameSeries(columns: ['city', 'id']) // renaming series names with the list passed as parameter\n</code></pre> output<pre><code>io_skipping_rows.csv\n city   |  id  |\n-----------------\nMadrid  |   1  |\n Paris  |   2  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#max-chars-x-col","title":"Max chars x col","text":"<p>You can instruct the csv reader to avoid parsing columns with more than a number of characters.</p> limiting col chars<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxCharsPerColumn: 20)\n</code></pre> <p>Warning</p> <p>If a column exceeds the number of characters the process will throw an exception</p>"},{"location":"dataframe/dataframe/import_export_csv/#max-cols","title":"Max cols","text":"<p>You can instruct the csv reader to avoid parsing more than a given number of columns.</p> limiting number of cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxNumberOfColumns: 2)\n</code></pre> <p>Warning</p> <p>If the number of columns exceeds the number specified the process will throw an exception</p>"},{"location":"dataframe/dataframe/mapping/","title":"Mapping","text":""},{"location":"dataframe/dataframe/mapping/#mapping","title":"Mapping","text":"<p>TODO</p>"},{"location":"dataframe/dataframe/sorting/","title":"Sorting","text":""},{"location":"dataframe/dataframe/sorting/#sorting","title":"Sorting","text":"<p>TODO</p>"},{"location":"dataframe/series/","title":"Index","text":""},{"location":"dataframe/series/#series","title":"Series","text":""},{"location":"dataframe/series/#intro","title":"Intro","text":"<p>A Series object represents a named one-dimensional array. It also supports operations and statistical methods. It also has operations to deal with missing values. You can create a Series object from different sources:</p> create<pre><code>// from a range of numbers\ndef numbers = (1..4).toSeries(\"numbers\")\n\n// from a range of letters\ndef letters = ('A'..'C').toSeries(\"letters\")\n\n// from a list\ndef stuff = [1, 2, null, 3, 4].toSeries(\"stuff\")\n</code></pre> <p>You can use operator symbols to apply simple operations over the Series object:</p> operations<pre><code>// multiplying a series by a number\ndef doubleSeries = numbers * 2\n\n// multiplying a series by another series\ndef rowProduct = numbers * stuff.dropna()\n\n// dividing a series\ndef halves = stuff / 2\n\n// using custom transformation to create a new series\ndef custom = letters(String, String) { \"letter-$it\".toString() }\n</code></pre> <p>Sometimes you may want to analyze a given Series object by using statistical methods:</p> statistics<pre><code>def mean = doubleSeries.mean()\ndef max = doubleSeries.max()\ndef min = doubleSeries.min()\ndef avg = doubleSeries.avg()\n</code></pre> <p>You can find all statistical available methods in the <code>underdog.impl.extensions.SeriesStatsExtensions</code> class.</p>"},{"location":"dataframe/series/#creating","title":"Creating","text":"<p>Series are meant to be created from collections or as a transformation from another Series.</p> <p>The only way to create a Series from a collection is invoking the extension method <code>toSeries()</code> from a list:</p> collection extension<pre><code>Series series = [1, 2, 3].toSeries(\"numbers\")\n</code></pre> <p>Most of the time we will be dealing with a Series creation inside the scope of a Dataframe. Sometimes as the result of the transformation of another series, sometimes because we would like to fill a series from a constant value.</p> <p>Lets say we have a DataFrame with some Series:</p> sample dataframe<pre><code>def numbers = Underdog.df().from([numbers: 1..10], \"numbers\")\n</code></pre> output<pre><code> numbers\n numbers  |\n-----------\n       1  |\n       2  |\n       3  |\n       4  |\n       5  |\n       6  |\n       7  |\n       8  |\n       9  |\n      10  |\n</code></pre> <p>And we want to create a new series named by_two with the result of multiplying all numbers in the numbers series:</p> new series<pre><code>numbers['by_two'] = numbers['numbers'] * 2\n</code></pre> output<pre><code>       numbers\n numbers  |  by_two  |\n----------------------\n       1  |       2  |\n       2  |       4  |\n       3  |       6  |\n       4  |       8  |\n       5  |      10  |\n       6  |      12  |\n       7  |      14  |\n       8  |      16  |\n       9  |      18  |\n      10  |      20  |\n</code></pre> <p>You can also create a new Series inside a dataframe filling all rows with the same value:</p> series from value<pre><code>numbers['one'] = 1\n</code></pre> output<pre><code>           numbers\n numbers  |  by_two  |  one  |\n------------------------------\n       1  |       2  |    1  |\n       2  |       4  |    1  |\n       3  |       6  |    1  |\n       4  |       8  |    1  |\n       5  |      10  |    1  |\n       6  |      12  |    1  |\n       7  |      14  |    1  |\n       8  |      16  |    1  |\n       9  |      18  |    1  |\n      10  |      20  |    1  |\n</code></pre>"},{"location":"dataframe/series/#statistics","title":"Statistics","text":"<p>TODO</p>"},{"location":"dataframe/series/creation/","title":"Creation","text":""},{"location":"dataframe/series/creation/#creating","title":"Creating","text":"<p>Series are meant to be created from collections or as a transformation from another Series.</p> <p>The only way to create a Series from a collection is invoking the extension method <code>toSeries()</code> from a list:</p> collection extension<pre><code>Series series = [1, 2, 3].toSeries(\"numbers\")\n</code></pre> <p>Most of the time we will be dealing with a Series creation inside the scope of a Dataframe. Sometimes as the result of the transformation of another series, sometimes because we would like to fill a series from a constant value.</p> <p>Lets say we have a DataFrame with some Series:</p> sample dataframe<pre><code>def numbers = Underdog.df().from([numbers: 1..10], \"numbers\")\n</code></pre> output<pre><code> numbers\n numbers  |\n-----------\n       1  |\n       2  |\n       3  |\n       4  |\n       5  |\n       6  |\n       7  |\n       8  |\n       9  |\n      10  |\n</code></pre> <p>And we want to create a new series named by_two with the result of multiplying all numbers in the numbers series:</p> new series<pre><code>numbers['by_two'] = numbers['numbers'] * 2\n</code></pre> output<pre><code>       numbers\n numbers  |  by_two  |\n----------------------\n       1  |       2  |\n       2  |       4  |\n       3  |       6  |\n       4  |       8  |\n       5  |      10  |\n       6  |      12  |\n       7  |      14  |\n       8  |      16  |\n       9  |      18  |\n      10  |      20  |\n</code></pre> <p>You can also create a new Series inside a dataframe filling all rows with the same value:</p> series from value<pre><code>numbers['one'] = 1\n</code></pre> output<pre><code>           numbers\n numbers  |  by_two  |  one  |\n------------------------------\n       1  |       2  |    1  |\n       2  |       4  |    1  |\n       3  |       6  |    1  |\n       4  |       8  |    1  |\n       5  |      10  |    1  |\n       6  |      12  |    1  |\n       7  |      14  |    1  |\n       8  |      16  |    1  |\n       9  |      18  |    1  |\n      10  |      20  |    1  |\n</code></pre>"},{"location":"dataframe/series/intro/","title":"Intro","text":""},{"location":"dataframe/series/intro/#intro","title":"Intro","text":"<p>A Series object represents a named one-dimensional array. It also supports operations and statistical methods. It also has operations to deal with missing values. You can create a Series object from different sources:</p> create<pre><code>// from a range of numbers\ndef numbers = (1..4).toSeries(\"numbers\")\n\n// from a range of letters\ndef letters = ('A'..'C').toSeries(\"letters\")\n\n// from a list\ndef stuff = [1, 2, null, 3, 4].toSeries(\"stuff\")\n</code></pre> <p>You can use operator symbols to apply simple operations over the Series object:</p> operations<pre><code>// multiplying a series by a number\ndef doubleSeries = numbers * 2\n\n// multiplying a series by another series\ndef rowProduct = numbers * stuff.dropna()\n\n// dividing a series\ndef halves = stuff / 2\n\n// using custom transformation to create a new series\ndef custom = letters(String, String) { \"letter-$it\".toString() }\n</code></pre> <p>Sometimes you may want to analyze a given Series object by using statistical methods:</p> statistics<pre><code>def mean = doubleSeries.mean()\ndef max = doubleSeries.max()\ndef min = doubleSeries.min()\ndef avg = doubleSeries.avg()\n</code></pre> <p>You can find all statistical available methods in the <code>underdog.impl.extensions.SeriesStatsExtensions</code> class.</p>"},{"location":"dataframe/series/statistics/","title":"Statistics","text":""},{"location":"dataframe/series/statistics/#statistics","title":"Statistics","text":"<p>TODO</p>"},{"location":"development/","title":"Development","text":"<p>TODO</p>"},{"location":"graphs/","title":"Graphs","text":"<p>Underdog's Graphs module can be used for exploring graph theory problems.</p>"},{"location":"graphs/#tutorial","title":"Tutorial","text":""},{"location":"graphs/#prerequisites","title":"Prerequisites","text":""},{"location":"graphs/#dependencies","title":"Dependencies","text":"<p>The modules required to follow this tutorial are the <code>graphs</code> and <code>plots</code> modules:</p> GradleMavenGrapes <pre><code>// graph data and algorithms\nimplementation 'com.github.grooviter:underdog-graphs:VERSION'\n\n// graph visualization\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <pre><code>&lt;!-- graph data and algorithms --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-graphs&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- graph visualization --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grapes([\n    // graph data and algorithms\n    @Grab('com.github.grooviter:underdog-graphs:VERSION'),\n    // graph visualization\n    @Grab('com.github.grooviter:underdog-plots:VERSION')\n])\n</code></pre>"},{"location":"graphs/#creating-a-graph","title":"Creating a graph","text":"<p>Creating an empty graph without vertices and edges:</p> create a graph<pre><code>import underdog.Underdog\n\ndef graph = Underdog.graphs().graph(String) {\n    // vertices and edges here\n}\n</code></pre> <p>The <code>graph(Class)</code> informs what is the type of the vertices this graph is going to have. This time vertices in this graph will be of type <code>String</code> but vertices could be potentially of any type.</p>"},{"location":"graphs/#vertices-nodes","title":"Vertices / Nodes","text":"<p>Lets create a graph with two nodes without any edge between them:</p> <p> </p> <p>There are a couple of ways of adding more vertices to a graph. One is when creating the graph:</p> adding vertices at creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    vertex(\"A\")\n    vertex(\"B\")\n}\n</code></pre> <p>You can also add more vertices after the graph has been created:</p> adding vertices after creation<pre><code>def graph = Underdog.graphs().graph(String) {}\n\ngraph.addVertex(\"A\")\ngraph.addVertex(\"B\")\n</code></pre> <p>You can add simple type vertices, but you can also add more complex objects. Imagine we can add the relationships between employees in a given company. First lets define the <code>Employee</code> class:</p> Employee<pre><code>/*\n * @Canonical implements equals and hashcode among other things.\n * These methods will help the graph to id each node in the graph.\n*/\n@groovy.transform.Canonical\nstatic class Employee {\n    String name, department\n}\n</code></pre> <p>Now we can create a Graph and add relationships between employees:</p> Adding employees<pre><code>def john = new Employee(\"John\", \"Engineering\")\ndef peter = new Employee(\"Peter\", \"Engineering\")\ndef lisa = new Employee(\"Lisa\", \"Engineering\")\n\ndef graph = Underdog.graphs().graph(Employee) {\n    vertex(john)\n    vertex(peter)\n    vertex(lisa)\n}\n</code></pre>"},{"location":"graphs/#edges","title":"Edges","text":"<p>Graphs normally are not very useful without setting edges between nodes. Lets add an edge between two nodes:</p> <p> </p> <p>We can add vertices at creation time:</p> Adding edges at creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices first\n    vertex('A')\n    vertex('B')\n\n    // then adding edges between vertices\n    edge('A', 'B')\n}\n</code></pre> <p>You can omit adding vertices before adding edges if all vertices are going to be included in <code>edge(...)</code> method calls. The following example produces the same graph as the previous one:</p> avoid using vertex(...)<pre><code>def graph2 = Underdog.graphs().graph(String) {\n    edge('A', 'B')\n}\n</code></pre> <p>It's also possible to add edges after the graph has been created:</p> Adding edges after creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices 'A', 'B', 'C', 'D'\n    ('A'..'D').each(delegate::vertex)\n}\n\n// adding edge between 'A' and 'B'\ngraph.addEdge('A', 'B')\n</code></pre> <p>You can also add several edges at once using <code>edges(...)</code></p> <p> </p> adding several edges<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices first\n    ('A'..'D').each(delegate::vertex)\n\n    // then adding several edges\n    edges(\n        'A', 'B',\n        'B', 'C',\n        'C', 'D'\n    )\n}\n</code></pre> <p>We can at any point ask about how many vertices and edges there are in the graph:</p> shape<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('A'..'D').each(delegate::vertex)\n    edge('A', 'B')\n    edge('B', 'C')\n    edge('C', 'D')\n}\n\n// getting vertices and edges count\ndef (nVertices, nEdges) = graph.shape()\n\n// or just println shape\nprintln(graph.shape())\n</code></pre> <p>which prints:</p> output<pre><code>4 vertices X 3 edges\n</code></pre>"},{"location":"graphs/#elements-of-a-graph","title":"Elements of a graph","text":"<ul> <li>vertices</li> <li>edges</li> <li>adjacent</li> <li>degree</li> </ul> <p>Lets create a graph first and then ask for its elements:</p> graph<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..10).each(delegate::vertex)\n    edges(\n        1, 2,\n        1, 3,\n        3, 4\n    )\n}\n</code></pre> <p>The lets ask for its vertices, edges, neighbors.</p> graph elements<pre><code>//  getting graph vertices\ngraph.vertices.containsAll(1..10)\n\n// getting graph edges\ngraph.edges.size() == 3\n\n// getting neighbors of a specific vertex\ngraph.neighborsOf(1) == [2, 3]\n</code></pre>"},{"location":"graphs/#removing-elements","title":"Removing elements","text":"<p>Here there are some example on how to remove vertices and edges from a given graph. When using the operator minus (<code>-</code>) the result return the graph minus the element removed whereas the <code>removeXXX(,,,)</code> functions only return a boolean value: true if the element was successfully removed, or false if it couldn't be removed from graph.</p> removing elements<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..14).each(delegate::vertex)\n    edges(\n        3, 5,\n        5, 7,\n        7, 9,\n        9, 11,\n        11, 12,\n        12, 14\n    )\n}\n\n// removing vertices using - operator\ndef graph2 = graph - [2, 4, 6, 8, 10]\ndef graph3 = graph2 - 1\n\n// removing vertices using function\ngraph3.removeVertex(3)\ngraph3.removeAllVertices([5, 7])\n\n// removing edge using - operator\ndef graph4 = graph3 - graph3.edgesOf(9).first()\n\n// removing edges\ngraph4.removeEdge(graph4.edgesOf(11).first())\n// graph4.removeAllEdges(graph4.edgesOf(12)) &lt;--- this fails: ConcurrentModificationException\ngraph4.removeAllEdges(graph4.edgesOf(12).toList()) // &lt;--- this works\n</code></pre>"},{"location":"graphs/#graph-types","title":"Graph types","text":"<p>You can create different types of graphs. There are a couple of methods you can use:</p> graph types<pre><code>// graphs\ndef g = Underdog.graphs()\n\n// undirected weighted\ndef graph1 = g.graph(String)\n\n// directed weighted\ndef graph2 = g.digraph(String)\n\n// directed weighted pseudo graph\ndef graph3 = g.multidigraph(String)\n\n// weighted pseudo graph\ndef graph4 = g.multigraph(String)\n</code></pre> <p>Tip</p> <p>Because graphs in underdog are using JGraphT underneath you can always create an instance of any type of graph  directly using JGraphT api.</p>"},{"location":"graphs/#what-to-use-as-vertices","title":"What to use as vertices","text":"<p>You can use almost anything as a vertex. The only mandatory condition is that the graph must be able to distinguish between vertices. For that your vertex should implement both equals and hashcode methods. In Groovy you can use the <code>@Canonical</code> annotation to get that.</p>"},{"location":"graphs/#analyzing-graphs","title":"Analyzing graphs","text":"<p>The structure of the graph can be analyzed by using various functions.</p> graph<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('a'..'f').each(delegate::vertex)\n\n    edges(\n        'a', 'b',\n        'a', 'c',\n        'a', 'd',\n        'b', 'e',\n    )\n}\n</code></pre> <p>What is the shape of the graph ?</p> shape<pre><code>def (nVertices, nEdges) = graph.shape()\n</code></pre> <p>What is the clustering of the graph ?</p> clustering of the graph<pre><code>def graphClustering = graph.clusteringGlobal()\n</code></pre> <p>What is the clustering avg ?</p> clustering average<pre><code>def graphAvg = graph.clusteringAvg()\n</code></pre> <p>And what about the clustering of a given vertex ?</p> clustering of a vertex<pre><code>def vertexClustering = graph.clusteringOf('a')\n</code></pre> <p>What is the vertex with max degree ?</p> max degree<pre><code>String maxDegreeVertex = graph.maxDegree()\n</code></pre> <p>Lets say we want to sort vertices by degree in descending order:</p> sort vertices by degree (desc)<pre><code>def sortedVertices = graph.vertices.sort { -graph.degreeOf(it) }\n</code></pre>"},{"location":"graphs/#traversal","title":"Traversal","text":"<p>To traverse a graph you can directly access the vertices or edges sets and use normal Groovy/Java mechanisms. In the following example we are looking in all vertices to find all vertices having the number two as a neighbor</p> collections (vertices)<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..10).each(delegate::vertex)\n    edges(\n        1, 3,\n        1, 5,\n        1, 7,\n        1, 9,\n        1, 2  // &lt;--- looking here\n    )\n}\n\ndef answer1 = graph.vertices.findAll {\n    graph.neighborsOf(it).any { it == 2 }\n}\n</code></pre> output<pre><code>1\n</code></pre> <p>Now in the next example we are exploring boss-employee relationships and we'd like to find all the bosses names:</p> collections (edges)<pre><code>def anna = new Person(\"Anna\", 24)\ndef chris = new Person(\"Chris\", 26)\ndef paul = new Person(\"Paul\", 30)\ndef john = new Person(\"John\", 28)\n\ndef employees = Underdog.graphs().graph(Person) {\n    // adding people\n    [anna, chris, paul, john].each(delegate::vertex)\n    edge(chris, paul, \"boss\")\n    edge(anna, john, \"boss\")\n    edge(chris, anna, \"mentor\")\n}\n\ndef bossesNames = employees.edges                        // search in all edges where...\n    .findAll { it.relation == 'boss' }            // boss-employee relationship\n    .collect { employees.verticesOf(it)[0].name } // get only the boss name\n</code></pre> <p>Sometimes the graph could be bigger and more complex and we could benefit from using breadthFirst (*) or depthFirst (**) algorithms:</p> depthFirst<pre><code>def anna = new Person(\"Anna\", 24)\ndef chris = new Person(\"Chris\", 26)\ndef paul = new Person(\"Paul\", 30)\ndef john = new Person(\"John\", 28)\n\ndef dates = Underdog.graphs().graph(Person){\n    // adding people\n    [anna, chris, paul, john].each(delegate::vertex)\n\n    // adding dates\n    edges(\n        anna, paul,\n        anna, chris,\n        chris, paul,\n        anna, john\n    )\n}\n\ndef answer = dates.'**'.find { Person person -&gt;\n    // less than 30 years\n    person.age &lt; 30 &amp;&amp;\n    // have dated John\n    dates.neighborsOf(person).any { p -&gt; p.name == \"John\"}\n}\n</code></pre>"},{"location":"graphs/#introspection","title":"Introspection","text":"<p>You can ask the tree for different attributes such as the degrees:</p> max degree<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('A'..'C').each {\n        vertex(it)\n    }\n\n    edge('A', 'B')\n    edge('A', 'C')\n}\n\ndef node = graph.maxDegree()\n</code></pre> <p>Imagine a more complex example where our nodes are beans. Here we have a class representing a person:</p> Person<pre><code>class Person {\n    String name\n    Integer age\n\n    // this is used for destructuring eg:\n    // def (name, age) = somePerson\n    //       ^     ^\n    //       |     |\n    //       0     1\n    Object getAt(Integer index) {\n        return [name, age][index]\n    }\n}\n</code></pre> <p>This class implements the method <code>getAt(int)</code> which allows to extract the object attribute values via  Groovy destructuring. That would become handy later on.</p> <p>Note</p> <p>You can learn more about Groovy destructuring in the Groovy docs</p> <p>We create the graph:</p> Graph using complex objects<pre><code>def john = new Person(name: \"John\", age: 23)\ndef elsa = new Person (name: \"Elsa\", age: 32)\ndef raul = new Person(name: \"Raul\", age: 28)\n\ndef graph = Underdog.graphs().digraph(Person) {\n    vertex(john)\n    vertex(elsa)\n    vertex(raul)\n\n    edge(john, elsa)\n    edge(john, raul)\n}\n</code></pre> <p>Then look for the person with more relationship and extract that person name and age:</p> Person with more relationships<pre><code>def (name, age) = graph.maxDegree()\n</code></pre> <p>Do you remember we implemented destructuring for the Person class ? Here the <code>maxDegree()</code> function returns an instance of type Person, therefore we can access the properties name and age using destructuring knowing that the property in index 0 is the name and the property in index 1 is age.</p>"},{"location":"graphs/#distances","title":"Distances","text":"<p>Sometimes comes handy to know what is the shortest path from one node to another. It could be anything: How many people do I have to meet to meet a specific person ? How many cities do I have to visit before getting to a specific location ? ...etc</p> <p>Here's a very naive example with cities. The following graph contains cities and the edges represent how many kilometers there are between them.</p> <p>We'd like to use the graph as a route planner to get from point A to B. First we need to create our graph:</p> cities distances graph<pre><code>def distances = Underdog.graphs().graph(String) {\n    [\"Madrid\", \"Guadalajara\", \"Cuenca\", \"Zaragoza\", \"Teruel\", \"Castellon\"].each(delegate::vertex)\n    edge(\"Madrid\", \"Guadalajara\", weight: 66.4)\n    edge(\"Madrid\", \"Salamanca\", weight: 210)\n    edge(\"Guadalajara\", \"Zaragoza\", weight: 256.9)\n    edge(\"Zaragoza\", \"Cuenca\", weight: 290.2)\n    edge(\"Cuenca\", \"Teruel\", weight: 147.9)\n    edge(\"Teruel\", \"Castellon\", weight: 144.2)\n}\n</code></pre> <p> </p> <p>See how the weight of the edges represent the km between then. Then I'd like to know how many kilometers I'm going to drive if I'd like to go from Teruel to Madrid:</p> kms<pre><code>def kmsDriven = distances\n    .shortestPathEdges(\"Teruel\", \"Madrid\")\n    .sum { it.weight }\n</code></pre> <p>I've taken the edges and I've added up all weights to get the whole trip in kms:</p> output<pre><code>761.4\n</code></pre> <p>To get the city names I'm asking for the shortest path getting the vertices this time:</p> cities visited<pre><code>def citiesVisited = distances.shortestPathVertices(\"Teruel\", \"Madrid\")\n</code></pre> output<pre><code>[\"Teruel\", \"Cuenca\", \"Zaragoza\", \"Guadalajara\", \"Madrid\"]\n</code></pre> <p> </p> <p>If you are not sure whether you are interested in vertices or edges, or maybe you are interested in both, just use <code>shortestPath</code>:</p> shortestPath<pre><code>def shortestPath = distances.shortestPath(\"Teruel\", \"Madrid\")\n</code></pre> <p>And access the resulting object:</p> shortestPath attributes<pre><code>// kms\nshortestPath.weight == 761.4\n\n// steps (edges)\nshortestPath.length == 4\nshortestPath.vertexList == [\"Teruel\", \"Cuenca\", \"Zaragoza\", \"Guadalajara\", \"Madrid\"] // cities (vertices)\n</code></pre>"},{"location":"graphs/#operators","title":"Operators","text":"<p>You can apply arithmetic operations over graphs. Here for example you can apply a union operation over two graphs using the <code>+</code> operator:</p> merging graphs<pre><code>def names1 = Underdog.graphs().graph(String) {\n    [\"John\", \"Lisa\", \"Robert\"].each(delegate::vertex)\n}\n\ndef names2 = Underdog.graphs().graph(String) {\n    [\"Anna\", \"Vesper\", \"Tania\"].each(delegate::vertex)\n}\n\ndef names3 = names1 + names2\n</code></pre> <p>You can check how the result has all the vertices from the previous merged graphs:</p> output<pre><code>assert names3.vertices == [\"John\", \"Lisa\", \"Robert\", \"Anna\", \"Vesper\", \"Tania\"] as Set\n</code></pre>"},{"location":"graphs/distances/","title":"Distances","text":""},{"location":"graphs/distances/#distances","title":"Distances","text":"<p>Sometimes comes handy to know what is the shortest path from one node to another. It could be anything: How many people do I have to meet to meet a specific person ? How many cities do I have to visit before getting to a specific location ? ...etc</p> <p>Here's a very naive example with cities. The following graph contains cities and the edges represent how many kilometers there are between them.</p> <p>We'd like to use the graph as a route planner to get from point A to B. First we need to create our graph:</p> cities distances graph<pre><code>def distances = Underdog.graphs().graph(String) {\n    [\"Madrid\", \"Guadalajara\", \"Cuenca\", \"Zaragoza\", \"Teruel\", \"Castellon\"].each(delegate::vertex)\n    edge(\"Madrid\", \"Guadalajara\", weight: 66.4)\n    edge(\"Madrid\", \"Salamanca\", weight: 210)\n    edge(\"Guadalajara\", \"Zaragoza\", weight: 256.9)\n    edge(\"Zaragoza\", \"Cuenca\", weight: 290.2)\n    edge(\"Cuenca\", \"Teruel\", weight: 147.9)\n    edge(\"Teruel\", \"Castellon\", weight: 144.2)\n}\n</code></pre> <p> </p> <p>See how the weight of the edges represent the km between then. Then I'd like to know how many kilometers I'm going to drive if I'd like to go from Teruel to Madrid:</p> kms<pre><code>def kmsDriven = distances\n    .shortestPathEdges(\"Teruel\", \"Madrid\")\n    .sum { it.weight }\n</code></pre> <p>I've taken the edges and I've added up all weights to get the whole trip in kms:</p> output<pre><code>761.4\n</code></pre> <p>To get the city names I'm asking for the shortest path getting the vertices this time:</p> cities visited<pre><code>def citiesVisited = distances.shortestPathVertices(\"Teruel\", \"Madrid\")\n</code></pre> output<pre><code>[\"Teruel\", \"Cuenca\", \"Zaragoza\", \"Guadalajara\", \"Madrid\"]\n</code></pre> <p> </p> <p>If you are not sure whether you are interested in vertices or edges, or maybe you are interested in both, just use <code>shortestPath</code>:</p> shortestPath<pre><code>def shortestPath = distances.shortestPath(\"Teruel\", \"Madrid\")\n</code></pre> <p>And access the resulting object:</p> shortestPath attributes<pre><code>// kms\nshortestPath.weight == 761.4\n\n// steps (edges)\nshortestPath.length == 4\nshortestPath.vertexList == [\"Teruel\", \"Cuenca\", \"Zaragoza\", \"Guadalajara\", \"Madrid\"] // cities (vertices)\n</code></pre>"},{"location":"graphs/introspection/","title":"Introspection","text":""},{"location":"graphs/introspection/#introspection","title":"Introspection","text":"<p>You can ask the tree for different attributes such as the degrees:</p> max degree<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('A'..'C').each {\n        vertex(it)\n    }\n\n    edge('A', 'B')\n    edge('A', 'C')\n}\n\ndef node = graph.maxDegree()\n</code></pre> <p>Imagine a more complex example where our nodes are beans. Here we have a class representing a person:</p> Person<pre><code>class Person {\n    String name\n    Integer age\n\n    // this is used for destructuring eg:\n    // def (name, age) = somePerson\n    //       ^     ^\n    //       |     |\n    //       0     1\n    Object getAt(Integer index) {\n        return [name, age][index]\n    }\n}\n</code></pre> <p>This class implements the method <code>getAt(int)</code> which allows to extract the object attribute values via  Groovy destructuring. That would become handy later on.</p> <p>Note</p> <p>You can learn more about Groovy destructuring in the Groovy docs</p> <p>We create the graph:</p> Graph using complex objects<pre><code>def john = new Person(name: \"John\", age: 23)\ndef elsa = new Person (name: \"Elsa\", age: 32)\ndef raul = new Person(name: \"Raul\", age: 28)\n\ndef graph = Underdog.graphs().digraph(Person) {\n    vertex(john)\n    vertex(elsa)\n    vertex(raul)\n\n    edge(john, elsa)\n    edge(john, raul)\n}\n</code></pre> <p>Then look for the person with more relationship and extract that person name and age:</p> Person with more relationships<pre><code>def (name, age) = graph.maxDegree()\n</code></pre> <p>Do you remember we implemented destructuring for the Person class ? Here the <code>maxDegree()</code> function returns an instance of type Person, therefore we can access the properties name and age using destructuring knowing that the property in index 0 is the name and the property in index 1 is age.</p>"},{"location":"graphs/operators/","title":"Operators","text":""},{"location":"graphs/operators/#operators","title":"Operators","text":"<p>You can apply arithmetic operations over graphs. Here for example you can apply a union operation over two graphs using the <code>+</code> operator:</p> merging graphs<pre><code>def names1 = Underdog.graphs().graph(String) {\n    [\"John\", \"Lisa\", \"Robert\"].each(delegate::vertex)\n}\n\ndef names2 = Underdog.graphs().graph(String) {\n    [\"Anna\", \"Vesper\", \"Tania\"].each(delegate::vertex)\n}\n\ndef names3 = names1 + names2\n</code></pre> <p>You can check how the result has all the vertices from the previous merged graphs:</p> output<pre><code>assert names3.vertices == [\"John\", \"Lisa\", \"Robert\", \"Anna\", \"Vesper\", \"Tania\"] as Set\n</code></pre>"},{"location":"graphs/traversal/","title":"Traversal","text":""},{"location":"graphs/traversal/#traversal","title":"Traversal","text":"<p>To traverse a graph you can directly access the vertices or edges sets and use normal Groovy/Java mechanisms. In the following example we are looking in all vertices to find all vertices having the number two as a neighbor</p> collections (vertices)<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..10).each(delegate::vertex)\n    edges(\n        1, 3,\n        1, 5,\n        1, 7,\n        1, 9,\n        1, 2  // &lt;--- looking here\n    )\n}\n\ndef answer1 = graph.vertices.findAll {\n    graph.neighborsOf(it).any { it == 2 }\n}\n</code></pre> output<pre><code>1\n</code></pre> <p>Now in the next example we are exploring boss-employee relationships and we'd like to find all the bosses names:</p> collections (edges)<pre><code>def anna = new Person(\"Anna\", 24)\ndef chris = new Person(\"Chris\", 26)\ndef paul = new Person(\"Paul\", 30)\ndef john = new Person(\"John\", 28)\n\ndef employees = Underdog.graphs().graph(Person) {\n    // adding people\n    [anna, chris, paul, john].each(delegate::vertex)\n    edge(chris, paul, \"boss\")\n    edge(anna, john, \"boss\")\n    edge(chris, anna, \"mentor\")\n}\n\ndef bossesNames = employees.edges                        // search in all edges where...\n    .findAll { it.relation == 'boss' }            // boss-employee relationship\n    .collect { employees.verticesOf(it)[0].name } // get only the boss name\n</code></pre> <p>Sometimes the graph could be bigger and more complex and we could benefit from using breadthFirst (*) or depthFirst (**) algorithms:</p> depthFirst<pre><code>def anna = new Person(\"Anna\", 24)\ndef chris = new Person(\"Chris\", 26)\ndef paul = new Person(\"Paul\", 30)\ndef john = new Person(\"John\", 28)\n\ndef dates = Underdog.graphs().graph(Person){\n    // adding people\n    [anna, chris, paul, john].each(delegate::vertex)\n\n    // adding dates\n    edges(\n        anna, paul,\n        anna, chris,\n        chris, paul,\n        anna, john\n    )\n}\n\ndef answer = dates.'**'.find { Person person -&gt;\n    // less than 30 years\n    person.age &lt; 30 &amp;&amp;\n    // have dated John\n    dates.neighborsOf(person).any { p -&gt; p.name == \"John\"}\n}\n</code></pre>"},{"location":"graphs/tutorial/","title":"Tutorial","text":""},{"location":"graphs/tutorial/#tutorial","title":"Tutorial","text":""},{"location":"graphs/tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"graphs/tutorial/#dependencies","title":"Dependencies","text":"<p>The modules required to follow this tutorial are the <code>graphs</code> and <code>plots</code> modules:</p> GradleMavenGrapes <pre><code>// graph data and algorithms\nimplementation 'com.github.grooviter:underdog-graphs:VERSION'\n\n// graph visualization\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <pre><code>&lt;!-- graph data and algorithms --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-graphs&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- graph visualization --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grapes([\n    // graph data and algorithms\n    @Grab('com.github.grooviter:underdog-graphs:VERSION'),\n    // graph visualization\n    @Grab('com.github.grooviter:underdog-plots:VERSION')\n])\n</code></pre>"},{"location":"graphs/tutorial/#creating-a-graph","title":"Creating a graph","text":"<p>Creating an empty graph without vertices and edges:</p> create a graph<pre><code>import underdog.Underdog\n\ndef graph = Underdog.graphs().graph(String) {\n    // vertices and edges here\n}\n</code></pre> <p>The <code>graph(Class)</code> informs what is the type of the vertices this graph is going to have. This time vertices in this graph will be of type <code>String</code> but vertices could be potentially of any type.</p>"},{"location":"graphs/tutorial/#vertices-nodes","title":"Vertices / Nodes","text":"<p>Lets create a graph with two nodes without any edge between them:</p> <p> </p> <p>There are a couple of ways of adding more vertices to a graph. One is when creating the graph:</p> adding vertices at creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    vertex(\"A\")\n    vertex(\"B\")\n}\n</code></pre> <p>You can also add more vertices after the graph has been created:</p> adding vertices after creation<pre><code>def graph = Underdog.graphs().graph(String) {}\n\ngraph.addVertex(\"A\")\ngraph.addVertex(\"B\")\n</code></pre> <p>You can add simple type vertices, but you can also add more complex objects. Imagine we can add the relationships between employees in a given company. First lets define the <code>Employee</code> class:</p> Employee<pre><code>/*\n * @Canonical implements equals and hashcode among other things.\n * These methods will help the graph to id each node in the graph.\n*/\n@groovy.transform.Canonical\nstatic class Employee {\n    String name, department\n}\n</code></pre> <p>Now we can create a Graph and add relationships between employees:</p> Adding employees<pre><code>def john = new Employee(\"John\", \"Engineering\")\ndef peter = new Employee(\"Peter\", \"Engineering\")\ndef lisa = new Employee(\"Lisa\", \"Engineering\")\n\ndef graph = Underdog.graphs().graph(Employee) {\n    vertex(john)\n    vertex(peter)\n    vertex(lisa)\n}\n</code></pre>"},{"location":"graphs/tutorial/#edges","title":"Edges","text":"<p>Graphs normally are not very useful without setting edges between nodes. Lets add an edge between two nodes:</p> <p> </p> <p>We can add vertices at creation time:</p> Adding edges at creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices first\n    vertex('A')\n    vertex('B')\n\n    // then adding edges between vertices\n    edge('A', 'B')\n}\n</code></pre> <p>You can omit adding vertices before adding edges if all vertices are going to be included in <code>edge(...)</code> method calls. The following example produces the same graph as the previous one:</p> avoid using vertex(...)<pre><code>def graph2 = Underdog.graphs().graph(String) {\n    edge('A', 'B')\n}\n</code></pre> <p>It's also possible to add edges after the graph has been created:</p> Adding edges after creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices 'A', 'B', 'C', 'D'\n    ('A'..'D').each(delegate::vertex)\n}\n\n// adding edge between 'A' and 'B'\ngraph.addEdge('A', 'B')\n</code></pre> <p>You can also add several edges at once using <code>edges(...)</code></p> <p> </p> adding several edges<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices first\n    ('A'..'D').each(delegate::vertex)\n\n    // then adding several edges\n    edges(\n        'A', 'B',\n        'B', 'C',\n        'C', 'D'\n    )\n}\n</code></pre> <p>We can at any point ask about how many vertices and edges there are in the graph:</p> shape<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('A'..'D').each(delegate::vertex)\n    edge('A', 'B')\n    edge('B', 'C')\n    edge('C', 'D')\n}\n\n// getting vertices and edges count\ndef (nVertices, nEdges) = graph.shape()\n\n// or just println shape\nprintln(graph.shape())\n</code></pre> <p>which prints:</p> output<pre><code>4 vertices X 3 edges\n</code></pre>"},{"location":"graphs/tutorial/#elements-of-a-graph","title":"Elements of a graph","text":"<ul> <li>vertices</li> <li>edges</li> <li>adjacent</li> <li>degree</li> </ul> <p>Lets create a graph first and then ask for its elements:</p> graph<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..10).each(delegate::vertex)\n    edges(\n        1, 2,\n        1, 3,\n        3, 4\n    )\n}\n</code></pre> <p>The lets ask for its vertices, edges, neighbors.</p> graph elements<pre><code>//  getting graph vertices\ngraph.vertices.containsAll(1..10)\n\n// getting graph edges\ngraph.edges.size() == 3\n\n// getting neighbors of a specific vertex\ngraph.neighborsOf(1) == [2, 3]\n</code></pre>"},{"location":"graphs/tutorial/#removing-elements","title":"Removing elements","text":"<p>Here there are some example on how to remove vertices and edges from a given graph. When using the operator minus (<code>-</code>) the result return the graph minus the element removed whereas the <code>removeXXX(,,,)</code> functions only return a boolean value: true if the element was successfully removed, or false if it couldn't be removed from graph.</p> removing elements<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..14).each(delegate::vertex)\n    edges(\n        3, 5,\n        5, 7,\n        7, 9,\n        9, 11,\n        11, 12,\n        12, 14\n    )\n}\n\n// removing vertices using - operator\ndef graph2 = graph - [2, 4, 6, 8, 10]\ndef graph3 = graph2 - 1\n\n// removing vertices using function\ngraph3.removeVertex(3)\ngraph3.removeAllVertices([5, 7])\n\n// removing edge using - operator\ndef graph4 = graph3 - graph3.edgesOf(9).first()\n\n// removing edges\ngraph4.removeEdge(graph4.edgesOf(11).first())\n// graph4.removeAllEdges(graph4.edgesOf(12)) &lt;--- this fails: ConcurrentModificationException\ngraph4.removeAllEdges(graph4.edgesOf(12).toList()) // &lt;--- this works\n</code></pre>"},{"location":"graphs/tutorial/#graph-types","title":"Graph types","text":"<p>You can create different types of graphs. There are a couple of methods you can use:</p> graph types<pre><code>// graphs\ndef g = Underdog.graphs()\n\n// undirected weighted\ndef graph1 = g.graph(String)\n\n// directed weighted\ndef graph2 = g.digraph(String)\n\n// directed weighted pseudo graph\ndef graph3 = g.multidigraph(String)\n\n// weighted pseudo graph\ndef graph4 = g.multigraph(String)\n</code></pre> <p>Tip</p> <p>Because graphs in underdog are using JGraphT underneath you can always create an instance of any type of graph  directly using JGraphT api.</p>"},{"location":"graphs/tutorial/#what-to-use-as-vertices","title":"What to use as vertices","text":"<p>You can use almost anything as a vertex. The only mandatory condition is that the graph must be able to distinguish between vertices. For that your vertex should implement both equals and hashcode methods. In Groovy you can use the <code>@Canonical</code> annotation to get that.</p>"},{"location":"graphs/tutorial/#analyzing-graphs","title":"Analyzing graphs","text":"<p>The structure of the graph can be analyzed by using various functions.</p> graph<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('a'..'f').each(delegate::vertex)\n\n    edges(\n        'a', 'b',\n        'a', 'c',\n        'a', 'd',\n        'b', 'e',\n    )\n}\n</code></pre> <p>What is the shape of the graph ?</p> shape<pre><code>def (nVertices, nEdges) = graph.shape()\n</code></pre> <p>What is the clustering of the graph ?</p> clustering of the graph<pre><code>def graphClustering = graph.clusteringGlobal()\n</code></pre> <p>What is the clustering avg ?</p> clustering average<pre><code>def graphAvg = graph.clusteringAvg()\n</code></pre> <p>And what about the clustering of a given vertex ?</p> clustering of a vertex<pre><code>def vertexClustering = graph.clusteringOf('a')\n</code></pre> <p>What is the vertex with max degree ?</p> max degree<pre><code>String maxDegreeVertex = graph.maxDegree()\n</code></pre> <p>Lets say we want to sort vertices by degree in descending order:</p> sort vertices by degree (desc)<pre><code>def sortedVertices = graph.vertices.sort { -graph.degreeOf(it) }\n</code></pre>"},{"location":"ml/","title":"ML","text":"<p>Underdog's ML module can be used to explore machine learning problems.</p>"},{"location":"ml/#tutorial","title":"Tutorial","text":""},{"location":"ml/#prerequisites","title":"Prerequisites","text":""},{"location":"ml/#dependencies","title":"Dependencies","text":"<p>The modules required to follow this tutorial are the <code>ml</code> and <code>plots</code> modules:</p> GradleMavenGrapes <pre><code>// machine learning\nimplementation 'com.github.grooviter:underdog-ml:VERSION'\n\n// plots\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <pre><code>&lt;!-- machine learning --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-ml&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- plots --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grapes([\n    // machine learning\n    @Grab('com.github.grooviter:underdog-ml:VERSION'),\n    // plots\n    @Grab('com.github.grooviter:underdog-plots:VERSION')\n])\n</code></pre>"},{"location":"ml/#data","title":"Data","text":"<p>You can find the data used in this tutorial here</p>"},{"location":"ml/#introduction","title":"Introduction","text":"<p>Info</p> <p>The current tutorial follows the Tablesaw Moneyball tutorial but using Underdog's dataframe and ml modules.</p> <p>In baseball, you make the playoffs by winning more games than your rivals, but you can\u2019t control the number of games your rivals win. How should you proceed? The A\u2019s needed to find controllable variables that affected their likelihood of making the playoffs.</p> <p>Specifically, they wanted to know how to spend their salary dollars to produce the most wins. Statistics like \"Batting Average\" are available for individual players so if you knew Batting Average had the greatest impact, you can trade for players with high batting averages, and thus improve your odds of success.</p> <p>To connect player stats to making the playoffs, they systematically decomposed their high-level goal. They started by asking how many wins they\u2019d need to make the playoffs. They decided that 95 wins would give them a strong chance. Here\u2019s how we might check that assumption using Underdog.</p> <p>The tutorial also tries to follow the iterative process:</p> <pre><code>flowchart LR\n    subgraph representation\n    a[Data Analysis]--&gt;b[Algorithm selection]\n    end\n    subgraph evaluation\n    b--&gt;c[Model Training]\n    c--&gt;d[Model Testing]\n    d-- ITERATION --&gt;a\n    end\n</code></pre>"},{"location":"ml/#analyzing-data","title":"Analyzing data","text":"<p>To connect player stats to making the playoffs, they systematically decomposed their high-level goal. They started by asking how many wins they\u2019d need to make the playoffs. They decided that 95 wins would give them a strong chance. Here\u2019s how we might check that assumption using Underdog. First lets load the data:</p> loading data<pre><code>import underdog.Underdog\n\ndef data = Underdog.df().read_csv(\"src/test/resources/data/baseball.csv\")\n</code></pre> <p>Lets take only data before 2002:</p> filtering<pre><code>data = data[data[\"year\"] &lt; 2002]\n</code></pre> <p>We can check the assumption visually by plotting wins per year in a way that separates the teams who make the playoffs from those who don\u2019t. This code produces the chart below:</p> playoffs<pre><code>def figure = Underdog\n    .plots()\n    .scatter(\n        data['W'],\n        data['year'],\n        group: data['playoffs'],\n        title: 'Regular seasons wins by year')\n\nfigure.show()\n</code></pre> <p>The Series <code>data['playoffs']</code> represents whether the team made it to the playoffs (1) or it didn't (0).</p>"},{"location":"ml/#preparing-data","title":"Preparing data","text":"<p>Unfortunately, you can\u2019t directly control the number of games you win. We need to go deeper. At the next level, we hypothesize that the number of wins can be predicted by the number of Runs Scored during the season, combined with the number of Runs Allowed.</p> <p>To check this assumption we compute Run Difference (RD) as Runs Scored (RS) - Runs Allowed (RA)</p> difference<pre><code>data['RD'] = data['RS'] - data['RA']\n</code></pre> <p>Now lets see if Run Difference is correlated with Wins. We use a scatter plot again:</p> correlation<pre><code>def figure = Underdog\n    .plots()\n    .scatter(\n        data['RD'],\n        data['W'],\n        title: 'Run difference vs Wins')\n\nfigure.show()\n</code></pre>"},{"location":"ml/#model-training","title":"Model training","text":"<p>Let\u2019s create our first predictive model using linear regression, with runDifference as the sole explanatory variable.  Here we use Ordinary Least Squares (OLS) regression model.</p> <p>Tip</p> <p>To know more about Ordinary Least Squares you can check out its definition in Wikipedia</p> Ordinary least square (OLS)<pre><code>// extracting features (X) and labels (y)\ndef X = data['RD'] as double[][]\ndef y = data['W'] as double[]\n\n// splitting between train and test datasets to avoid over fitting\ndef (xTrain, xTest, yTrain, yTest) = Underdog.ml().utils.trainTestSplit(X, y)\n\n// training the model\ndef winsModel = Underdog.ml().regression.ols(xTrain, yTrain)\n</code></pre> <p>If we print our \u201cwinsModel\u201d, it produces the output below:</p> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-14.4535     -2.5195      0.2255      2.9513     11.5651\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept          80.9257     0.1838   440.3014     0.0000 ***\nX0                  0.1038     0.0019    54.6362     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.9032 on 449 degrees of freedom\nMultiple R-squared: 0.8693,    Adjusted R-squared: 0.8690\nF-statistic: 2985.1152 on 2 and 449 DF,  p-value: 1.757e-200\n</code></pre> <p>If you\u2019re new to regression, here are some take-aways from the output:</p> <ul> <li>The R-squared of .88 can be interpreted to mean that roughly 88% of the variance in Wins can be explained by the Run Difference variable. The rest is determined by some combination of other variables and pure chance.</li> <li>The estimate for the Intercept is the average wins independent of Run Difference. In baseball, we have a 162 game season so we expect this value to be about 81, as it is.</li> <li>The estimate for the RD variable of .1, suggests that an increase of 10 in Run Difference, should produce about 1 additional win over the course of the season.</li> </ul> <p>Of course, this model is not simply descriptive. We can use it to make predictions. In the code below, we predict how many games we will win if we score 135 more runs than our opponents.  To do this, we pass an array of doubles, one for each explanatory variable in our model, to the predict() method. In this case, there\u2019s just one variable: run difference.</p> using prediction<pre><code>def prediction = winsModel.predict([135] as double[])\n</code></pre> output<pre><code>94.93591869149651\n</code></pre> <p>We\u2019d expect almost 95 wins when we outscore opponents by 135 runs.</p>"},{"location":"ml/#scoring-with-test-data","title":"Scoring with test data","text":"<p>If we want to check how the model is performing overall we can pick the testing datasets we kept aside from the model training and use them to get a measure on how well the model is predicting a new case.</p> <p>In this example we are using the R2 square metric.  In regression, the R2 score is a statistical measure of how well the regression predictions approximate the real data points.</p> <p>We're passing to the model the testing features (xTest) and comparing the model predictions with the test labels we have (yTest):</p> r2score<pre><code>// generating predictions for the test features\ndef predictions = winsModel.predict(xTest)\n\n// comparing predictions with the actual truth for those features\ndef r2score = Underdog.ml().metrics.r2Score(yTest, predictions)\n</code></pre> <p>Getting a result of:</p> output<pre><code>0.8895307770897616\n</code></pre>"},{"location":"ml/#modeling-runs-scored","title":"Modeling Runs Scored","text":"<p>It\u2019s time to go deeper again and see how we can model Runs Scored and Runs Allowed. The approach the A\u2019s took was to model Runs Scored using team On-base percent (OBP) and team Slugging Average (SLG). In Underdog, we write:</p> <pre><code>def X = data['OBP', 'SLG'] as double[][]\ndef y = data['RS'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef runsScored = ml.regression.ols(xTrain, yTrain)\n</code></pre> <p>Once again the first parameter takes a Underdog column containing the values we want to predict (Runs scored). The next two parameters take the explanatory variables OBP and SLG.</p> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-67.7289    -18.0586     -1.5988     16.8863     68.9436\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept        -846.1069    29.6301   -28.5556     0.0000 ***\nX0               2937.5751   141.7312    20.7264     0.0000 ***\nX1               1524.8355    65.4379    23.3020     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.4627 on 448 degrees of freedom\nMultiple R-squared: 0.9204,    Adjusted R-squared: 0.9201\nF-statistic: 2590.2693 on 3 and 448 DF,  p-value: 6.276e-247\n</code></pre> <p>Again we have a model with excellent explanatory power with an R-squared of 92. Now we\u2019ll check the model visually to see if it violates any assumptions. Our residuals should be normally distributed. We can use a histogram to verify:</p> histogram<pre><code>def histogram = Underdog\n    .plots()\n    .histogram(residuals.toList(), title: 'Runs Scored from OBP and SLG')\n\nhistogram.show()\n</code></pre> <p>It looks great.  It\u2019s also important to plot the predicted (or \u201cfitted\u201d) values against the residuals. We want to see if the model fits some values better than others, which will influence whether we can trust its predictions or not. Ideally, we want to see a cloud of random dots around zero on the y axis.</p> <p>Our Scatter class can create this plot directly from the model:</p> fitted vs residuals<pre><code>def modelResiduals = runsScored.residuals().toList()\ndef modelFitted = runsScored.fittedValues().toList()\n\ndef fittedVsResiduals = Underdog\n    .plots()\n    .scatter(modelFitted, modelResiduals,\n        title: \"Runs Scored from OBP and SLG\",\n        xLabel: \"Fitted\",\n        yLabel: \"Residuals\")\n\nfittedVsResiduals.show()\n</code></pre> <p>Again, the plot looks good.</p> <p>Let\u2019s review.  We\u2019ve created a model of baseball that predicts entry into the playoffs based on batting stats, with the influence of the variables as:</p> <pre><code>graph LR\n  A[SLG &amp; OBP] --&gt; B[Runs Scored];\n  B --&gt; C[Run Difference];\n  C --&gt; D[Regular Season Wins];</code></pre>"},{"location":"ml/#modeling-runs-allowed","title":"Modeling Runs Allowed","text":"<p>Of course, we haven\u2019t modeled the Runs Allowed side of Run Difference. We could use pitching and field stats to do this, but the A\u2019s cleverly used the same two variables (SLG and OBP), but now looked at how their opponent\u2019s performed against the A\u2019s. We could do the same as these data are encoded in the dataset as OOBP and OSLG.</p> SLG &amp; OBP<pre><code>def X = data['OOBP', 'OSLG'].dropna() as double[][]\ndef y = data['RA'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef runsAllowed = ml.regression.ols(xTrain, yTrain)\n</code></pre> print<pre><code>println(runsAllowed)\n</code></pre> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-82.1479     -8.9954      0.7291     15.7773     46.4004\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept        -822.7172    97.7824    -8.4138     0.0000 ***\nX0               2844.9158   524.5965     5.4231     0.0000 ***\nX1               1532.0857   286.8341     5.3414     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27.7020 on 42 degrees of freedom\nMultiple R-squared: 0.8985,    Adjusted R-squared: 0.8936\nF-statistic: 185.8286 on 3 and 42 DF,  p-value: 1.377e-21\n</code></pre> <p>This model also looks good, but you\u2019d want to look at the plots again, and do other checking as well. Checking the predictive variables for collinearity is always good.</p> <p>Finally, we can tie this all together and see how well wins is predicted when we consider both offensive and defensive stats.</p> regression<pre><code>def X = data[\"OOBP\", \"OBP\", \"OSLG\", \"SLG\"].dropna() as double[][]\ndef y = data['W'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef winsFinal = ml.regression.ols(xTrain, yTrain)\n</code></pre>"},{"location":"ml/#the-as-in-2001","title":"The A's in 2001","text":"<p>For fun, I decided to see what the model predicts for the 2001 A\u2019s. First, I got the independent variables for the A\u2019s in that year.</p> A's in 2001<pre><code>def asIn2001 = data[\n    data['team'] == 'OAK' &amp;\n    data['year'] == 2001].loc[__, [\"year\", \"OOBP\", \"OBP\", \"OSLG\", \"SLG\"]]\n</code></pre> output<pre><code>                 baseball.csv\nYear  |  OOBP   |   OBP   |  OSLG  |   SLG   |\n-----------------------------------------------\n2001  |  0.308  |  0.345  |  0.38  |  0.439  |\n</code></pre> <p>Now we get the prediction:</p> A's in 2001<pre><code>double[][] values = asIn2001.loc[__, [\"OOBP\", \"OBP\", \"OSLG\", \"SLG\"]] as double[][]\ndouble[] value = winsFinal.predict(values);\n</code></pre> output<pre><code>102.22837899017894\n</code></pre> <p>The model predicted that the 2001 A\u2019s would win 102 games given their slugging and On-Base stats. They won 103.</p>"},{"location":"ml/#extensions","title":"Extensions","text":"<p>TODO</p>"},{"location":"ml/extensions/","title":"Extensions","text":""},{"location":"ml/extensions/#extensions","title":"Extensions","text":"<p>TODO</p>"},{"location":"ml/tutorial/","title":"Tutorial","text":""},{"location":"ml/tutorial/#tutorial","title":"Tutorial","text":""},{"location":"ml/tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"ml/tutorial/#dependencies","title":"Dependencies","text":"<p>The modules required to follow this tutorial are the <code>ml</code> and <code>plots</code> modules:</p> GradleMavenGrapes <pre><code>// machine learning\nimplementation 'com.github.grooviter:underdog-ml:VERSION'\n\n// plots\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <pre><code>&lt;!-- machine learning --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-ml&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- plots --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grapes([\n    // machine learning\n    @Grab('com.github.grooviter:underdog-ml:VERSION'),\n    // plots\n    @Grab('com.github.grooviter:underdog-plots:VERSION')\n])\n</code></pre>"},{"location":"ml/tutorial/#data","title":"Data","text":"<p>You can find the data used in this tutorial here</p>"},{"location":"ml/tutorial/#introduction","title":"Introduction","text":"<p>Info</p> <p>The current tutorial follows the Tablesaw Moneyball tutorial but using Underdog's dataframe and ml modules.</p> <p>In baseball, you make the playoffs by winning more games than your rivals, but you can\u2019t control the number of games your rivals win. How should you proceed? The A\u2019s needed to find controllable variables that affected their likelihood of making the playoffs.</p> <p>Specifically, they wanted to know how to spend their salary dollars to produce the most wins. Statistics like \"Batting Average\" are available for individual players so if you knew Batting Average had the greatest impact, you can trade for players with high batting averages, and thus improve your odds of success.</p> <p>To connect player stats to making the playoffs, they systematically decomposed their high-level goal. They started by asking how many wins they\u2019d need to make the playoffs. They decided that 95 wins would give them a strong chance. Here\u2019s how we might check that assumption using Underdog.</p> <p>The tutorial also tries to follow the iterative process:</p> <pre><code>flowchart LR\n    subgraph representation\n    a[Data Analysis]--&gt;b[Algorithm selection]\n    end\n    subgraph evaluation\n    b--&gt;c[Model Training]\n    c--&gt;d[Model Testing]\n    d-- ITERATION --&gt;a\n    end\n</code></pre>"},{"location":"ml/tutorial/#analyzing-data","title":"Analyzing data","text":"<p>To connect player stats to making the playoffs, they systematically decomposed their high-level goal. They started by asking how many wins they\u2019d need to make the playoffs. They decided that 95 wins would give them a strong chance. Here\u2019s how we might check that assumption using Underdog. First lets load the data:</p> loading data<pre><code>import underdog.Underdog\n\ndef data = Underdog.df().read_csv(\"src/test/resources/data/baseball.csv\")\n</code></pre> <p>Lets take only data before 2002:</p> filtering<pre><code>data = data[data[\"year\"] &lt; 2002]\n</code></pre> <p>We can check the assumption visually by plotting wins per year in a way that separates the teams who make the playoffs from those who don\u2019t. This code produces the chart below:</p> playoffs<pre><code>def figure = Underdog\n    .plots()\n    .scatter(\n        data['W'],\n        data['year'],\n        group: data['playoffs'],\n        title: 'Regular seasons wins by year')\n\nfigure.show()\n</code></pre> <p>The Series <code>data['playoffs']</code> represents whether the team made it to the playoffs (1) or it didn't (0).</p>"},{"location":"ml/tutorial/#preparing-data","title":"Preparing data","text":"<p>Unfortunately, you can\u2019t directly control the number of games you win. We need to go deeper. At the next level, we hypothesize that the number of wins can be predicted by the number of Runs Scored during the season, combined with the number of Runs Allowed.</p> <p>To check this assumption we compute Run Difference (RD) as Runs Scored (RS) - Runs Allowed (RA)</p> difference<pre><code>data['RD'] = data['RS'] - data['RA']\n</code></pre> <p>Now lets see if Run Difference is correlated with Wins. We use a scatter plot again:</p> correlation<pre><code>def figure = Underdog\n    .plots()\n    .scatter(\n        data['RD'],\n        data['W'],\n        title: 'Run difference vs Wins')\n\nfigure.show()\n</code></pre>"},{"location":"ml/tutorial/#model-training","title":"Model training","text":"<p>Let\u2019s create our first predictive model using linear regression, with runDifference as the sole explanatory variable.  Here we use Ordinary Least Squares (OLS) regression model.</p> <p>Tip</p> <p>To know more about Ordinary Least Squares you can check out its definition in Wikipedia</p> Ordinary least square (OLS)<pre><code>// extracting features (X) and labels (y)\ndef X = data['RD'] as double[][]\ndef y = data['W'] as double[]\n\n// splitting between train and test datasets to avoid over fitting\ndef (xTrain, xTest, yTrain, yTest) = Underdog.ml().utils.trainTestSplit(X, y)\n\n// training the model\ndef winsModel = Underdog.ml().regression.ols(xTrain, yTrain)\n</code></pre> <p>If we print our \u201cwinsModel\u201d, it produces the output below:</p> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-14.4535     -2.5195      0.2255      2.9513     11.5651\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept          80.9257     0.1838   440.3014     0.0000 ***\nX0                  0.1038     0.0019    54.6362     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.9032 on 449 degrees of freedom\nMultiple R-squared: 0.8693,    Adjusted R-squared: 0.8690\nF-statistic: 2985.1152 on 2 and 449 DF,  p-value: 1.757e-200\n</code></pre> <p>If you\u2019re new to regression, here are some take-aways from the output:</p> <ul> <li>The R-squared of .88 can be interpreted to mean that roughly 88% of the variance in Wins can be explained by the Run Difference variable. The rest is determined by some combination of other variables and pure chance.</li> <li>The estimate for the Intercept is the average wins independent of Run Difference. In baseball, we have a 162 game season so we expect this value to be about 81, as it is.</li> <li>The estimate for the RD variable of .1, suggests that an increase of 10 in Run Difference, should produce about 1 additional win over the course of the season.</li> </ul> <p>Of course, this model is not simply descriptive. We can use it to make predictions. In the code below, we predict how many games we will win if we score 135 more runs than our opponents.  To do this, we pass an array of doubles, one for each explanatory variable in our model, to the predict() method. In this case, there\u2019s just one variable: run difference.</p> using prediction<pre><code>def prediction = winsModel.predict([135] as double[])\n</code></pre> output<pre><code>94.93591869149651\n</code></pre> <p>We\u2019d expect almost 95 wins when we outscore opponents by 135 runs.</p>"},{"location":"ml/tutorial/#scoring-with-test-data","title":"Scoring with test data","text":"<p>If we want to check how the model is performing overall we can pick the testing datasets we kept aside from the model training and use them to get a measure on how well the model is predicting a new case.</p> <p>In this example we are using the R2 square metric.  In regression, the R2 score is a statistical measure of how well the regression predictions approximate the real data points.</p> <p>We're passing to the model the testing features (xTest) and comparing the model predictions with the test labels we have (yTest):</p> r2score<pre><code>// generating predictions for the test features\ndef predictions = winsModel.predict(xTest)\n\n// comparing predictions with the actual truth for those features\ndef r2score = Underdog.ml().metrics.r2Score(yTest, predictions)\n</code></pre> <p>Getting a result of:</p> output<pre><code>0.8895307770897616\n</code></pre>"},{"location":"ml/tutorial/#modeling-runs-scored","title":"Modeling Runs Scored","text":"<p>It\u2019s time to go deeper again and see how we can model Runs Scored and Runs Allowed. The approach the A\u2019s took was to model Runs Scored using team On-base percent (OBP) and team Slugging Average (SLG). In Underdog, we write:</p> <pre><code>def X = data['OBP', 'SLG'] as double[][]\ndef y = data['RS'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef runsScored = ml.regression.ols(xTrain, yTrain)\n</code></pre> <p>Once again the first parameter takes a Underdog column containing the values we want to predict (Runs scored). The next two parameters take the explanatory variables OBP and SLG.</p> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-67.7289    -18.0586     -1.5988     16.8863     68.9436\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept        -846.1069    29.6301   -28.5556     0.0000 ***\nX0               2937.5751   141.7312    20.7264     0.0000 ***\nX1               1524.8355    65.4379    23.3020     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.4627 on 448 degrees of freedom\nMultiple R-squared: 0.9204,    Adjusted R-squared: 0.9201\nF-statistic: 2590.2693 on 3 and 448 DF,  p-value: 6.276e-247\n</code></pre> <p>Again we have a model with excellent explanatory power with an R-squared of 92. Now we\u2019ll check the model visually to see if it violates any assumptions. Our residuals should be normally distributed. We can use a histogram to verify:</p> histogram<pre><code>def histogram = Underdog\n    .plots()\n    .histogram(residuals.toList(), title: 'Runs Scored from OBP and SLG')\n\nhistogram.show()\n</code></pre> <p>It looks great.  It\u2019s also important to plot the predicted (or \u201cfitted\u201d) values against the residuals. We want to see if the model fits some values better than others, which will influence whether we can trust its predictions or not. Ideally, we want to see a cloud of random dots around zero on the y axis.</p> <p>Our Scatter class can create this plot directly from the model:</p> fitted vs residuals<pre><code>def modelResiduals = runsScored.residuals().toList()\ndef modelFitted = runsScored.fittedValues().toList()\n\ndef fittedVsResiduals = Underdog\n    .plots()\n    .scatter(modelFitted, modelResiduals,\n        title: \"Runs Scored from OBP and SLG\",\n        xLabel: \"Fitted\",\n        yLabel: \"Residuals\")\n\nfittedVsResiduals.show()\n</code></pre> <p>Again, the plot looks good.</p> <p>Let\u2019s review.  We\u2019ve created a model of baseball that predicts entry into the playoffs based on batting stats, with the influence of the variables as:</p> <pre><code>graph LR\n  A[SLG &amp; OBP] --&gt; B[Runs Scored];\n  B --&gt; C[Run Difference];\n  C --&gt; D[Regular Season Wins];</code></pre>"},{"location":"ml/tutorial/#modeling-runs-allowed","title":"Modeling Runs Allowed","text":"<p>Of course, we haven\u2019t modeled the Runs Allowed side of Run Difference. We could use pitching and field stats to do this, but the A\u2019s cleverly used the same two variables (SLG and OBP), but now looked at how their opponent\u2019s performed against the A\u2019s. We could do the same as these data are encoded in the dataset as OOBP and OSLG.</p> SLG &amp; OBP<pre><code>def X = data['OOBP', 'OSLG'].dropna() as double[][]\ndef y = data['RA'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef runsAllowed = ml.regression.ols(xTrain, yTrain)\n</code></pre> print<pre><code>println(runsAllowed)\n</code></pre> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-82.1479     -8.9954      0.7291     15.7773     46.4004\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept        -822.7172    97.7824    -8.4138     0.0000 ***\nX0               2844.9158   524.5965     5.4231     0.0000 ***\nX1               1532.0857   286.8341     5.3414     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27.7020 on 42 degrees of freedom\nMultiple R-squared: 0.8985,    Adjusted R-squared: 0.8936\nF-statistic: 185.8286 on 3 and 42 DF,  p-value: 1.377e-21\n</code></pre> <p>This model also looks good, but you\u2019d want to look at the plots again, and do other checking as well. Checking the predictive variables for collinearity is always good.</p> <p>Finally, we can tie this all together and see how well wins is predicted when we consider both offensive and defensive stats.</p> regression<pre><code>def X = data[\"OOBP\", \"OBP\", \"OSLG\", \"SLG\"].dropna() as double[][]\ndef y = data['W'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef winsFinal = ml.regression.ols(xTrain, yTrain)\n</code></pre>"},{"location":"ml/tutorial/#the-as-in-2001","title":"The A's in 2001","text":"<p>For fun, I decided to see what the model predicts for the 2001 A\u2019s. First, I got the independent variables for the A\u2019s in that year.</p> A's in 2001<pre><code>def asIn2001 = data[\n    data['team'] == 'OAK' &amp;\n    data['year'] == 2001].loc[__, [\"year\", \"OOBP\", \"OBP\", \"OSLG\", \"SLG\"]]\n</code></pre> output<pre><code>                 baseball.csv\nYear  |  OOBP   |   OBP   |  OSLG  |   SLG   |\n-----------------------------------------------\n2001  |  0.308  |  0.345  |  0.38  |  0.439  |\n</code></pre> <p>Now we get the prediction:</p> A's in 2001<pre><code>double[][] values = asIn2001.loc[__, [\"OOBP\", \"OBP\", \"OSLG\", \"SLG\"]] as double[][]\ndouble[] value = winsFinal.predict(values);\n</code></pre> output<pre><code>102.22837899017894\n</code></pre> <p>The model predicted that the 2001 A\u2019s would win 102 games given their slugging and On-Base stats. They won 103.</p>"},{"location":"plots/","title":"Plots","text":"<p>Underdog's Plots module renders different types of charts and adds integration with rest of Underdog's modules.</p>"},{"location":"plots/#introduction","title":"Introduction","text":""},{"location":"plots/#echarts-dsl","title":"Echarts DSL","text":"<p>Underdog plots uses the Apache Echarts under the hood, so the idea is to try to be able to render whatever is possible in Echarts. To accomplish that this project creates a Groovy DSL mimicking the Echarts Option object. You can access the DSL when customizing the chart.</p> <p>At the moment the support of the Echarts Option object is limited but we aim to improve that overtime.</p>"},{"location":"plots/#basic-properties","title":"Basic properties","text":"<p>For every chart we must provide methods containing the following properties:</p> <ul> <li>data entry as list of numbers</li> <li>data entry as Series</li> <li>chart title</li> <li>chart subtitle</li> </ul> <p>When the method is receiving data as List instances:</p> <ul> <li>X coordinate label (by default is X)</li> <li>Y coordinate label (by default is Y)</li> </ul> <p>In methods receiving Series objects the name of the X and Y coordinate will be taken from the Series' name passed as parameter.</p> <p>There could be extra properties added to specific charts depending on how practical these properties are when dealing which that type of charts.</p> <p>On top of that, every plot has a <code>customize(Closure)</code> which allows to customize the chart following Echarts documentation using a Groovy DSL.</p>"},{"location":"plots/#customizing-chart","title":"Customizing chart","text":"<p>All default methods provide a limited setup of the chart via the mandatory attributes we saw previously.To access the full Groovy Echarts DSL we can always access the Options#customize(Closure) method rendering the chart calling the Options#show().</p>"},{"location":"plots/#return-options","title":"Return Options","text":"<p>All plotting methods return a <code>memento.plots.charts.Options</code> instance which represents the Echarts Options object.</p>"},{"location":"plots/#line","title":"Line","text":""},{"location":"plots/#simple","title":"Simple","text":"<p>Here is a simple line chart:</p> simple line<pre><code>def line = Underdog.plots()\n    .line(\n        // You can use a **range or a list** for X axis\n        2000..2010,\n        // You can use a **range or a list** for the Y axis\n        [10, 15, 18, 3, 5, 9, 10, 11, 12, 10],\n        // Optional attributes\n        title: \"Wins of Team A\",\n        subtitle: \"Between years 2000 - 2010\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    )\n\nline.show()\n</code></pre> <p> </p> <p>There are methods adapted for using Underdog's Series objects:</p> Series<pre><code>// load data\ndef df = Underdog.df().read_csv(baseballPath)\n\n// filter &amp; aggregate &amp; sort\ndf = df[df['Team'] == 'BOS']\n    .agg(W: 'sum')\n    .by('year')\n    .sort_values(by: 'year')\n\n// show\ndef plot = Underdog.plots()\n    .line(\n        // using `year` series for X axis\n        df['year'],\n        // renaming series to `Wins X Years` and using it for Y axis\n        df['Sum [W]'].rename('Wins X Years'),\n        title: \"Wins of 'BOS' team over time\")\n\nplot.show()\n</code></pre> <p></p> <p>In any method using Underdog's Series there is no attribute for changing the xLabel or the yLabel as it takes the Series' name. To change the label you can rename the Series' name as mentioned in the example.</p>"},{"location":"plots/#n-lines","title":"N-lines","text":"lines using collections<pre><code>Map&lt;String, List&lt;Number&gt;&gt; data = [\n    // A list of lists of 2 elements [[x1, y1], [x2, y2],..., [xn, yn]]\n    A: [[2000, 13],[2001, 5], [2002, 7], [2003, 10], [2004,6]],\n    B: [[2000, 5], [2001, 6], [2002, 7], [2003, 8], [2004, 9]],\n    // Using [listX, listY]transpose()` == [[x1, y1], [x2, y2],..., [xn, yn]]\n    C: [2000..2004, 3..7].transpose()\n]\n\ndef plot = Underdog.plots()\n    .lines(\n        data,\n        title: \"Progress of Teams A, B, C\",\n        subtitle: \"Between years 2000 - 2010\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/#customize","title":"Customize","text":"<p>As in any chart once we've created our Options object and before calling <code>show()</code> we can use the <code>Options#customize()</code> method to customize the chart using Groovy's Echart DSL.</p> Customize<pre><code>def plot = Underdog.plots()\n    .lines(\n        dataFrame,\n        title: \"Team comparison (BOS, ATL, CIN)\",\n        subtitle: \"Years 2000-2004\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    ).customize {\n        // Adding legend in the top right corner\n        legend {\n            top(\"10%\")\n            right('15%')\n            show(true)\n        }\n        // Adding tooltip of type `axis`\n        tooltip {\n            trigger('axis')\n        }\n    }\n\nplot.show()\n</code></pre> <p>In this occasion we are adding the chart legend and positioning it to the top-right side of the chart.</p> <p> </p>"},{"location":"plots/#bar","title":"Bar","text":""},{"location":"plots/#simple_1","title":"Simple","text":"simple bar<pre><code>def plot = Underdog.plots()\n    .bar(\n        1..12,\n        [10, 12, 18, 3, 0, 20, 10, 12, 18, 3, 0, 10],\n        title: \"Using bars\",\n        xLabel: \"Months\",\n        yLabel: \"Indicator\"\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/#histogram","title":"Histogram","text":"simple histogram<pre><code>// generating data\ndef random = new Random()\ndef distribution = (0..1_000).collect { random.nextGaussian(0, 50) }\n\n// plot\ndef plot = Underdog\n    .plots()\n    .histogram(\n        distribution,\n        title: \"Distribution\",\n        subtitle: \"mean: 0 / stddev: 50\",\n        bins: 10\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/#scatter","title":"Scatter","text":""},{"location":"plots/#simple_2","title":"Simple","text":"simple<pre><code>// numbers from 0 to 99\n// You can use a \"range or a list\" for X axis\ndef xs = 0..&lt;100\n\n// 100 random numbers\n// You can use a \"range or a list\" for the Y axis\ndef ys = (0..&lt;100).collect { new Random().nextInt(100) }\n\n// plot\ndef plot = Underdog.plots()\n    .scatter(\n        xs,\n        ys,\n        title: \"Random Numbers\") // Optional attributes\n\nplot.show()\n</code></pre> <p>Here's the same example but using Underdog's series for X and Y axes. Given the dataframe instance <code>df</code> and series <code>xs</code> and <code>ys</code>:</p> simple series<pre><code>Plots.plots()\n        .scatter(\n            // using a series for x axis and renaming it to X\n            df['xs'].rename('X'),\n            // using another series for y axis and renaming it to Y\n            df['ys'].rename('Y'),\n            title: \"Random Numbers\")\n        .show()\n</code></pre>"},{"location":"plots/#graphs","title":"Graphs","text":""},{"location":"plots/#simple_3","title":"Simple","text":"<p>Here's a simple graph representation just showing 2 edges connecting 3 vertices. We only have to pass an instance of a graph to the <code>graph(...)</code> function:</p> <p> </p> simple graph<pre><code>// create instance of Graph\ndef friends = Graphs.graph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\n// show plot\ndef plot = Underdog.plots().graph(friends)\n\nplot.show()\n</code></pre>"},{"location":"plots/#directed-graph","title":"Directed graph","text":"<p>If the graph we're passing to the <code>plots().graph(...)</code> function is a directed graph, the direction of the edges will show up:</p> <p> </p> directed graph<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Underdog.plots().graph(friends)\nplot.show()\n</code></pre>"},{"location":"plots/#edge-labels","title":"Edge labels","text":"<p>If we want to show the edges labels we can do so by setting the <code>showEdgeLabel</code> parameter to <code>true</code>:</p> <p> </p> edge labels<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Plots.plots().graph(friends, showEdgeLabel: true)\nplot.show()\n</code></pre>"},{"location":"plots/#showing-paths","title":"Showing paths","text":"<p>Sometimes we may want highlight a given path between vertices. We can use the parameter <code>paths</code> which receives a list of paths to highlight any number of paths:</p> <p> </p> show paths<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef friendship = friends.shortestPath('Robert', 'Troy')\n\ndef plot = Underdog.plots().graph(\n    friends,\n    paths: [friendship],\n    showEdgeLabel: true)\n\nplot.show()\n</code></pre>"},{"location":"plots/#graph-domain","title":"Graph domain","text":"<p>To go one step further and play with vertices sizes and colors, we can use the graph chart domain classes. These classes are:</p> <ul> <li><code>underdog.plots.charts.Graph.Node</code>: represents a node graphically (size, color, label)</li> <li><code>underdog.plots.charts.Graph.Edge</code>: represents an edge graphically (width, color, label)</li> </ul> <p> </p> domain classes<pre><code>List&lt;Graph.Node&gt; nodes = [\n    new Graph.Node(id: \"robert\", name: \"Robert\", symbolSize: 75),\n    new Graph.Node(id: \"thelma\", name: \"Thelma\", symbolSize: 40),\n    new Graph.Node(id: \"troy\", name: \"Troy\", symbolSize: 40)\n]\n\nList&lt;Graph.Edge&gt; edges = [\n    new Graph.Edge(\n        source: \"robert\",\n        target: \"thelma\",\n        color: \"green\",\n        width: 2,\n        value: \"bff\"),\n    new Graph.Edge(\n        source: \"robert\",\n        target: \"troy\",\n        color: \"red\",\n        width: 10,\n        value: \"friend\")\n]\n\ndef plot = Underdog.plots().graph(\n    nodes,\n    edges,\n    showEdgeLabel: true)\n\nplot.show()\n</code></pre>"},{"location":"plots/#customize_1","title":"Customize","text":"<p>As any of the charts in Underdog, we can use the <code>customize(...)</code> method of any chart to customize the chart by using the Groovy Echarts DSL:</p> customization<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Underdog.plots()\n    .graph(friends, showEdgeLabel: true)\n    .customize {\n        title {\n            text \"New title\"\n            subtext(\"New subtitle\")\n            top(\"bottom\")\n            left(\"right\")\n        }\n    }\n\nplot.show()\n</code></pre>"},{"location":"plots/#pie","title":"Pie","text":"<p>Wikipedia</p> <p>According to Wikipedia a pie chart (or a circle chart) is a circular statistical graphic which is divided into slices to illustrate numerical proportion.</p> <p>In a pie chart, the arc length of each slice (and consequently its central angle and area) is proportional to the quantity it represents.</p>"},{"location":"plots/#simple_4","title":"Simple","text":"<p>To create a minimal representation of a Pie we must provide at least a collection of the labels of each partition, and another collection with the values of each partition:</p> building pie<pre><code>def plot = Underdog\n    .plots()\n    .pie(\n\n        ('A'..'D'), // slice labels\n        [9,5,6,4]   // slice values\n    )\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/#color-mapping","title":"Color mapping","text":"<p>In some situations the color of each partition is really meaningful. For example, it would be strange to represent a  group of race teams and represent the team Ferrari (which historically is red) with other color than red. In order to map the colors to each partition we can provide a map of entries of type <code>partitionLabel: color</code>  to the parameter <code>colorMap</code>:</p> color mapping<pre><code>// Colors matching the labels\ndef COLORS = [\n    \"Red Bull\": \"#101864\",\n    \"Ferrari\": \"#b03641\",\n    \"Mclaren\": \"#d26f30\",\n    \"Mercedes\": \"#505c62\"\n]\n\ndef plot = Underdog.plots()\n    .pie(\n        // Labels\n        [\"Red Bull\", \"Ferrari\", \"Mclaren\", \"Mercedes\"],\n        // Values\n        [9,5,6,4],\n        // Passing color mappings\n        colorMap: COLORS,\n        title: \"Top 4 Teams F1(tm) 2024 season\",\n        subtitle: \"Total number of driver victories per team\"\n    )\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/#dataframe","title":"Dataframe","text":"<p>In order to use an Underdog's dataframe we have to make sure that the name of the series should match  the names: <code>names</code>, <code>values</code>, <code>colors</code>. </p> dataframe<pre><code>// source map\ndef df = [\n    names: ('A'..'D'),\n    values: (10..40).by(10),\n    colors: ['red', 'pink', 'yellow', 'lightblue']\n].toDataFrame(\"dataframe\")\n\n// passing dataframe to pie plot and show it\ndef plot = Underdog.plots().pie(df)\n\nplot.show()\n</code></pre> <p> </p> <p>You can also use Underdog's Series following the same rules:</p> series<pre><code>// given a dataframe\ndef df = [\n        A: ('D'..'G'),\n        B: (110..140).by(10),\n        C: ['orange', 'gray', 'lightgray', 'blue']\n].toDataFrame(\"dataframe\")\n\n// we can pass series\ndef plot = Underdog\n    .plots()\n    .pie(\n        // using series \"A\" and renaming it to \"names\"\n        df['A'].rename(\"names\"),\n        // using series \"B\" and renaming it to \"values\"\n        df['B'].rename(\"values\"),\n        // using series \"C\" and renaming it to \"colors\"\n        df['C'].rename(\"colors\")\n    )\n\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/#radar","title":"Radar","text":"<p>According to Wikipedia A radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point</p>"},{"location":"plots/#simple_5","title":"Simple","text":"simple<pre><code>def plot = Underdog\n    .plots()\n    .radar(\n        [\"power\", \"consumption\", \"price\"], // Name of the categories\n        [200, 10, 100000],                 // Maximum values for each category\n        [150, 5, 54_350]                   // Actual value for each category\n    )\nplot.show()\n</code></pre>"},{"location":"plots/#extensions","title":"Extensions","text":""},{"location":"plots/#dataframe_1","title":"Dataframe","text":"dataframe extensions<pre><code>Underdog.df()\n    // Dataframe created\n    .from(X: 10..&lt;20, Y: [1, 3, 9, 3, 19, 10, 11, 4, 14, 20], \"dataframe name\")\n    // Plots extensions for dataframe add plots methods such as `scatter()`\n    .scatter()\n    .show()\n</code></pre>"},{"location":"plots/#graphs_1","title":"Graphs","text":"<p>TODO</p>"},{"location":"plots/bar/","title":"Bar","text":""},{"location":"plots/bar/#bar","title":"Bar","text":""},{"location":"plots/bar/#simple","title":"Simple","text":"simple bar<pre><code>def plot = Underdog.plots()\n    .bar(\n        1..12,\n        [10, 12, 18, 3, 0, 20, 10, 12, 18, 3, 0, 10],\n        title: \"Using bars\",\n        xLabel: \"Months\",\n        yLabel: \"Indicator\"\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/bar/#histogram","title":"Histogram","text":"simple histogram<pre><code>// generating data\ndef random = new Random()\ndef distribution = (0..1_000).collect { random.nextGaussian(0, 50) }\n\n// plot\ndef plot = Underdog\n    .plots()\n    .histogram(\n        distribution,\n        title: \"Distribution\",\n        subtitle: \"mean: 0 / stddev: 50\",\n        bins: 10\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/extensions/","title":"Extensions","text":""},{"location":"plots/extensions/#extensions","title":"Extensions","text":""},{"location":"plots/extensions/#dataframe","title":"Dataframe","text":"dataframe extensions<pre><code>Underdog.df()\n    // Dataframe created\n    .from(X: 10..&lt;20, Y: [1, 3, 9, 3, 19, 10, 11, 4, 14, 20], \"dataframe name\")\n    // Plots extensions for dataframe add plots methods such as `scatter()`\n    .scatter()\n    .show()\n</code></pre>"},{"location":"plots/extensions/#graphs","title":"Graphs","text":"<p>TODO</p>"},{"location":"plots/graphs/","title":"Graphs","text":""},{"location":"plots/graphs/#graphs","title":"Graphs","text":""},{"location":"plots/graphs/#simple","title":"Simple","text":"<p>Here's a simple graph representation just showing 2 edges connecting 3 vertices. We only have to pass an instance of a graph to the <code>graph(...)</code> function:</p> <p> </p> simple graph<pre><code>// create instance of Graph\ndef friends = Graphs.graph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\n// show plot\ndef plot = Underdog.plots().graph(friends)\n\nplot.show()\n</code></pre>"},{"location":"plots/graphs/#directed-graph","title":"Directed graph","text":"<p>If the graph we're passing to the <code>plots().graph(...)</code> function is a directed graph, the direction of the edges will show up:</p> <p> </p> directed graph<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Underdog.plots().graph(friends)\nplot.show()\n</code></pre>"},{"location":"plots/graphs/#edge-labels","title":"Edge labels","text":"<p>If we want to show the edges labels we can do so by setting the <code>showEdgeLabel</code> parameter to <code>true</code>:</p> <p> </p> edge labels<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Plots.plots().graph(friends, showEdgeLabel: true)\nplot.show()\n</code></pre>"},{"location":"plots/graphs/#showing-paths","title":"Showing paths","text":"<p>Sometimes we may want highlight a given path between vertices. We can use the parameter <code>paths</code> which receives a list of paths to highlight any number of paths:</p> <p> </p> show paths<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef friendship = friends.shortestPath('Robert', 'Troy')\n\ndef plot = Underdog.plots().graph(\n    friends,\n    paths: [friendship],\n    showEdgeLabel: true)\n\nplot.show()\n</code></pre>"},{"location":"plots/graphs/#graph-domain","title":"Graph domain","text":"<p>To go one step further and play with vertices sizes and colors, we can use the graph chart domain classes. These classes are:</p> <ul> <li><code>underdog.plots.charts.Graph.Node</code>: represents a node graphically (size, color, label)</li> <li><code>underdog.plots.charts.Graph.Edge</code>: represents an edge graphically (width, color, label)</li> </ul> <p> </p> domain classes<pre><code>List&lt;Graph.Node&gt; nodes = [\n    new Graph.Node(id: \"robert\", name: \"Robert\", symbolSize: 75),\n    new Graph.Node(id: \"thelma\", name: \"Thelma\", symbolSize: 40),\n    new Graph.Node(id: \"troy\", name: \"Troy\", symbolSize: 40)\n]\n\nList&lt;Graph.Edge&gt; edges = [\n    new Graph.Edge(\n        source: \"robert\",\n        target: \"thelma\",\n        color: \"green\",\n        width: 2,\n        value: \"bff\"),\n    new Graph.Edge(\n        source: \"robert\",\n        target: \"troy\",\n        color: \"red\",\n        width: 10,\n        value: \"friend\")\n]\n\ndef plot = Underdog.plots().graph(\n    nodes,\n    edges,\n    showEdgeLabel: true)\n\nplot.show()\n</code></pre>"},{"location":"plots/graphs/#customize","title":"Customize","text":"<p>As any of the charts in Underdog, we can use the <code>customize(...)</code> method of any chart to customize the chart by using the Groovy Echarts DSL:</p> customization<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Underdog.plots()\n    .graph(friends, showEdgeLabel: true)\n    .customize {\n        title {\n            text \"New title\"\n            subtext(\"New subtitle\")\n            top(\"bottom\")\n            left(\"right\")\n        }\n    }\n\nplot.show()\n</code></pre>"},{"location":"plots/introduction/","title":"Introduction","text":""},{"location":"plots/introduction/#introduction","title":"Introduction","text":""},{"location":"plots/introduction/#echarts-dsl","title":"Echarts DSL","text":"<p>Underdog plots uses the Apache Echarts under the hood, so the idea is to try to be able to render whatever is possible in Echarts. To accomplish that this project creates a Groovy DSL mimicking the Echarts Option object. You can access the DSL when customizing the chart.</p> <p>At the moment the support of the Echarts Option object is limited but we aim to improve that overtime.</p>"},{"location":"plots/introduction/#basic-properties","title":"Basic properties","text":"<p>For every chart we must provide methods containing the following properties:</p> <ul> <li>data entry as list of numbers</li> <li>data entry as Series</li> <li>chart title</li> <li>chart subtitle</li> </ul> <p>When the method is receiving data as List instances:</p> <ul> <li>X coordinate label (by default is X)</li> <li>Y coordinate label (by default is Y)</li> </ul> <p>In methods receiving Series objects the name of the X and Y coordinate will be taken from the Series' name passed as parameter.</p> <p>There could be extra properties added to specific charts depending on how practical these properties are when dealing which that type of charts.</p> <p>On top of that, every plot has a <code>customize(Closure)</code> which allows to customize the chart following Echarts documentation using a Groovy DSL.</p>"},{"location":"plots/introduction/#customizing-chart","title":"Customizing chart","text":"<p>All default methods provide a limited setup of the chart via the mandatory attributes we saw previously.To access the full Groovy Echarts DSL we can always access the Options#customize(Closure) method rendering the chart calling the Options#show().</p>"},{"location":"plots/introduction/#return-options","title":"Return Options","text":"<p>All plotting methods return a <code>memento.plots.charts.Options</code> instance which represents the Echarts Options object.</p>"},{"location":"plots/line/","title":"Line","text":""},{"location":"plots/line/#line","title":"Line","text":""},{"location":"plots/line/#simple","title":"Simple","text":"<p>Here is a simple line chart:</p> simple line<pre><code>def line = Underdog.plots()\n    .line(\n        // You can use a **range or a list** for X axis\n        2000..2010,\n        // You can use a **range or a list** for the Y axis\n        [10, 15, 18, 3, 5, 9, 10, 11, 12, 10],\n        // Optional attributes\n        title: \"Wins of Team A\",\n        subtitle: \"Between years 2000 - 2010\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    )\n\nline.show()\n</code></pre> <p> </p> <p>There are methods adapted for using Underdog's Series objects:</p> Series<pre><code>// load data\ndef df = Underdog.df().read_csv(baseballPath)\n\n// filter &amp; aggregate &amp; sort\ndf = df[df['Team'] == 'BOS']\n    .agg(W: 'sum')\n    .by('year')\n    .sort_values(by: 'year')\n\n// show\ndef plot = Underdog.plots()\n    .line(\n        // using `year` series for X axis\n        df['year'],\n        // renaming series to `Wins X Years` and using it for Y axis\n        df['Sum [W]'].rename('Wins X Years'),\n        title: \"Wins of 'BOS' team over time\")\n\nplot.show()\n</code></pre> <p></p> <p>In any method using Underdog's Series there is no attribute for changing the xLabel or the yLabel as it takes the Series' name. To change the label you can rename the Series' name as mentioned in the example.</p>"},{"location":"plots/line/#n-lines","title":"N-lines","text":"lines using collections<pre><code>Map&lt;String, List&lt;Number&gt;&gt; data = [\n    // A list of lists of 2 elements [[x1, y1], [x2, y2],..., [xn, yn]]\n    A: [[2000, 13],[2001, 5], [2002, 7], [2003, 10], [2004,6]],\n    B: [[2000, 5], [2001, 6], [2002, 7], [2003, 8], [2004, 9]],\n    // Using [listX, listY]transpose()` == [[x1, y1], [x2, y2],..., [xn, yn]]\n    C: [2000..2004, 3..7].transpose()\n]\n\ndef plot = Underdog.plots()\n    .lines(\n        data,\n        title: \"Progress of Teams A, B, C\",\n        subtitle: \"Between years 2000 - 2010\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/line/#customize","title":"Customize","text":"<p>As in any chart once we've created our Options object and before calling <code>show()</code> we can use the <code>Options#customize()</code> method to customize the chart using Groovy's Echart DSL.</p> Customize<pre><code>def plot = Underdog.plots()\n    .lines(\n        dataFrame,\n        title: \"Team comparison (BOS, ATL, CIN)\",\n        subtitle: \"Years 2000-2004\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    ).customize {\n        // Adding legend in the top right corner\n        legend {\n            top(\"10%\")\n            right('15%')\n            show(true)\n        }\n        // Adding tooltip of type `axis`\n        tooltip {\n            trigger('axis')\n        }\n    }\n\nplot.show()\n</code></pre> <p>In this occasion we are adding the chart legend and positioning it to the top-right side of the chart.</p> <p> </p>"},{"location":"plots/pie/","title":"Pie","text":""},{"location":"plots/pie/#pie","title":"Pie","text":"<p>Wikipedia</p> <p>According to Wikipedia a pie chart (or a circle chart) is a circular statistical graphic which is divided into slices to illustrate numerical proportion.</p> <p>In a pie chart, the arc length of each slice (and consequently its central angle and area) is proportional to the quantity it represents.</p>"},{"location":"plots/pie/#simple","title":"Simple","text":"<p>To create a minimal representation of a Pie we must provide at least a collection of the labels of each partition, and another collection with the values of each partition:</p> building pie<pre><code>def plot = Underdog\n    .plots()\n    .pie(\n\n        ('A'..'D'), // slice labels\n        [9,5,6,4]   // slice values\n    )\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/pie/#color-mapping","title":"Color mapping","text":"<p>In some situations the color of each partition is really meaningful. For example, it would be strange to represent a  group of race teams and represent the team Ferrari (which historically is red) with other color than red. In order to map the colors to each partition we can provide a map of entries of type <code>partitionLabel: color</code>  to the parameter <code>colorMap</code>:</p> color mapping<pre><code>// Colors matching the labels\ndef COLORS = [\n    \"Red Bull\": \"#101864\",\n    \"Ferrari\": \"#b03641\",\n    \"Mclaren\": \"#d26f30\",\n    \"Mercedes\": \"#505c62\"\n]\n\ndef plot = Underdog.plots()\n    .pie(\n        // Labels\n        [\"Red Bull\", \"Ferrari\", \"Mclaren\", \"Mercedes\"],\n        // Values\n        [9,5,6,4],\n        // Passing color mappings\n        colorMap: COLORS,\n        title: \"Top 4 Teams F1(tm) 2024 season\",\n        subtitle: \"Total number of driver victories per team\"\n    )\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/pie/#dataframe","title":"Dataframe","text":"<p>In order to use an Underdog's dataframe we have to make sure that the name of the series should match  the names: <code>names</code>, <code>values</code>, <code>colors</code>. </p> dataframe<pre><code>// source map\ndef df = [\n    names: ('A'..'D'),\n    values: (10..40).by(10),\n    colors: ['red', 'pink', 'yellow', 'lightblue']\n].toDataFrame(\"dataframe\")\n\n// passing dataframe to pie plot and show it\ndef plot = Underdog.plots().pie(df)\n\nplot.show()\n</code></pre> <p> </p> <p>You can also use Underdog's Series following the same rules:</p> series<pre><code>// given a dataframe\ndef df = [\n        A: ('D'..'G'),\n        B: (110..140).by(10),\n        C: ['orange', 'gray', 'lightgray', 'blue']\n].toDataFrame(\"dataframe\")\n\n// we can pass series\ndef plot = Underdog\n    .plots()\n    .pie(\n        // using series \"A\" and renaming it to \"names\"\n        df['A'].rename(\"names\"),\n        // using series \"B\" and renaming it to \"values\"\n        df['B'].rename(\"values\"),\n        // using series \"C\" and renaming it to \"colors\"\n        df['C'].rename(\"colors\")\n    )\n\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/radar/","title":"Radar","text":""},{"location":"plots/radar/#radar","title":"Radar","text":"<p>According to Wikipedia A radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point</p>"},{"location":"plots/radar/#simple","title":"Simple","text":"simple<pre><code>def plot = Underdog\n    .plots()\n    .radar(\n        [\"power\", \"consumption\", \"price\"], // Name of the categories\n        [200, 10, 100000],                 // Maximum values for each category\n        [150, 5, 54_350]                   // Actual value for each category\n    )\nplot.show()\n</code></pre>"},{"location":"plots/scatter/","title":"Scatter","text":""},{"location":"plots/scatter/#scatter","title":"Scatter","text":""},{"location":"plots/scatter/#simple","title":"Simple","text":"simple<pre><code>// numbers from 0 to 99\n// You can use a \"range or a list\" for X axis\ndef xs = 0..&lt;100\n\n// 100 random numbers\n// You can use a \"range or a list\" for the Y axis\ndef ys = (0..&lt;100).collect { new Random().nextInt(100) }\n\n// plot\ndef plot = Underdog.plots()\n    .scatter(\n        xs,\n        ys,\n        title: \"Random Numbers\") // Optional attributes\n\nplot.show()\n</code></pre> <p>Here's the same example but using Underdog's series for X and Y axes. Given the dataframe instance <code>df</code> and series <code>xs</code> and <code>ys</code>:</p> simple series<pre><code>Plots.plots()\n        .scatter(\n            // using a series for x axis and renaming it to X\n            df['xs'].rename('X'),\n            // using another series for y axis and renaming it to Y\n            df['ys'].rename('Y'),\n            title: \"Random Numbers\")\n        .show()\n</code></pre>"},{"location":"ta/","title":"Technical Analysis","text":"<p>Underdog's TA module provides a series of tools for financial technical analysis.</p>"},{"location":"ta/#tutorial","title":"Tutorial","text":""},{"location":"ta/#prerequisites","title":"Prerequisites","text":""},{"location":"ta/#dependencies","title":"Dependencies","text":"<p>To be able to follow the tutorial you should add the following modules to your gradle project:</p> GradleMavenGrapes <pre><code>// technical analysis\nimplementation 'com.github.grooviter:underdog-ta:VERSION'\n\n// plots\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <pre><code>&lt;!-- technical analysis --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-ta&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- plots --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grapes([\n    // technical analysis\n    @Grab('com.github.grooviter:underdog-ta:VERSION'),\n    // plots\n    @Grab('com.github.grooviter:underdog-plots:VERSION')\n])\n</code></pre>"},{"location":"ta/#data","title":"Data","text":"<p>You can find the data used in this tutorial here</p>"},{"location":"ta/#barseries-vs-dataframe","title":"BarSeries vs DataFrame","text":"<p>Info</p> <p>This getting started section mimics the getting started section steps from Ta4j wiki but using underdog-ta module. You can compare both entries to see the differences.</p> <p>It's important to start defining some concepts.</p> <ul> <li> <p>BarSeries: Ta4j's BarSeries is like a dataframe containing series (columns) such as open price, lower price, close price, and volume data.</p> </li> <li> <p>DataFrame: Underdog's dataframe which is composed of different Series (columns), Each Series or column is like an array of objects.</p> </li> </ul> <p>As this module integrates Ta4j with Underdog there will be methods which converts from BarSeries to DataFrame and vice versa.</p>"},{"location":"ta/#loading-data","title":"Loading data","text":"<p>In this example we are using Underdog to load stock quotes from a csv file and create a DataFrame:</p> stock quotes from csv<pre><code>// The file path where the csv file can be found\ndef filePath = \"src/test/resources/data/ta/stock_quotes_10_years.csv\"\n\n// The format of the dates included in the file\ndef dateFormat = \"yyyy-MM-dd HH:mm:ss+00:00\"\n\ndef quotes = Underdog.df().read_csv(filePath, dateFormat: dateFormat)\n</code></pre> <p>Warning</p> <p>If dates are not treated as dates the csv reader will consider them as strings. That will cause you problems when converting an Underdog's DataFrame to Ta4j's BarSeries.</p> <p>Which outputs something like the following (the prices and volume are truncated here to make it look good).</p> output<pre><code>                                stock_quotes_10_years.csv\n           Date             |  Adj Close  |  Close |  High  |  Low  |  Open  |   Volume |\n-----------------------------------------------------------------------------------------\n 2014-12-05 00:00:00+00:00  |   0.50      |  0.52  |  0.52  |  0.52 |  0.52  |  165680  |\n</code></pre> <p>In order to successfully convert the dataframe to a bar series the name of the series (columns) should match to the expected ones which are: DATE, CLOSE, HIGH, LOW, OPEN, VOLUME.</p> renaming<pre><code>quotes = quotes\n    .drop(\"Adj Close\") // Removing the \"Adj Close\" series\n    .renameSeries(fn: String::toUpperCase) // Renaming all remaining columns to upper case to match\n</code></pre> output<pre><code>                          stock_quotes_10_years.csv\n           DATE             |  CLOSE |  HIGH  |  LOW  |  OPEN  |  VOLUME  |\n---------------------------------------------------------------------------\n 2014-12-05 00:00:00+00:00  |  0.52  |  0.52  |  0.52 |  0.52  |  165680  |\n</code></pre>"},{"location":"ta/#dataframe-to-barseries","title":"DataFrame to BarSeries","text":"<p>Because we need to convert the dataframe to a BarSeries in order to create technical analysis rules:</p> to bar series<pre><code>def quotesBarSeries = quotes.toBarSeries()\n</code></pre> <p>Now we can operate with the bar series.</p>"},{"location":"ta/#indicators","title":"Indicators","text":"<p>Now we can start creating some indicators and metrics. This time we are getting metrics based on the closing price indicator:</p> indicators<pre><code>// Using the close price indicator as root indicator...\ndef closePrice = quotesBarSeries.closePriceIndicator\n// Getting the simple moving average (SMA) of the close price over the last 5 bars\nSMAIndicator shortSma = closePrice.sma(5)\n\n// Here is the 5-bars-SMA value at the 42nd index\nprintln(\"5-bars-SMA value at the 42nd index: \" + shortSma.getValue(42).doubleValue())\n\n// Getting a longer SMA (e.g. over the 30 last bars)\nSMAIndicator longSma = closePrice.sma(30)\n</code></pre> output<pre><code>5-bars-SMA value at the 42nd index: 0.503899997472763\n</code></pre>"},{"location":"ta/#building-a-trading-strategy","title":"Building a trading strategy","text":"<p>Now that we've got a couple of indicators ready lets build a strategy. Strategies are made of two trading rules: one for entry (buying), the other for exit (selling).</p> strategy base<pre><code>// Buying rules\n// We want to buy:\n//  - if the 5-bars SMA crosses over 30-bars SMA\n//  - or if the price goes below a defined price (e.g $800.00)\nRule buyingRule = shortSma.xUp(longSma)\n    .or(closePrice.xDown(120))\n\n// Selling rules\n// We want to sell:\n//  - if the 5-bars SMA crosses under 30-bars SMA\n//  - or if the price loses more than 3%\n//  - or if the price earns more than 2%\nRule sellingRule = shortSma.xDown(longSma)\n    .or(closePrice.stopLoss(3))\n    .or(closePrice.stopGain(2))\n\n// Create the strategy\nStrategy strategy = new BaseStrategy(buyingRule, sellingRule)\n</code></pre> <p>We can use Groovy's syntax to refactor the rules a little bit:</p> strategy using operators<pre><code>buyingRule = shortSma.xUp(longSma)\n    | closePrice.xDown(120)\n\nsellingRule = shortSma.xDown(longSma)\n    | closePrice.stopLoss(3)\n    | closePrice.stopGain(2)\n</code></pre> <p>In Groovy | and &amp; represent calls to methods or and and of any object (any object having those methods implemented). So if your object has these methods, you can substitute your method call by the operators.</p>"},{"location":"ta/#backtesting","title":"Backtesting","text":"<p>What is backtesting ? According to Investopedia:</p> <p>\"Backtesting is the general method for seeing how well a strategy or model would have done after the fact. It assesses the viability of a trading strategy by discovering how it would play out using historical data. If backtesting works, traders and analysts may have the confidence to employ it going forward\" -- Investopedia, https://www.investopedia.com/terms/b/backtesting.asp</p> <p>The backtest step is pretty simple:</p> backtesting<pre><code>// Running our juicy trading strategy...\nBarSeriesManager seriesManager = new BarSeriesManager(quotesBarSeries)\nTradingRecord tradingRecord = seriesManager.run(strategy)\nprintln(\"Number of positions (trades) for our strategy: \" + tradingRecord.getPositionCount())\n</code></pre> output<pre><code>Number of positions (trades) for our strategy: 57\n</code></pre> <p>For many scenarios we can run a base strategy backtesting by just executing the run method in the BarSeries object and pass directly the entry (buying) rule and the exit (selling) rule:</p> backtesting<pre><code>tradingRecord = quotesBarSeries.run(buyingRule, sellingRule)\n</code></pre> <p>We can see this visually. Follow-up showing only trades from 2024-04-01:</p> plotting trades<pre><code>// getting only stocks from 2024-04-01\nquotes = quotes[quotes['DATE'] &gt;= LocalDate.parse('2024-04-01')]\n\n// getting Underdog's series for x and y coordinates\ndef xs = quotes['DATE'](LocalDate, String) { it.format(\"dd/MM/yyyy\") }\ndef ys = quotes['CLOSE']\n\n// building a line plot\ndef plot = Underdog.plots()\n    .line(\n        xs.rename(\"Dates\"),\n        ys.rename(\"Closing Price\"),\n        title: \"Trades from 2024-01-01\",\n        subtitle: \"Using Underdog's TA and Ta4j\")\n\n// showing trades over stock quotes\ntradingRecord.trades.each {trade -&gt;\n    String x = quotesBarSeries.getBar(trade.index).endTime.format('dd/MM/yyyy')\n    double y = trade.value.doubleValue()\n    String t = trade.type.name()[0] // first letter of type name ('B' for buy, 'S' for sell)\n    plot.addAnnotation(x, y, text: t, color: t == 'B' ? 'green' : '#dd7474')\n}\n\n// showing the plot\nplot.show()\n</code></pre> <p> </p> <p>We can also visualize winning vs losing positions.</p> winning vs losing<pre><code>// creating a function to map every position to a map containing date and profit\ndef positionToDataFrameEntry = { Position pos -&gt;\n    return [\n        date: quotesBarSeries.getBar(pos.exit.index).endTime.toLocalDate(),\n        profit: pos.grossProfit.doubleValue()\n    ]\n}\n\n// getting gross profit and date of every position\ndef wins = tradingRecord\n    .positions\n    .collect(positionToDataFrameEntry)\n    .toDataFrame(\"trade values\")\n\n// mark every position as winners/losers\nwins['winner'] = wins['profit'](Double, String) { it &gt; 0 ? \"winners\" : \"losers\" }\n\n// grouping by winners/losers and get the count\ndef byMonth = wins\n    .agg(profit: 'count')\n    .by('winner')\n    .renameSeries(mapper: [\"count [profit]\": \"value\", winner: \"name\"])\n\n// getting the min/max date for the subtitle of the plot\ndef minDate = wins['date'].min(LocalDate).format(\"dd/MM/yyyy\")\ndef maxDate = wins['date'].max(LocalDate).format(\"dd/MM/yyyy\")\n\n// building the plot\ndef piePlot = Underdog\n    .plots()\n    .pie(\n        byMonth['name'].toList(),\n        byMonth['value'].toList(),\n        title: \"Winners vs Losers positions\",\n        subtitle: \"From ${minDate} to ${maxDate}\"\n    )\n\n// showing the plot\npiePlot.show()\n</code></pre> <p> </p>"},{"location":"ta/#analyzing-our-results","title":"Analyzing our results","text":"<p>Here is how we can analyze the results of our backtest:</p> analysis<pre><code>// Getting the winning positions ratio\nAnalysisCriterion winningPositionsRatio = new PositionsRatioCriterion(AnalysisCriterion.PositionFilter.PROFIT)\ndouble winningPositionRatioValue = winningPositionsRatio.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Winning positions ratio: \" + winningPositionRatioValue)\n\n// Getting a risk-reward ratio\nAnalysisCriterion romad = new ReturnOverMaxDrawdownCriterion()\ndouble nomadValue = romad.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Return over Max Drawdown: \" + nomadValue)\n\n// Total return of our strategy vs total return of a buy-and-hold strategy\nAnalysisCriterion vsBuyAndHold = new VersusEnterAndHoldCriterion(new ReturnCriterion())\ndouble vsBuyAndHoldValue = vsBuyAndHold.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Our return vs buy-and-hold return: \" + vsBuyAndHoldValue)\n</code></pre> output<pre><code>Winning positions ratio: 0.54385964912280701754385964912281\nReturn over Max Drawdown: 3.4519153297405649777237484258582\nOur return vs buy-and-hold return: 0.0040904249296843023287166775087440\n</code></pre> <p>Showing these metrics in a chart:</p> radar<pre><code>def winningRatioPlot = Underdog.plots()\n    .radar(\n        // names of metrics\n        ['winning ratio', 'return over drawdown', 'return vs buy-and-hold'],\n        // maximum possible value of each metric\n        [1, 100, 1],\n        // values from metrics\n        [winningPositionRatioValue, nomadValue, vsBuyAndHoldValue],\n        title: \"Metrics comparison\",\n        subtitle: \"Winning Ratio / Risk Reward Ratio / Return vs Buy-And-Hold\"\n    )\nwinningRatioPlot.show()\n</code></pre> <p>Which displays:</p> <p> </p>"},{"location":"ta/tutorial/","title":"Tutorial","text":""},{"location":"ta/tutorial/#tutorial","title":"Tutorial","text":""},{"location":"ta/tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"ta/tutorial/#dependencies","title":"Dependencies","text":"<p>To be able to follow the tutorial you should add the following modules to your gradle project:</p> GradleMavenGrapes <pre><code>// technical analysis\nimplementation 'com.github.grooviter:underdog-ta:VERSION'\n\n// plots\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <pre><code>&lt;!-- technical analysis --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-ta&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- plots --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>@Grapes([\n    // technical analysis\n    @Grab('com.github.grooviter:underdog-ta:VERSION'),\n    // plots\n    @Grab('com.github.grooviter:underdog-plots:VERSION')\n])\n</code></pre>"},{"location":"ta/tutorial/#data","title":"Data","text":"<p>You can find the data used in this tutorial here</p>"},{"location":"ta/tutorial/#barseries-vs-dataframe","title":"BarSeries vs DataFrame","text":"<p>Info</p> <p>This getting started section mimics the getting started section steps from Ta4j wiki but using underdog-ta module. You can compare both entries to see the differences.</p> <p>It's important to start defining some concepts.</p> <ul> <li> <p>BarSeries: Ta4j's BarSeries is like a dataframe containing series (columns) such as open price, lower price, close price, and volume data.</p> </li> <li> <p>DataFrame: Underdog's dataframe which is composed of different Series (columns), Each Series or column is like an array of objects.</p> </li> </ul> <p>As this module integrates Ta4j with Underdog there will be methods which converts from BarSeries to DataFrame and vice versa.</p>"},{"location":"ta/tutorial/#loading-data","title":"Loading data","text":"<p>In this example we are using Underdog to load stock quotes from a csv file and create a DataFrame:</p> stock quotes from csv<pre><code>// The file path where the csv file can be found\ndef filePath = \"src/test/resources/data/ta/stock_quotes_10_years.csv\"\n\n// The format of the dates included in the file\ndef dateFormat = \"yyyy-MM-dd HH:mm:ss+00:00\"\n\ndef quotes = Underdog.df().read_csv(filePath, dateFormat: dateFormat)\n</code></pre> <p>Warning</p> <p>If dates are not treated as dates the csv reader will consider them as strings. That will cause you problems when converting an Underdog's DataFrame to Ta4j's BarSeries.</p> <p>Which outputs something like the following (the prices and volume are truncated here to make it look good).</p> output<pre><code>                                stock_quotes_10_years.csv\n           Date             |  Adj Close  |  Close |  High  |  Low  |  Open  |   Volume |\n-----------------------------------------------------------------------------------------\n 2014-12-05 00:00:00+00:00  |   0.50      |  0.52  |  0.52  |  0.52 |  0.52  |  165680  |\n</code></pre> <p>In order to successfully convert the dataframe to a bar series the name of the series (columns) should match to the expected ones which are: DATE, CLOSE, HIGH, LOW, OPEN, VOLUME.</p> renaming<pre><code>quotes = quotes\n    .drop(\"Adj Close\") // Removing the \"Adj Close\" series\n    .renameSeries(fn: String::toUpperCase) // Renaming all remaining columns to upper case to match\n</code></pre> output<pre><code>                          stock_quotes_10_years.csv\n           DATE             |  CLOSE |  HIGH  |  LOW  |  OPEN  |  VOLUME  |\n---------------------------------------------------------------------------\n 2014-12-05 00:00:00+00:00  |  0.52  |  0.52  |  0.52 |  0.52  |  165680  |\n</code></pre>"},{"location":"ta/tutorial/#dataframe-to-barseries","title":"DataFrame to BarSeries","text":"<p>Because we need to convert the dataframe to a BarSeries in order to create technical analysis rules:</p> to bar series<pre><code>def quotesBarSeries = quotes.toBarSeries()\n</code></pre> <p>Now we can operate with the bar series.</p>"},{"location":"ta/tutorial/#indicators","title":"Indicators","text":"<p>Now we can start creating some indicators and metrics. This time we are getting metrics based on the closing price indicator:</p> indicators<pre><code>// Using the close price indicator as root indicator...\ndef closePrice = quotesBarSeries.closePriceIndicator\n// Getting the simple moving average (SMA) of the close price over the last 5 bars\nSMAIndicator shortSma = closePrice.sma(5)\n\n// Here is the 5-bars-SMA value at the 42nd index\nprintln(\"5-bars-SMA value at the 42nd index: \" + shortSma.getValue(42).doubleValue())\n\n// Getting a longer SMA (e.g. over the 30 last bars)\nSMAIndicator longSma = closePrice.sma(30)\n</code></pre> output<pre><code>5-bars-SMA value at the 42nd index: 0.503899997472763\n</code></pre>"},{"location":"ta/tutorial/#building-a-trading-strategy","title":"Building a trading strategy","text":"<p>Now that we've got a couple of indicators ready lets build a strategy. Strategies are made of two trading rules: one for entry (buying), the other for exit (selling).</p> strategy base<pre><code>// Buying rules\n// We want to buy:\n//  - if the 5-bars SMA crosses over 30-bars SMA\n//  - or if the price goes below a defined price (e.g $800.00)\nRule buyingRule = shortSma.xUp(longSma)\n    .or(closePrice.xDown(120))\n\n// Selling rules\n// We want to sell:\n//  - if the 5-bars SMA crosses under 30-bars SMA\n//  - or if the price loses more than 3%\n//  - or if the price earns more than 2%\nRule sellingRule = shortSma.xDown(longSma)\n    .or(closePrice.stopLoss(3))\n    .or(closePrice.stopGain(2))\n\n// Create the strategy\nStrategy strategy = new BaseStrategy(buyingRule, sellingRule)\n</code></pre> <p>We can use Groovy's syntax to refactor the rules a little bit:</p> strategy using operators<pre><code>buyingRule = shortSma.xUp(longSma)\n    | closePrice.xDown(120)\n\nsellingRule = shortSma.xDown(longSma)\n    | closePrice.stopLoss(3)\n    | closePrice.stopGain(2)\n</code></pre> <p>In Groovy | and &amp; represent calls to methods or and and of any object (any object having those methods implemented). So if your object has these methods, you can substitute your method call by the operators.</p>"},{"location":"ta/tutorial/#backtesting","title":"Backtesting","text":"<p>What is backtesting ? According to Investopedia:</p> <p>\"Backtesting is the general method for seeing how well a strategy or model would have done after the fact. It assesses the viability of a trading strategy by discovering how it would play out using historical data. If backtesting works, traders and analysts may have the confidence to employ it going forward\" -- Investopedia, https://www.investopedia.com/terms/b/backtesting.asp</p> <p>The backtest step is pretty simple:</p> backtesting<pre><code>// Running our juicy trading strategy...\nBarSeriesManager seriesManager = new BarSeriesManager(quotesBarSeries)\nTradingRecord tradingRecord = seriesManager.run(strategy)\nprintln(\"Number of positions (trades) for our strategy: \" + tradingRecord.getPositionCount())\n</code></pre> output<pre><code>Number of positions (trades) for our strategy: 57\n</code></pre> <p>For many scenarios we can run a base strategy backtesting by just executing the run method in the BarSeries object and pass directly the entry (buying) rule and the exit (selling) rule:</p> backtesting<pre><code>tradingRecord = quotesBarSeries.run(buyingRule, sellingRule)\n</code></pre> <p>We can see this visually. Follow-up showing only trades from 2024-04-01:</p> plotting trades<pre><code>// getting only stocks from 2024-04-01\nquotes = quotes[quotes['DATE'] &gt;= LocalDate.parse('2024-04-01')]\n\n// getting Underdog's series for x and y coordinates\ndef xs = quotes['DATE'](LocalDate, String) { it.format(\"dd/MM/yyyy\") }\ndef ys = quotes['CLOSE']\n\n// building a line plot\ndef plot = Underdog.plots()\n    .line(\n        xs.rename(\"Dates\"),\n        ys.rename(\"Closing Price\"),\n        title: \"Trades from 2024-01-01\",\n        subtitle: \"Using Underdog's TA and Ta4j\")\n\n// showing trades over stock quotes\ntradingRecord.trades.each {trade -&gt;\n    String x = quotesBarSeries.getBar(trade.index).endTime.format('dd/MM/yyyy')\n    double y = trade.value.doubleValue()\n    String t = trade.type.name()[0] // first letter of type name ('B' for buy, 'S' for sell)\n    plot.addAnnotation(x, y, text: t, color: t == 'B' ? 'green' : '#dd7474')\n}\n\n// showing the plot\nplot.show()\n</code></pre> <p> </p> <p>We can also visualize winning vs losing positions.</p> winning vs losing<pre><code>// creating a function to map every position to a map containing date and profit\ndef positionToDataFrameEntry = { Position pos -&gt;\n    return [\n        date: quotesBarSeries.getBar(pos.exit.index).endTime.toLocalDate(),\n        profit: pos.grossProfit.doubleValue()\n    ]\n}\n\n// getting gross profit and date of every position\ndef wins = tradingRecord\n    .positions\n    .collect(positionToDataFrameEntry)\n    .toDataFrame(\"trade values\")\n\n// mark every position as winners/losers\nwins['winner'] = wins['profit'](Double, String) { it &gt; 0 ? \"winners\" : \"losers\" }\n\n// grouping by winners/losers and get the count\ndef byMonth = wins\n    .agg(profit: 'count')\n    .by('winner')\n    .renameSeries(mapper: [\"count [profit]\": \"value\", winner: \"name\"])\n\n// getting the min/max date for the subtitle of the plot\ndef minDate = wins['date'].min(LocalDate).format(\"dd/MM/yyyy\")\ndef maxDate = wins['date'].max(LocalDate).format(\"dd/MM/yyyy\")\n\n// building the plot\ndef piePlot = Underdog\n    .plots()\n    .pie(\n        byMonth['name'].toList(),\n        byMonth['value'].toList(),\n        title: \"Winners vs Losers positions\",\n        subtitle: \"From ${minDate} to ${maxDate}\"\n    )\n\n// showing the plot\npiePlot.show()\n</code></pre> <p> </p>"},{"location":"ta/tutorial/#analyzing-our-results","title":"Analyzing our results","text":"<p>Here is how we can analyze the results of our backtest:</p> analysis<pre><code>// Getting the winning positions ratio\nAnalysisCriterion winningPositionsRatio = new PositionsRatioCriterion(AnalysisCriterion.PositionFilter.PROFIT)\ndouble winningPositionRatioValue = winningPositionsRatio.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Winning positions ratio: \" + winningPositionRatioValue)\n\n// Getting a risk-reward ratio\nAnalysisCriterion romad = new ReturnOverMaxDrawdownCriterion()\ndouble nomadValue = romad.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Return over Max Drawdown: \" + nomadValue)\n\n// Total return of our strategy vs total return of a buy-and-hold strategy\nAnalysisCriterion vsBuyAndHold = new VersusEnterAndHoldCriterion(new ReturnCriterion())\ndouble vsBuyAndHoldValue = vsBuyAndHold.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Our return vs buy-and-hold return: \" + vsBuyAndHoldValue)\n</code></pre> output<pre><code>Winning positions ratio: 0.54385964912280701754385964912281\nReturn over Max Drawdown: 3.4519153297405649777237484258582\nOur return vs buy-and-hold return: 0.0040904249296843023287166775087440\n</code></pre> <p>Showing these metrics in a chart:</p> radar<pre><code>def winningRatioPlot = Underdog.plots()\n    .radar(\n        // names of metrics\n        ['winning ratio', 'return over drawdown', 'return vs buy-and-hold'],\n        // maximum possible value of each metric\n        [1, 100, 1],\n        // values from metrics\n        [winningPositionRatioValue, nomadValue, vsBuyAndHoldValue],\n        title: \"Metrics comparison\",\n        subtitle: \"Winning Ratio / Risk Reward Ratio / Return vs Buy-And-Hold\"\n    )\nwinningRatioPlot.show()\n</code></pre> <p>Which displays:</p> <p> </p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/ml/","title":"ml","text":""}]}