{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Underdog","text":""},{"location":"#about","title":"About","text":"<p>Underdog is a set of Groovy libraries for data analysis.</p> <p>It doesn't expect to be the best data analysis tool, but it combines Groovy's expressiveness, and some of the best Java data analysis libraries to at least, make data analysis fun.</p>"},{"location":"#modules","title":"Modules","text":"<p>Underdog project covers several data analysis fields with the following subprojects:</p>"},{"location":"#underdog-dataframe","title":"underdog-dataframe","text":"<p>Combines tools for working with dataframes and series. You can use it adding the following dependency:</p> gradle<pre><code>implementation \"com.github.grooviter:underdog-dataframe:VERSION\"\n</code></pre> <p>More information in the DataFrame section</p>"},{"location":"#underdog-graphs","title":"underdog-graphs","text":"<p>It helps working on graph theory data structures and algorithms. You can use it adding the following dependency:</p> gradle<pre><code>implementation \"com.github.grooviter:underdog-graphs:VERSION\"\n</code></pre> <p>More information in the Graphs section</p>"},{"location":"#underdog-ml","title":"underdog-ml","text":"<p>Contains machine learning algorithms and evaluation mechanisms. You can use it adding the following dependency:</p> gradle<pre><code>implementation \"com.github.grooviter:underdog-ml:VERSION\"\n</code></pre> <p>More information in the ML section</p>"},{"location":"#underdog-plots","title":"underdog-plots","text":"<p>Creates different types of charts using the Apache Echarts library underneath. You can use it adding the following dependency:</p> gradle<pre><code>implementation \"com.github.grooviter:underdog-plots:VERSION\"\n</code></pre> <p>More information in the Plots section</p>"},{"location":"#underdog-ta","title":"underdog-ta","text":"<p>The technical analysis module is a wrapper over the Ta4j library. It adds some extension modules to the existent classes so that it makes easier to play with technical indicators and rules and integrate with Underdog's dataframes and series. You can use it adding the following dependency:</p> gradle<pre><code>implementation \"com.github.grooviter:underdog-ta:VERSION\"\n</code></pre> <p>More information in the Technical Analysis section</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/12/20/using-ml-module-to-create-suggestions/","title":"Using ML module to create suggestions","text":"<p>TODO</p>"},{"location":"blog/2024/12/20/madrid-bike-accident-analysis/","title":"Madrid bike accident analysis","text":"<p>TODO</p>"},{"location":"blog/2025/01/10/chrismas-in-numbers/","title":"Chrismas in numbers","text":"<p>TODO</p>"},{"location":"dataframe/","title":"DataFrame","text":"<p>Underdog's DataFrame combines tools for working with columnar data as tables (dataframes) and columns (series). And also has extra features such statistical functions and visualizations via Underdog's plots module.</p>"},{"location":"dataframe/#tutorial","title":"Tutorial","text":""},{"location":"dataframe/#prerequisites","title":"Prerequisites","text":""},{"location":"dataframe/#dependencies","title":"Dependencies","text":"<p>To be able to follow the tutorial you need the <code>underdog-dataframe</code> module. If you're using Gradle in your project:</p> gradle<pre><code>implementation \"com.github.grooviter:underdog-dataframe:VERSION\"\n</code></pre> <p>Or Maven:</p> maven<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-dataframe&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Or if you're using a Groovy script:</p> Grape<pre><code>@Grab(\"com.github.grooviter:underdog-dataframe:VERSION\")\n</code></pre>"},{"location":"dataframe/#data","title":"Data","text":"<p>The data we are using in the tutorial is </p>"},{"location":"dataframe/#loading-data","title":"Loading data","text":"<p>Here we read a csv file of tornado data. Underdog infers the column types by sampling the data.</p> reading<pre><code>def tornadoes = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> <p>Note the file is addressed relative to the current working directory. You may have to change it for your code.</p>"},{"location":"dataframe/#metadata","title":"Metadata","text":"<p>Often, the best way to start is to print the column names for reference:</p> list dataframe column names<pre><code>tornadoes.columns\n</code></pre> output<pre><code>[Date, Time, State, State No, Scale, Injuries, Fatalities, Start Lat, Start Lon, Length, Width]\n</code></pre> <p>The shape() method displays the row and column counts:</p> .shape of the dataframe<pre><code>def (int rows, int cols) = tornadoes.shape()\n</code></pre> shape of the dataframe<pre><code>println(tornadoes.shape())\n</code></pre> output<pre><code>59945 rows X 11 cols\n</code></pre> <p>structure() shows the index, name and type of each column</p> tornadoes schemas<pre><code>// getting tornadoes schema\ndef schema = tornadoes.schema()\n</code></pre> output<pre><code>  Structure of tornadoes_1950-2014.csv\n Index  |  Column Name  |  Column Type  |\n-----------------------------------------\n     0  |         Date  |   LOCAL_DATE  |\n     1  |         Time  |   LOCAL_TIME  |\n     2  |        State  |       STRING  |\n     3  |     State No  |      INTEGER  |\n     4  |        Scale  |      INTEGER  |\n     5  |     Injuries  |      INTEGER  |\n     6  |   Fatalities  |      INTEGER  |\n     7  |    Start Lat  |       DOUBLE  |\n     8  |    Start Lon  |       DOUBLE  |\n     9  |       Length  |       DOUBLE  |\n    10  |        Width  |      INTEGER  |\n</code></pre> <p>Like many Tablesaw methods, structure() returns a table. You can then produce a string representation for display. For convenience, calling toString() on a table invokes print(), which produces a string representation of the table table. To display the table then, you can simply call.</p> <pre><code>println(tornadoes)\n</code></pre> <p>You can also perform other table operations on it. For example, the code below removes all columns whose type isn\u2019t DOUBLE:</p> using schema to change dataframe structure<pre><code>def customSchema = schema[schema['Column Type'] == 'DOUBLE']\n</code></pre> output<pre><code>  Structure of tornadoes_1950-2014.csv\n Index  |  Column Name  |  Column Type  |\n-----------------------------------------\n     7  |    Start Lat  |       DOUBLE  |\n     8  |    Start Lon  |       DOUBLE  |\n     9  |       Length  |       DOUBLE  |\n</code></pre> <p>Of course, that also returned a table. We\u2019ll cover selecting rows in more detail later.</p>"},{"location":"dataframe/#previewing","title":"Previewing","text":"<p>The first(n) method returns a new table containing the first n rows.</p> getting first 3 lines<pre><code>tornadoes.head(3)\n</code></pre> output<pre><code>                                tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |  ... |\n</code></pre>"},{"location":"dataframe/#transforming","title":"Transforming","text":"<p>Mapping operations take one or more columns as inputs and produce a new column as output. We can map arbitrary expressions onto the table, but many common operations are built in. You can, for example, calculate the difference in days, weeks, or years between the values in two date columns. The method below extracts the Month name from the date column into a new column.</p> creating a new series to dataframe<pre><code>def monthSeries = tornadoes[\"Date\"](LocalDate, String) {\n    it.format(\"MMMM\")\n}\n</code></pre> <p>Now that you have a new column, you can add it to the table:</p> adding month series to tornadoes dataframe<pre><code>tornadoes['month'] = monthSeries\n</code></pre> <p>Of course nothing prevents you from doing everything altogether.</p> <p>You can remove columns from tables to save memory or reduce clutter:</p> remove series from dataframe<pre><code>tornadoes = tornadoes - tornadoes['Date']\n</code></pre>"},{"location":"dataframe/#sorting","title":"Sorting","text":"<p>Now lets sort the table in reverse order by the id column. The negative sign before the name indicates a descending sort.</p> sort asc<pre><code>def df1 = tornadoes.sort_values(by: 'Fatalities')\n</code></pre> <p>You can also sort in descending order:</p> sort desc<pre><code>def df2 = tornadoes.sort_values(by: '-Fatalities')\n</code></pre> <p>and even sorting by more than one field:</p> sort by n-fields<pre><code>def df3 = tornadoes.sort_values(by: ['Fatalities', 'State'])\n</code></pre>"},{"location":"dataframe/#descriptive-statistics","title":"Descriptive statistics","text":"<p>Descriptive statistics are calculated using the summary() method:</p> printing column insights<pre><code>def columnStats = tornadoes[\"Fatalities\"].describe()\n\nprintln(columnStats)\n</code></pre> <p>Showing the following output:</p> column describe output<pre><code>         Column: Fatalities\n Measure   |         Value         |\n------------------------------------\n        n  |                59945  |\n      sum  |                 6802  |\n     Mean  |  0.11347068145800349  |\n      Min  |                    0  |\n      Max  |                  158  |\n    Range  |                  158  |\n Variance  |    2.901978053261765  |\n Std. Dev  |   1.7035193140266314  |\n</code></pre>"},{"location":"dataframe/#filtering","title":"Filtering","text":"<p>You can write your own methods to filter rows, but it\u2019s easier to use the built-in filter classes as shown below:</p> filtering dataframe by column expressions<pre><code>// reading tornadoes\ndef ts = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\n// adding a new series to dataframe with the name of the month\nts['month'] = ts[\"Date\"](LocalDate, String) { it.format(\"MMMM\") }\n\n// filtering\ndef result = ts[\n    ts['Fatalities'] &gt; 0 &amp;                   // at least 1 fatalities\n    ts['month'] == \"April\" &amp;                 // in the month of April\n    (ts['Width'] &gt; 300 | ts['Length'] &gt; 10)  // a tornado with a\n]\n\n// selecting only two columns\ndef stateAndDate = result['State', 'Date']\n</code></pre> output<pre><code> tornadoes_1950-2014.csv\n State  |     Date     |\n------------------------\n    MO  |  1950-01-03  |\n    IL  |  1950-01-03  |\n    OH  |  1950-01-03  |\n</code></pre> <p>The last example above returns a table containing only the columns named in select() parameters,rather than all the columns in the original. Totals and sub-totals</p> <p>Column metrics can be calculated using methods like sum(), product(), mean(), max(), etc.</p> <p>You can apply those methods to a table, calculating results on one column, grouped by the values in another.</p> <pre><code>def tornadoes = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\ndef injuriesByScale = tornadoes\n    .rename(\"Median Injuries by Tornado Scale\")\n    .agg(Injuries: \"median\")\n    .by(\"Scale\")\n    .sort_values(by: \"Scale\")\n</code></pre> <p>This produces the following table, in which Group represents the Tornado Scale and Median the median injures for that group:</p> output<pre><code>Median injuries by Tornado Scale\n Scale  |  Median [Injuries]  |\n-------------------------------\n    -9  |                  0  |\n     0  |                  0  |\n     1  |                  0  |\n     2  |                  0  |\n     3  |                  1  |\n     4  |                 12  |\n     5  |                107  |\n</code></pre>"},{"location":"dataframe/#cross-tabs","title":"Cross Tabs","text":"<p>Tablesaw lets you easily produce two-dimensional cross-tabulations (\u201ccross tabs\u201d) of counts and proportions with row and column subtotals. Here\u2019s a count example where we look at the interaction of tornado severity and US state:</p> crosstabs (count)<pre><code>def crossTab = tornadoes.xTabCounts(labels: 'State', values: 'Scale')\n\ncrossTab.head()\n</code></pre> output<pre><code>                       Crosstab Counts: State x Scale\n [labels]  |  -9  |   0    |   1   |   2   |   3   |  4   |  5   |  total  |\n----------------------------------------------------------------------------\n       AL  |   0  |   624  |  770  |  425  |  142  |  38  |  12  |   2011  |\n       AR  |   1  |   486  |  667  |  420  |  162  |  29  |   0  |   1765  |\n       AZ  |   1  |   146  |   71  |   16  |    3  |   0  |   0  |    237  |\n       CA  |   1  |   271  |  117  |   23  |    2  |   0  |   0  |    414  |\n       CO  |   3  |  1322  |  563  |  112  |   22  |   1  |   0  |   2023  |\n       CT  |   0  |    18  |   53  |   22  |    4  |   2  |   0  |     99  |\n       DC  |   0  |     2  |    0  |    0  |    0  |   0  |   0  |      2  |\n       DE  |   0  |    22  |   26  |   12  |    1  |   0  |   0  |     61  |\n       FL  |   2  |  1938  |  912  |  319  |   37  |   3  |   0  |   3211  |\n       GA  |   0  |   413  |  700  |  309  |   74  |  11  |   0  |   1507  |\n</code></pre> <p>Putting it all together</p> <p>Now that you\u2019ve seen the pieces, we can put them together to perform a more complex data analysis. Lets say we want to know how frequently Tornadoes occur in the summer. Here\u2019\u2019s one way to approach that:</p> <p>Let\u2019s start by getting only those tornadoes that occurred in the summer.</p> tornadoes in the summer<pre><code>def ts = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\n// adding some series to the dataframe to make filtering easier\nts['month']      = ts['Date'](Date, String) { it.format(\"MMMM\") }\nts['dayOfMonth'] = ts['Date'](Date, Integer) { it.format(\"dd\").toInteger() }\n\n// filtering\ndef summer = ts[\n    (ts['month'] == 'June' &amp; ts['dayOfMonth'] &gt; 21) |    // after June the 21st or...\n    (ts['month'] in ['July', 'August']) |                // in July or August or...\n    (ts['month'] == 'September' &amp; ts['dayOfMonth'] &lt; 22) // before September the 22nd\n]\n</code></pre> <p>To get the frequency, we calculate the difference in days between successive tornadoes. The lag() method creates a column where every value equals the previous value (the prior row) of the source column. Then we can simply get the difference in days between the two dates. DateColumn has a method daysUntil() that does this. It returns a NumberColumn that we\u2019ll call \u201cdelta\u201d.</p> lag and delta dates<pre><code>// sorting by Date and Time series\nsummer = summer.sort_values(by: ['Date', 'Time'])\n\n// creating a series with lagged dates\nsummer['Lagged'] = summer['Date'].lag(1)\n\n// creating a series with delta days between lagged dates and summer dates\nsummer['Delta'] = summer['Lagged'] - summer['Date']\n</code></pre> <p>Now we simply calculate the mean of the delta column. Splitting on year keeps us from inadvertently including the time between the last tornado of one summer and the first tornado of the next.</p> create summary<pre><code>// creating year series to be able to group by it\nsummer['year'] = summer['Date'](Date, String) { it.format(\"YYYY\") }\n\n// aggregating delta\ndef summary = summer.agg(Delta: [\"mean\", \"count\"]).by(\"year\")\n\n// print out summary\nprintln(summary)\n</code></pre> <p>Printing summary gives us the answer by year.</p> summary output<pre><code>                           tornadoes_1950-2014.csv summary\n Date year  |  Mean [Date lag(1) - Date[DAYS]]  |  Count [Date lag(1) - Date[DAYS]]  |\n--------------------------------------------------------------------------------------\n      1950  |               2.0555555555555545  |                               162  |\n      1951  |               1.7488584474885829  |                               219  |\n      1952  |               1.8673469387755088  |                               196  |\n      1953  |                0.983870967741935  |                               372  |\n      1954  |               0.8617283950617302  |                               405  |\n</code></pre> <p>To get a DOUBLE for the entire period, we can take the average of the annual means.</p> average second column<pre><code>def meanOfSeries = summary.iloc[__, 1].mean()\n</code></pre> Average days between tornadoes in the summer:<pre><code>0.5931137164104612\n</code></pre>"},{"location":"dataframe/#saving-your-data","title":"Saving your data","text":"<p>To save a table, you can write it as a CSV file:</p> saving csv file<pre><code>tornadoes.write().csv(\"rev_tornadoes_1950-2014.csv\");\n</code></pre> <p>And that\u2019s it for the introduction. Please see the User Guide for more information.</p>"},{"location":"dataframe/#dataframe","title":"DataFrame","text":"<p>Underdog's DataFrame combines tools for working with columnar data as tables (dataframes) and columns (series). And also has extra features such statistical functions and visualizations via  Underdog's plots module.</p>"},{"location":"dataframe/#creation","title":"Creation","text":"<p>The easiest way to create a Dataframe is using the Underdog extension method <code>Underdog.df()</code>. Here we're creating an empty DataFrame:</p> empty dataframe<pre><code>DataFrame emptyDataFrame = Underdog.df().empty()\n</code></pre> <p>We can create a dataframe with a series of map entries representing series. In this case the key entry is the name of the series and the value is a collection which will become the content of the series.</p> dataframe from a map<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from the map\nDataFrame map2DataFrame = Underdog.df().from(map, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Underdog dataframe library adds additional methods to collection types so that you can convert from collections to Dataframes. And example is invoking the <code>toDataFrame(...)</code> method from the map directly:</p> map extension<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from a map\nDataFrame map2DataFrame = map.toDataFrame(\"people-dataframe\")\n</code></pre> <p>You can also pass a list of maps to the <code>Underdog.df().from(col, name)</code> method:</p> collection of maps<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = Underdog.df().from(list, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Here there is also an extension method for collections so that, IF your list complies to this structure you can call to the method <code>toDataFrame(name)</code> and create a DataFrame from that collection.</p> collection extension<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = list.toDataFrame(\"people-dataframe\")\n</code></pre>"},{"location":"dataframe/#filtering_1","title":"Filtering","text":"<p>In a dataframe you can filter data by any of the Series the dataframe has.</p>"},{"location":"dataframe/#numbers","title":"Numbers","text":"<p>The following dataframe builds a theoretical increase of population in ten years:</p> numbers<pre><code>def df = [\n    years: (1991..2000),\n    population: (1..10).collect { 1000 * it }\n].toDataFrame(\"population increase\")\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n 1994  |        4000  |\n 1995  |        5000  |\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>If we wanted to take the records after year 1995:</p> greater than<pre><code>def last5Years = df[df['years'] &gt; 1995]\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>Or getting records with population less than 4000:</p> less than<pre><code>def yearsWithLessThan = df[df['population'] &lt; 4000]\n</code></pre> output<pre><code>population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n</code></pre>"},{"location":"dataframe/#string","title":"String","text":"<p>Of course we can filter by strings. Follow up we've got a dataframe with some employee data:</p> employees<pre><code>def df = [\n    employees: ['Udo', 'John', 'Albert', 'Ronda'],\n    department: ['sales', 'it', 'sales', 'it'],\n    payroll: [10_000, 12_000, 11_000, 13_000]\n].toDataFrame(\"employees\")\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n    Albert  |       sales  |    11000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>Getting employees from sales department:</p> sales<pre><code>def salesPeople = df[df['department'] == 'sales']\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n    Albert  |       sales  |    11000  |\n</code></pre> <p>You can also use to filter by a list of possible choices:</p> in list<pre><code>def employeesByName = df[df['employees'] in ['Ronda', 'Udo']]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>You can even try by a regular expression. Lets look for employees with an 'o' in their name:</p> regex<pre><code>def employeesWithAnO = df[df['employees'] ==~ /.*o.*/ ]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n     Ronda  |          it  |    13000  |\n</code></pre>"},{"location":"dataframe/#dates","title":"Dates","text":"<p>Of course in time series is crucial to allow searches by time frame.</p> dates<pre><code>// Using a given date as the beginning of our df dates series\ndef initialDate = LocalDate.parse('01/01/2000', 'dd/MM/yyyy')\n\n// a dataframe containing the simulation of bicycles rented through 2000\ndef df = [\n    dates: (1..365).collect(initialDate::plusDays),\n    rented: (1..365).collect { new Random().nextInt(200) }\n].toDataFrame(\"rented bicycles 2000\")\n</code></pre> output<pre><code>rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-01-02  |      41  |\n2000-01-03  |      47  |\n2000-01-04  |      27  |\n2000-01-05  |      95  |\n2000-01-06  |      30  |\n2000-01-07  |     162  |\n2000-01-08  |      52  |\n2000-01-09  |     197  |\n2000-01-10  |     125  |\n2000-01-11  |      15  |\n       ...  |     ...  |\n</code></pre> <p>What if we'd like to get only those records of december 2000 ?</p> after<pre><code>def lastMonth = df[df['dates'] &gt;= LocalDate.parse('01/12/2000', 'dd/MM/yyyy')]\n</code></pre> output<pre><code>  rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-12-01  |     104  |\n2000-12-02  |     193  |\n2000-12-03  |     107  |\n2000-12-04  |     108  |\n2000-12-05  |     193  |\n2000-12-06  |     165  |\n2000-12-07  |      82  |\n2000-12-08  |      77  |\n2000-12-09  |     176  |\n2000-12-10  |     158  |\n       ...  |     ...  |\n2000-12-31  |     150  |\n</code></pre>"},{"location":"dataframe/#summary","title":"Summary","text":"<p>Here you have the tables with the supported operators:</p> <p>Arithmetic</p> Left Right Operator Example Status Series Series + <code>df['a'] + df['b']</code> Yes Series Series - <code>df['a'] - df['b]</code> Yes Series Object + <code>df['a'] + 1</code> Yes Series Object - <code>df['a'] - 1</code> Yes Series Object * <code>df['a'] * 2</code> Yes Series Object / <code>df['a'] / 2</code> Yes <p>filtering operators</p> Type Operator Example Status String == <code>df['a'] == 'x'</code> Yes String != <code>df['a'] != 'x'</code> Yes String ==~ <code>df['a'] ==~ /.*/</code> Yes String in <code>df['a'] in ['x']</code> Yes Number == <code>df['a'] == 1</code> Yes Number != <code>df['a'] != 1</code> Yes Number &gt; <code>df['a'] &gt; 1</code> Yes Number &gt;= <code>df['a'] &gt;= 1</code> Yes Number &lt; <code>df['a'] &lt; 1</code> Yes Number &lt;= <code>df['a'] &lt;= 1</code> Yes LocalDate &gt; <code>df['a'] &gt; date</code> Yes LocalDate &gt;= <code>df['a'] &gt;= date</code> Yes LocalDate &lt; <code>df['a'] &lt; date</code> Yes LocalDate &lt;= <code>df['a'] &lt;= date</code> Yes"},{"location":"dataframe/#sorting_1","title":"Sorting","text":"<p>TODO</p>"},{"location":"dataframe/#mapping","title":"Mapping","text":"<p>TODO</p>"},{"location":"dataframe/#import-export","title":"Import / Export","text":""},{"location":"dataframe/#csv","title":"CSV","text":""},{"location":"dataframe/#reading","title":"Reading","text":"<p>You can read csv files via <code>Underdog.df().read_csv(...)</code> method. Here we are importing a csv files containing tornado incidents in the USA:</p> import csv<pre><code>DataFrame dataframe = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> output<pre><code>                               tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |      |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |      |\n1950-01-03  |  11:55:00  |     IL  |         2  |      3  |         3  |      |\n1950-01-03  |  16:00:00  |     OH  |         1  |      1  |         1  |      |\n1950-01-13  |  05:25:00  |     AR  |         1  |      3  |         1  |      |\n1950-01-25  |  19:30:00  |     MO  |         2  |      2  |         5  |      |\n1950-01-25  |  21:00:00  |     IL  |         3  |      2  |         0  |      |\n1950-01-26  |  18:00:00  |     TX  |         1  |      2  |         2  |      |\n1950-02-11  |  13:10:00  |     TX  |         2  |      2  |         0  |      |\n       ...  |       ...  |    ...  |       ...  |    ...  |       ...  |  ... |\n</code></pre>"},{"location":"dataframe/#separator","title":"Separator","text":"<p>By default the csv reader assumes the csv file is using comma (,) as the separator character, but you can provide a custom separator. For example the following csv file content:</p> separator<pre><code>name;age\nLorna;34\nYule;63\nTom;28\n</code></pre> <p>Can be read by using the sep argument:</p> custom separator<pre><code>def dataframe = Underdog.df().read_csv(filePath, sep: \";\")\n</code></pre>"},{"location":"dataframe/#duplicated-names","title":"Duplicated names","text":"<p>Sometimes you can find a csv where columns are repeated, by default if you don't specify you allow repeated columns the import process will fail. Imagine we've got the following csv:</p> csv with repeated cols<pre><code>bronze,silver,gold,summer_total,bronze,silver,gold,winter_total\n1,2,1,4,1,1,1,3\n</code></pre> <p>To allow repeated columns you should set the <code>allowDuplicatedNames</code> flag to true.</p> allow repeated cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, allowedDuplicatedNames: true)\n</code></pre> <p>Then all repeated names will be prefixed in order with a number to avoid collisions:</p> output<pre><code>                                        io_repeated_cols.csv\n bronze |  silver |  gold |  summer_total |  bronze-2  |  silver-2  |  gold-2  |  winter_total|\n-----------------------------------------------------------------------------------------------\n      1 |       2 |     1 |             4 |         1  |         1  |       1  |           3  |\n</code></pre>"},{"location":"dataframe/#missing-values","title":"Missing values","text":"<p>If a csv file contains values which should be considered as well as missing values, we can pass this information before reading the csv file.</p> csv file with missing data<pre><code>from,to,id\nN/C,Madrid,123\nMadrid,Paris,124\nParis,London,125\nLondon,NONE,126\n</code></pre> <p>Here we're considering missing data the values N/C and NONE:</p> considering missing data<pre><code>def dataframe = Underdog.df().read_csv(filePath, nanValues: ['NONE', 'N/C'])\n</code></pre> <p>That will inform the reader to consider cells containing that value as missing values:</p> output<pre><code>io_custom_missing_data.csv\n  from   |    to    |  id   |\n-----------------------------\n         |  Madrid  |  123  |\n Madrid  |   Paris  |  124  |\n  Paris  |  London  |  125  |\n London  |          |  126  |\n</code></pre>"},{"location":"dataframe/#date-format","title":"Date format","text":"<p>If your csv files have a custom date format you can provide the date pattern as a parameter. Here we have a file with a custom format:</p> custom date format<pre><code>Date,Close\n2014-12-05 00:00:00+00:00,0.5267500281333923\n2014-12-08 00:00:00+00:00,0.5199999809265137\n2014-12-09 00:00:00+00:00,0.5182499885559082\n</code></pre> <p>Passing the pattern as parameter:</p> custom date format<pre><code>def dataframe = Underdog.df().read_csv(filePath, dateFormat: \"yyyy-MM-dd HH:mm:ss+00:00\")\n</code></pre> <p>Gives the following output:</p> output<pre><code>      io_custom_date_format.csv\n   Date     |        Close         |\n-------------------------------------\n2014-12-05  |  0.5267500281333923  |\n2014-12-08  |  0.5199999809265137  |\n2014-12-09  |  0.5182499885559082  |\n</code></pre>"},{"location":"dataframe/#skip-rowsfooter","title":"Skip rows/footer","text":"<p>If you're sure that there is data you'd like to avoid parsing, like nonsense data, you can skip parsing those rows. Check the following example:</p> csv file with comments<pre><code># some information about the data\n# col1: city\n# col2: weight\ncol1,col2\nNC,0\nNC,0\nNC,0\nNC,0\nMadrid,1\nParis,2\nNC,0\nNC,0\nNC,0\nNC,0\n# another comment here\n</code></pre> <p>There are lines we don't want to consider when creating our dataframe:</p> <ul> <li>comments in the beginning of the file (lines 1-3)</li> <li>comments in the end of the file (line 15)</li> <li>rows we don't want to parse because they don't add any meaningful information (4-8 and 11-14)</li> </ul> <p>To avoid parsing any of these lines we can instruct the csv reader to skip lines in the header and/or in the footer of the file:</p> skipping rows<pre><code>def dataframe = Underdog.df()\n    .read_csv(filePath,\n        header: false,    // not using first row as header\n        skipRows: 8,      // skipping rows at the beginning of the file\n        skipFooter: 4     // skipping rows at the end of the file\n    ).renameSeries(columns: ['city', 'id']) // renaming series names with the list passed as parameter\n</code></pre> output<pre><code>io_skipping_rows.csv\n city   |  id  |\n-----------------\nMadrid  |   1  |\n Paris  |   2  |\n</code></pre>"},{"location":"dataframe/#max-chars-x-col","title":"Max chars x col","text":"<p>You can instruct the csv reader to avoid parsing columns with more than a number of characters.</p> limiting col chars<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxCharsPerColumn: 20)\n</code></pre> <p>Warning</p> <p>If a column exceeds the number of characters the process will throw an exception</p>"},{"location":"dataframe/#max-cols","title":"Max cols","text":"<p>You can instruct the csv reader to avoid parsing more than a given number of columns.</p> limiting number of cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxNumberOfColumns: 2)\n</code></pre> <p>Warning</p> <p>If the number of columns exceeds the number specified the process will throw an exception</p>"},{"location":"dataframe/#series","title":"Series","text":""},{"location":"dataframe/#intro","title":"Intro","text":"<p>A Series object represents a named one-dimensional array. It also supports operations and statistical methods. It also has operations to deal with missing values. You can create a Series object from different sources:</p> create<pre><code>// from a range of numbers\ndef numbers = (1..4).toSeries(\"numbers\")\n\n// from a range of letters\ndef letters = ('A'..'C').toSeries(\"letters\")\n\n// from a list\ndef stuff = [1, 2, null, 3, 4].toSeries(\"stuff\")\n</code></pre> <p>You can use operator symbols to apply simple operations over the Series object:</p> operations<pre><code>// multiplying a series by a number\ndef doubleSeries = numbers * 2\n\n// multiplying a series by another series\ndef rowProduct = numbers * stuff.dropna()\n\n// dividing a series\ndef halves = stuff / 2\n\n// using custom transformation to create a new series\ndef custom = letters(String, String) { \"letter-$it\".toString() }\n</code></pre> <p>Sometimes you may want to analyze a given Series object by using statistical methods:</p> statistics<pre><code>def mean = doubleSeries.mean()\ndef max = doubleSeries.max()\ndef min = doubleSeries.min()\ndef avg = doubleSeries.avg()\n</code></pre> <p>You can find all statistical available methods in the <code>underdog.impl.extensions.SeriesStatsExtensions</code> class.</p>"},{"location":"dataframe/#creating","title":"Creating","text":"<p>Series are meant to be created from collections or as a transformation from another Series.</p> <p>The only way to create a Series from a collection is invoking the extension method <code>toSeries()</code> from a list:</p> collection extension<pre><code>Series series = [1, 2, 3].toSeries(\"numbers\")\n</code></pre> <p>Most of the time we will be dealing with a Series creation inside the scope of a Dataframe. Sometimes as the result of the transformation of another series, sometimes because we would like to fill a series from a constant value.</p> <p>Lets say we have a DataFrame with some Series:</p> sample dataframe<pre><code>def numbers = Underdog.df().from([numbers: 1..10], \"numbers\")\n</code></pre> output<pre><code> numbers\n numbers  |\n-----------\n       1  |\n       2  |\n       3  |\n       4  |\n       5  |\n       6  |\n       7  |\n       8  |\n       9  |\n      10  |\n</code></pre> <p>And we want to create a new series named by_two with the result of multiplying all numbers in the numbers series:</p> new series<pre><code>numbers['by_two'] = numbers['numbers'] * 2\n</code></pre> output<pre><code>       numbers\n numbers  |  by_two  |\n----------------------\n       1  |       2  |\n       2  |       4  |\n       3  |       6  |\n       4  |       8  |\n       5  |      10  |\n       6  |      12  |\n       7  |      14  |\n       8  |      16  |\n       9  |      18  |\n      10  |      20  |\n</code></pre> <p>You can also create a new Series inside a dataframe filling all rows with the same value:</p> series from value<pre><code>numbers['one'] = 1\n</code></pre> output<pre><code>           numbers\n numbers  |  by_two  |  one  |\n------------------------------\n       1  |       2  |    1  |\n       2  |       4  |    1  |\n       3  |       6  |    1  |\n       4  |       8  |    1  |\n       5  |      10  |    1  |\n       6  |      12  |    1  |\n       7  |      14  |    1  |\n       8  |      16  |    1  |\n       9  |      18  |    1  |\n      10  |      20  |    1  |\n</code></pre>"},{"location":"dataframe/#statistics","title":"Statistics","text":"<p>TODO</p>"},{"location":"dataframe/tutorial/","title":"Tutorial","text":""},{"location":"dataframe/tutorial/#tutorial","title":"Tutorial","text":""},{"location":"dataframe/tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"dataframe/tutorial/#dependencies","title":"Dependencies","text":"<p>To be able to follow the tutorial you need the <code>underdog-dataframe</code> module. If you're using Gradle in your project:</p> gradle<pre><code>implementation \"com.github.grooviter:underdog-dataframe:VERSION\"\n</code></pre> <p>Or Maven:</p> maven<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-dataframe&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Or if you're using a Groovy script:</p> Grape<pre><code>@Grab(\"com.github.grooviter:underdog-dataframe:VERSION\")\n</code></pre>"},{"location":"dataframe/tutorial/#data","title":"Data","text":"<p>The data we are using in the tutorial is </p>"},{"location":"dataframe/tutorial/#loading-data","title":"Loading data","text":"<p>Here we read a csv file of tornado data. Underdog infers the column types by sampling the data.</p> reading<pre><code>def tornadoes = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> <p>Note the file is addressed relative to the current working directory. You may have to change it for your code.</p>"},{"location":"dataframe/tutorial/#metadata","title":"Metadata","text":"<p>Often, the best way to start is to print the column names for reference:</p> list dataframe column names<pre><code>tornadoes.columns\n</code></pre> output<pre><code>[Date, Time, State, State No, Scale, Injuries, Fatalities, Start Lat, Start Lon, Length, Width]\n</code></pre> <p>The shape() method displays the row and column counts:</p> .shape of the dataframe<pre><code>def (int rows, int cols) = tornadoes.shape()\n</code></pre> shape of the dataframe<pre><code>println(tornadoes.shape())\n</code></pre> output<pre><code>59945 rows X 11 cols\n</code></pre> <p>structure() shows the index, name and type of each column</p> tornadoes schemas<pre><code>// getting tornadoes schema\ndef schema = tornadoes.schema()\n</code></pre> output<pre><code>  Structure of tornadoes_1950-2014.csv\n Index  |  Column Name  |  Column Type  |\n-----------------------------------------\n     0  |         Date  |   LOCAL_DATE  |\n     1  |         Time  |   LOCAL_TIME  |\n     2  |        State  |       STRING  |\n     3  |     State No  |      INTEGER  |\n     4  |        Scale  |      INTEGER  |\n     5  |     Injuries  |      INTEGER  |\n     6  |   Fatalities  |      INTEGER  |\n     7  |    Start Lat  |       DOUBLE  |\n     8  |    Start Lon  |       DOUBLE  |\n     9  |       Length  |       DOUBLE  |\n    10  |        Width  |      INTEGER  |\n</code></pre> <p>Like many Tablesaw methods, structure() returns a table. You can then produce a string representation for display. For convenience, calling toString() on a table invokes print(), which produces a string representation of the table table. To display the table then, you can simply call.</p> <pre><code>println(tornadoes)\n</code></pre> <p>You can also perform other table operations on it. For example, the code below removes all columns whose type isn\u2019t DOUBLE:</p> using schema to change dataframe structure<pre><code>def customSchema = schema[schema['Column Type'] == 'DOUBLE']\n</code></pre> output<pre><code>  Structure of tornadoes_1950-2014.csv\n Index  |  Column Name  |  Column Type  |\n-----------------------------------------\n     7  |    Start Lat  |       DOUBLE  |\n     8  |    Start Lon  |       DOUBLE  |\n     9  |       Length  |       DOUBLE  |\n</code></pre> <p>Of course, that also returned a table. We\u2019ll cover selecting rows in more detail later.</p>"},{"location":"dataframe/tutorial/#previewing","title":"Previewing","text":"<p>The first(n) method returns a new table containing the first n rows.</p> getting first 3 lines<pre><code>tornadoes.head(3)\n</code></pre> output<pre><code>                                tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |  ... |\n</code></pre>"},{"location":"dataframe/tutorial/#transforming","title":"Transforming","text":"<p>Mapping operations take one or more columns as inputs and produce a new column as output. We can map arbitrary expressions onto the table, but many common operations are built in. You can, for example, calculate the difference in days, weeks, or years between the values in two date columns. The method below extracts the Month name from the date column into a new column.</p> creating a new series to dataframe<pre><code>def monthSeries = tornadoes[\"Date\"](LocalDate, String) {\n    it.format(\"MMMM\")\n}\n</code></pre> <p>Now that you have a new column, you can add it to the table:</p> adding month series to tornadoes dataframe<pre><code>tornadoes['month'] = monthSeries\n</code></pre> <p>Of course nothing prevents you from doing everything altogether.</p> <p>You can remove columns from tables to save memory or reduce clutter:</p> remove series from dataframe<pre><code>tornadoes = tornadoes - tornadoes['Date']\n</code></pre>"},{"location":"dataframe/tutorial/#sorting","title":"Sorting","text":"<p>Now lets sort the table in reverse order by the id column. The negative sign before the name indicates a descending sort.</p> sort asc<pre><code>def df1 = tornadoes.sort_values(by: 'Fatalities')\n</code></pre> <p>You can also sort in descending order:</p> sort desc<pre><code>def df2 = tornadoes.sort_values(by: '-Fatalities')\n</code></pre> <p>and even sorting by more than one field:</p> sort by n-fields<pre><code>def df3 = tornadoes.sort_values(by: ['Fatalities', 'State'])\n</code></pre>"},{"location":"dataframe/tutorial/#descriptive-statistics","title":"Descriptive statistics","text":"<p>Descriptive statistics are calculated using the summary() method:</p> printing column insights<pre><code>def columnStats = tornadoes[\"Fatalities\"].describe()\n\nprintln(columnStats)\n</code></pre> <p>Showing the following output:</p> column describe output<pre><code>         Column: Fatalities\n Measure   |         Value         |\n------------------------------------\n        n  |                59945  |\n      sum  |                 6802  |\n     Mean  |  0.11347068145800349  |\n      Min  |                    0  |\n      Max  |                  158  |\n    Range  |                  158  |\n Variance  |    2.901978053261765  |\n Std. Dev  |   1.7035193140266314  |\n</code></pre>"},{"location":"dataframe/tutorial/#filtering","title":"Filtering","text":"<p>You can write your own methods to filter rows, but it\u2019s easier to use the built-in filter classes as shown below:</p> filtering dataframe by column expressions<pre><code>// reading tornadoes\ndef ts = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\n// adding a new series to dataframe with the name of the month\nts['month'] = ts[\"Date\"](LocalDate, String) { it.format(\"MMMM\") }\n\n// filtering\ndef result = ts[\n    ts['Fatalities'] &gt; 0 &amp;                   // at least 1 fatalities\n    ts['month'] == \"April\" &amp;                 // in the month of April\n    (ts['Width'] &gt; 300 | ts['Length'] &gt; 10)  // a tornado with a\n]\n\n// selecting only two columns\ndef stateAndDate = result['State', 'Date']\n</code></pre> output<pre><code> tornadoes_1950-2014.csv\n State  |     Date     |\n------------------------\n    MO  |  1950-01-03  |\n    IL  |  1950-01-03  |\n    OH  |  1950-01-03  |\n</code></pre> <p>The last example above returns a table containing only the columns named in select() parameters,rather than all the columns in the original. Totals and sub-totals</p> <p>Column metrics can be calculated using methods like sum(), product(), mean(), max(), etc.</p> <p>You can apply those methods to a table, calculating results on one column, grouped by the values in another.</p> <pre><code>def tornadoes = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\ndef injuriesByScale = tornadoes\n    .rename(\"Median Injuries by Tornado Scale\")\n    .agg(Injuries: \"median\")\n    .by(\"Scale\")\n    .sort_values(by: \"Scale\")\n</code></pre> <p>This produces the following table, in which Group represents the Tornado Scale and Median the median injures for that group:</p> output<pre><code>Median injuries by Tornado Scale\n Scale  |  Median [Injuries]  |\n-------------------------------\n    -9  |                  0  |\n     0  |                  0  |\n     1  |                  0  |\n     2  |                  0  |\n     3  |                  1  |\n     4  |                 12  |\n     5  |                107  |\n</code></pre>"},{"location":"dataframe/tutorial/#cross-tabs","title":"Cross Tabs","text":"<p>Tablesaw lets you easily produce two-dimensional cross-tabulations (\u201ccross tabs\u201d) of counts and proportions with row and column subtotals. Here\u2019s a count example where we look at the interaction of tornado severity and US state:</p> crosstabs (count)<pre><code>def crossTab = tornadoes.xTabCounts(labels: 'State', values: 'Scale')\n\ncrossTab.head()\n</code></pre> output<pre><code>                       Crosstab Counts: State x Scale\n [labels]  |  -9  |   0    |   1   |   2   |   3   |  4   |  5   |  total  |\n----------------------------------------------------------------------------\n       AL  |   0  |   624  |  770  |  425  |  142  |  38  |  12  |   2011  |\n       AR  |   1  |   486  |  667  |  420  |  162  |  29  |   0  |   1765  |\n       AZ  |   1  |   146  |   71  |   16  |    3  |   0  |   0  |    237  |\n       CA  |   1  |   271  |  117  |   23  |    2  |   0  |   0  |    414  |\n       CO  |   3  |  1322  |  563  |  112  |   22  |   1  |   0  |   2023  |\n       CT  |   0  |    18  |   53  |   22  |    4  |   2  |   0  |     99  |\n       DC  |   0  |     2  |    0  |    0  |    0  |   0  |   0  |      2  |\n       DE  |   0  |    22  |   26  |   12  |    1  |   0  |   0  |     61  |\n       FL  |   2  |  1938  |  912  |  319  |   37  |   3  |   0  |   3211  |\n       GA  |   0  |   413  |  700  |  309  |   74  |  11  |   0  |   1507  |\n</code></pre> <p>Putting it all together</p> <p>Now that you\u2019ve seen the pieces, we can put them together to perform a more complex data analysis. Lets say we want to know how frequently Tornadoes occur in the summer. Here\u2019\u2019s one way to approach that:</p> <p>Let\u2019s start by getting only those tornadoes that occurred in the summer.</p> tornadoes in the summer<pre><code>def ts = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n\n// adding some series to the dataframe to make filtering easier\nts['month']      = ts['Date'](Date, String) { it.format(\"MMMM\") }\nts['dayOfMonth'] = ts['Date'](Date, Integer) { it.format(\"dd\").toInteger() }\n\n// filtering\ndef summer = ts[\n    (ts['month'] == 'June' &amp; ts['dayOfMonth'] &gt; 21) |    // after June the 21st or...\n    (ts['month'] in ['July', 'August']) |                // in July or August or...\n    (ts['month'] == 'September' &amp; ts['dayOfMonth'] &lt; 22) // before September the 22nd\n]\n</code></pre> <p>To get the frequency, we calculate the difference in days between successive tornadoes. The lag() method creates a column where every value equals the previous value (the prior row) of the source column. Then we can simply get the difference in days between the two dates. DateColumn has a method daysUntil() that does this. It returns a NumberColumn that we\u2019ll call \u201cdelta\u201d.</p> lag and delta dates<pre><code>// sorting by Date and Time series\nsummer = summer.sort_values(by: ['Date', 'Time'])\n\n// creating a series with lagged dates\nsummer['Lagged'] = summer['Date'].lag(1)\n\n// creating a series with delta days between lagged dates and summer dates\nsummer['Delta'] = summer['Lagged'] - summer['Date']\n</code></pre> <p>Now we simply calculate the mean of the delta column. Splitting on year keeps us from inadvertently including the time between the last tornado of one summer and the first tornado of the next.</p> create summary<pre><code>// creating year series to be able to group by it\nsummer['year'] = summer['Date'](Date, String) { it.format(\"YYYY\") }\n\n// aggregating delta\ndef summary = summer.agg(Delta: [\"mean\", \"count\"]).by(\"year\")\n\n// print out summary\nprintln(summary)\n</code></pre> <p>Printing summary gives us the answer by year.</p> summary output<pre><code>                           tornadoes_1950-2014.csv summary\n Date year  |  Mean [Date lag(1) - Date[DAYS]]  |  Count [Date lag(1) - Date[DAYS]]  |\n--------------------------------------------------------------------------------------\n      1950  |               2.0555555555555545  |                               162  |\n      1951  |               1.7488584474885829  |                               219  |\n      1952  |               1.8673469387755088  |                               196  |\n      1953  |                0.983870967741935  |                               372  |\n      1954  |               0.8617283950617302  |                               405  |\n</code></pre> <p>To get a DOUBLE for the entire period, we can take the average of the annual means.</p> average second column<pre><code>def meanOfSeries = summary.iloc[__, 1].mean()\n</code></pre> Average days between tornadoes in the summer:<pre><code>0.5931137164104612\n</code></pre>"},{"location":"dataframe/tutorial/#saving-your-data","title":"Saving your data","text":"<p>To save a table, you can write it as a CSV file:</p> saving csv file<pre><code>tornadoes.write().csv(\"rev_tornadoes_1950-2014.csv\");\n</code></pre> <p>And that\u2019s it for the introduction. Please see the User Guide for more information.</p>"},{"location":"dataframe/dataframe/","title":"Index","text":""},{"location":"dataframe/dataframe/#dataframe","title":"DataFrame","text":"<p>Underdog's DataFrame combines tools for working with columnar data as tables (dataframes) and columns (series). And also has extra features such statistical functions and visualizations via  Underdog's plots module.</p>"},{"location":"dataframe/dataframe/#creation","title":"Creation","text":"<p>The easiest way to create a Dataframe is using the Underdog extension method <code>Underdog.df()</code>. Here we're creating an empty DataFrame:</p> empty dataframe<pre><code>DataFrame emptyDataFrame = Underdog.df().empty()\n</code></pre> <p>We can create a dataframe with a series of map entries representing series. In this case the key entry is the name of the series and the value is a collection which will become the content of the series.</p> dataframe from a map<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from the map\nDataFrame map2DataFrame = Underdog.df().from(map, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Underdog dataframe library adds additional methods to collection types so that you can convert from collections to Dataframes. And example is invoking the <code>toDataFrame(...)</code> method from the map directly:</p> map extension<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from a map\nDataFrame map2DataFrame = map.toDataFrame(\"people-dataframe\")\n</code></pre> <p>You can also pass a list of maps to the <code>Underdog.df().from(col, name)</code> method:</p> collection of maps<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = Underdog.df().from(list, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Here there is also an extension method for collections so that, IF your list complies to this structure you can call to the method <code>toDataFrame(name)</code> and create a DataFrame from that collection.</p> collection extension<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = list.toDataFrame(\"people-dataframe\")\n</code></pre>"},{"location":"dataframe/dataframe/#filtering","title":"Filtering","text":"<p>In a dataframe you can filter data by any of the Series the dataframe has.</p>"},{"location":"dataframe/dataframe/#numbers","title":"Numbers","text":"<p>The following dataframe builds a theoretical increase of population in ten years:</p> numbers<pre><code>def df = [\n    years: (1991..2000),\n    population: (1..10).collect { 1000 * it }\n].toDataFrame(\"population increase\")\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n 1994  |        4000  |\n 1995  |        5000  |\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>If we wanted to take the records after year 1995:</p> greater than<pre><code>def last5Years = df[df['years'] &gt; 1995]\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>Or getting records with population less than 4000:</p> less than<pre><code>def yearsWithLessThan = df[df['population'] &lt; 4000]\n</code></pre> output<pre><code>population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n</code></pre>"},{"location":"dataframe/dataframe/#string","title":"String","text":"<p>Of course we can filter by strings. Follow up we've got a dataframe with some employee data:</p> employees<pre><code>def df = [\n    employees: ['Udo', 'John', 'Albert', 'Ronda'],\n    department: ['sales', 'it', 'sales', 'it'],\n    payroll: [10_000, 12_000, 11_000, 13_000]\n].toDataFrame(\"employees\")\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n    Albert  |       sales  |    11000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>Getting employees from sales department:</p> sales<pre><code>def salesPeople = df[df['department'] == 'sales']\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n    Albert  |       sales  |    11000  |\n</code></pre> <p>You can also use to filter by a list of possible choices:</p> in list<pre><code>def employeesByName = df[df['employees'] in ['Ronda', 'Udo']]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>You can even try by a regular expression. Lets look for employees with an 'o' in their name:</p> regex<pre><code>def employeesWithAnO = df[df['employees'] ==~ /.*o.*/ ]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n     Ronda  |          it  |    13000  |\n</code></pre>"},{"location":"dataframe/dataframe/#dates","title":"Dates","text":"<p>Of course in time series is crucial to allow searches by time frame.</p> dates<pre><code>// Using a given date as the beginning of our df dates series\ndef initialDate = LocalDate.parse('01/01/2000', 'dd/MM/yyyy')\n\n// a dataframe containing the simulation of bicycles rented through 2000\ndef df = [\n    dates: (1..365).collect(initialDate::plusDays),\n    rented: (1..365).collect { new Random().nextInt(200) }\n].toDataFrame(\"rented bicycles 2000\")\n</code></pre> output<pre><code>rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-01-02  |      41  |\n2000-01-03  |      47  |\n2000-01-04  |      27  |\n2000-01-05  |      95  |\n2000-01-06  |      30  |\n2000-01-07  |     162  |\n2000-01-08  |      52  |\n2000-01-09  |     197  |\n2000-01-10  |     125  |\n2000-01-11  |      15  |\n       ...  |     ...  |\n</code></pre> <p>What if we'd like to get only those records of december 2000 ?</p> after<pre><code>def lastMonth = df[df['dates'] &gt;= LocalDate.parse('01/12/2000', 'dd/MM/yyyy')]\n</code></pre> output<pre><code>  rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-12-01  |     104  |\n2000-12-02  |     193  |\n2000-12-03  |     107  |\n2000-12-04  |     108  |\n2000-12-05  |     193  |\n2000-12-06  |     165  |\n2000-12-07  |      82  |\n2000-12-08  |      77  |\n2000-12-09  |     176  |\n2000-12-10  |     158  |\n       ...  |     ...  |\n2000-12-31  |     150  |\n</code></pre>"},{"location":"dataframe/dataframe/#summary","title":"Summary","text":"<p>Here you have the tables with the supported operators:</p> <p>Arithmetic</p> Left Right Operator Example Status Series Series + <code>df['a'] + df['b']</code> Yes Series Series - <code>df['a'] - df['b]</code> Yes Series Object + <code>df['a'] + 1</code> Yes Series Object - <code>df['a'] - 1</code> Yes Series Object * <code>df['a'] * 2</code> Yes Series Object / <code>df['a'] / 2</code> Yes <p>filtering operators</p> Type Operator Example Status String == <code>df['a'] == 'x'</code> Yes String != <code>df['a'] != 'x'</code> Yes String ==~ <code>df['a'] ==~ /.*/</code> Yes String in <code>df['a'] in ['x']</code> Yes Number == <code>df['a'] == 1</code> Yes Number != <code>df['a'] != 1</code> Yes Number &gt; <code>df['a'] &gt; 1</code> Yes Number &gt;= <code>df['a'] &gt;= 1</code> Yes Number &lt; <code>df['a'] &lt; 1</code> Yes Number &lt;= <code>df['a'] &lt;= 1</code> Yes LocalDate &gt; <code>df['a'] &gt; date</code> Yes LocalDate &gt;= <code>df['a'] &gt;= date</code> Yes LocalDate &lt; <code>df['a'] &lt; date</code> Yes LocalDate &lt;= <code>df['a'] &lt;= date</code> Yes"},{"location":"dataframe/dataframe/#sorting","title":"Sorting","text":"<p>TODO</p>"},{"location":"dataframe/dataframe/#mapping","title":"Mapping","text":"<p>TODO</p>"},{"location":"dataframe/dataframe/#import-export","title":"Import / Export","text":""},{"location":"dataframe/dataframe/#csv","title":"CSV","text":""},{"location":"dataframe/dataframe/#reading","title":"Reading","text":"<p>You can read csv files via <code>Underdog.df().read_csv(...)</code> method. Here we are importing a csv files containing tornado incidents in the USA:</p> import csv<pre><code>DataFrame dataframe = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> output<pre><code>                               tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |      |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |      |\n1950-01-03  |  11:55:00  |     IL  |         2  |      3  |         3  |      |\n1950-01-03  |  16:00:00  |     OH  |         1  |      1  |         1  |      |\n1950-01-13  |  05:25:00  |     AR  |         1  |      3  |         1  |      |\n1950-01-25  |  19:30:00  |     MO  |         2  |      2  |         5  |      |\n1950-01-25  |  21:00:00  |     IL  |         3  |      2  |         0  |      |\n1950-01-26  |  18:00:00  |     TX  |         1  |      2  |         2  |      |\n1950-02-11  |  13:10:00  |     TX  |         2  |      2  |         0  |      |\n       ...  |       ...  |    ...  |       ...  |    ...  |       ...  |  ... |\n</code></pre>"},{"location":"dataframe/dataframe/#separator","title":"Separator","text":"<p>By default the csv reader assumes the csv file is using comma (,) as the separator character, but you can provide a custom separator. For example the following csv file content:</p> separator<pre><code>name;age\nLorna;34\nYule;63\nTom;28\n</code></pre> <p>Can be read by using the sep argument:</p> custom separator<pre><code>def dataframe = Underdog.df().read_csv(filePath, sep: \";\")\n</code></pre>"},{"location":"dataframe/dataframe/#duplicated-names","title":"Duplicated names","text":"<p>Sometimes you can find a csv where columns are repeated, by default if you don't specify you allow repeated columns the import process will fail. Imagine we've got the following csv:</p> csv with repeated cols<pre><code>bronze,silver,gold,summer_total,bronze,silver,gold,winter_total\n1,2,1,4,1,1,1,3\n</code></pre> <p>To allow repeated columns you should set the <code>allowDuplicatedNames</code> flag to true.</p> allow repeated cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, allowedDuplicatedNames: true)\n</code></pre> <p>Then all repeated names will be prefixed in order with a number to avoid collisions:</p> output<pre><code>                                        io_repeated_cols.csv\n bronze |  silver |  gold |  summer_total |  bronze-2  |  silver-2  |  gold-2  |  winter_total|\n-----------------------------------------------------------------------------------------------\n      1 |       2 |     1 |             4 |         1  |         1  |       1  |           3  |\n</code></pre>"},{"location":"dataframe/dataframe/#missing-values","title":"Missing values","text":"<p>If a csv file contains values which should be considered as well as missing values, we can pass this information before reading the csv file.</p> csv file with missing data<pre><code>from,to,id\nN/C,Madrid,123\nMadrid,Paris,124\nParis,London,125\nLondon,NONE,126\n</code></pre> <p>Here we're considering missing data the values N/C and NONE:</p> considering missing data<pre><code>def dataframe = Underdog.df().read_csv(filePath, nanValues: ['NONE', 'N/C'])\n</code></pre> <p>That will inform the reader to consider cells containing that value as missing values:</p> output<pre><code>io_custom_missing_data.csv\n  from   |    to    |  id   |\n-----------------------------\n         |  Madrid  |  123  |\n Madrid  |   Paris  |  124  |\n  Paris  |  London  |  125  |\n London  |          |  126  |\n</code></pre>"},{"location":"dataframe/dataframe/#date-format","title":"Date format","text":"<p>If your csv files have a custom date format you can provide the date pattern as a parameter. Here we have a file with a custom format:</p> custom date format<pre><code>Date,Close\n2014-12-05 00:00:00+00:00,0.5267500281333923\n2014-12-08 00:00:00+00:00,0.5199999809265137\n2014-12-09 00:00:00+00:00,0.5182499885559082\n</code></pre> <p>Passing the pattern as parameter:</p> custom date format<pre><code>def dataframe = Underdog.df().read_csv(filePath, dateFormat: \"yyyy-MM-dd HH:mm:ss+00:00\")\n</code></pre> <p>Gives the following output:</p> output<pre><code>      io_custom_date_format.csv\n   Date     |        Close         |\n-------------------------------------\n2014-12-05  |  0.5267500281333923  |\n2014-12-08  |  0.5199999809265137  |\n2014-12-09  |  0.5182499885559082  |\n</code></pre>"},{"location":"dataframe/dataframe/#skip-rowsfooter","title":"Skip rows/footer","text":"<p>If you're sure that there is data you'd like to avoid parsing, like nonsense data, you can skip parsing those rows. Check the following example:</p> csv file with comments<pre><code># some information about the data\n# col1: city\n# col2: weight\ncol1,col2\nNC,0\nNC,0\nNC,0\nNC,0\nMadrid,1\nParis,2\nNC,0\nNC,0\nNC,0\nNC,0\n# another comment here\n</code></pre> <p>There are lines we don't want to consider when creating our dataframe:</p> <ul> <li>comments in the beginning of the file (lines 1-3)</li> <li>comments in the end of the file (line 15)</li> <li>rows we don't want to parse because they don't add any meaningful information (4-8 and 11-14)</li> </ul> <p>To avoid parsing any of these lines we can instruct the csv reader to skip lines in the header and/or in the footer of the file:</p> skipping rows<pre><code>def dataframe = Underdog.df()\n    .read_csv(filePath,\n        header: false,    // not using first row as header\n        skipRows: 8,      // skipping rows at the beginning of the file\n        skipFooter: 4     // skipping rows at the end of the file\n    ).renameSeries(columns: ['city', 'id']) // renaming series names with the list passed as parameter\n</code></pre> output<pre><code>io_skipping_rows.csv\n city   |  id  |\n-----------------\nMadrid  |   1  |\n Paris  |   2  |\n</code></pre>"},{"location":"dataframe/dataframe/#max-chars-x-col","title":"Max chars x col","text":"<p>You can instruct the csv reader to avoid parsing columns with more than a number of characters.</p> limiting col chars<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxCharsPerColumn: 20)\n</code></pre> <p>Warning</p> <p>If a column exceeds the number of characters the process will throw an exception</p>"},{"location":"dataframe/dataframe/#max-cols","title":"Max cols","text":"<p>You can instruct the csv reader to avoid parsing more than a given number of columns.</p> limiting number of cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxNumberOfColumns: 2)\n</code></pre> <p>Warning</p> <p>If the number of columns exceeds the number specified the process will throw an exception</p>"},{"location":"dataframe/dataframe/creation/","title":"Creation","text":""},{"location":"dataframe/dataframe/creation/#creation","title":"Creation","text":"<p>The easiest way to create a Dataframe is using the Underdog extension method <code>Underdog.df()</code>. Here we're creating an empty DataFrame:</p> empty dataframe<pre><code>DataFrame emptyDataFrame = Underdog.df().empty()\n</code></pre> <p>We can create a dataframe with a series of map entries representing series. In this case the key entry is the name of the series and the value is a collection which will become the content of the series.</p> dataframe from a map<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from the map\nDataFrame map2DataFrame = Underdog.df().from(map, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Underdog dataframe library adds additional methods to collection types so that you can convert from collections to Dataframes. And example is invoking the <code>toDataFrame(...)</code> method from the map directly:</p> map extension<pre><code>// creating a map\ndef map = [\n    names: [\"John\", \"Laura\", \"Ursula\"],\n    ages: [22, 34, 83]\n]\n\n// creating a dataframe from a map\nDataFrame map2DataFrame = map.toDataFrame(\"people-dataframe\")\n</code></pre> <p>You can also pass a list of maps to the <code>Underdog.df().from(col, name)</code> method:</p> collection of maps<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = Underdog.df().from(list, \"people-dataframe\")\n</code></pre> output<pre><code>people-dataframe\n name   |  age  |\n------------------\n  John  |   22  |\n Laura  |   34  |\nUrsula  |   83  |\n</code></pre> <p>Here there is also an extension method for collections so that, IF your list complies to this structure you can call to the method <code>toDataFrame(name)</code> and create a DataFrame from that collection.</p> collection extension<pre><code>// creating a list of maps\ndef list = [\n    [name: \"John\", age: 22],\n    [name: \"Laura\", age: 34],\n    [name: \"Ursula\", age: 83]\n]\n\n// creating a dataframe from the list\nDataFrame colOfMaps2DataFrame = list.toDataFrame(\"people-dataframe\")\n</code></pre>"},{"location":"dataframe/dataframe/filtering/","title":"Filtering","text":""},{"location":"dataframe/dataframe/filtering/#filtering","title":"Filtering","text":"<p>In a dataframe you can filter data by any of the Series the dataframe has.</p>"},{"location":"dataframe/dataframe/filtering/#numbers","title":"Numbers","text":"<p>The following dataframe builds a theoretical increase of population in ten years:</p> numbers<pre><code>def df = [\n    years: (1991..2000),\n    population: (1..10).collect { 1000 * it }\n].toDataFrame(\"population increase\")\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n 1994  |        4000  |\n 1995  |        5000  |\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>If we wanted to take the records after year 1995:</p> greater than<pre><code>def last5Years = df[df['years'] &gt; 1995]\n</code></pre> output<pre><code>  population increase\nyears  |  population  |\n------------------------\n 1996  |        6000  |\n 1997  |        7000  |\n 1998  |        8000  |\n 1999  |        9000  |\n 2000  |       10000  |\n</code></pre> <p>Or getting records with population less than 4000:</p> less than<pre><code>def yearsWithLessThan = df[df['population'] &lt; 4000]\n</code></pre> output<pre><code>population increase\nyears  |  population  |\n------------------------\n 1991  |        1000  |\n 1992  |        2000  |\n 1993  |        3000  |\n</code></pre>"},{"location":"dataframe/dataframe/filtering/#string","title":"String","text":"<p>Of course we can filter by strings. Follow up we've got a dataframe with some employee data:</p> employees<pre><code>def df = [\n    employees: ['Udo', 'John', 'Albert', 'Ronda'],\n    department: ['sales', 'it', 'sales', 'it'],\n    payroll: [10_000, 12_000, 11_000, 13_000]\n].toDataFrame(\"employees\")\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n    Albert  |       sales  |    11000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>Getting employees from sales department:</p> sales<pre><code>def salesPeople = df[df['department'] == 'sales']\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n    Albert  |       sales  |    11000  |\n</code></pre> <p>You can also use to filter by a list of possible choices:</p> in list<pre><code>def employeesByName = df[df['employees'] in ['Ronda', 'Udo']]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n     Ronda  |          it  |    13000  |\n</code></pre> <p>You can even try by a regular expression. Lets look for employees with an 'o' in their name:</p> regex<pre><code>def employeesWithAnO = df[df['employees'] ==~ /.*o.*/ ]\n</code></pre> output<pre><code>               employees\n employees  |  department  |  payroll  |\n----------------------------------------\n       Udo  |       sales  |    10000  |\n      John  |          it  |    12000  |\n     Ronda  |          it  |    13000  |\n</code></pre>"},{"location":"dataframe/dataframe/filtering/#dates","title":"Dates","text":"<p>Of course in time series is crucial to allow searches by time frame.</p> dates<pre><code>// Using a given date as the beginning of our df dates series\ndef initialDate = LocalDate.parse('01/01/2000', 'dd/MM/yyyy')\n\n// a dataframe containing the simulation of bicycles rented through 2000\ndef df = [\n    dates: (1..365).collect(initialDate::plusDays),\n    rented: (1..365).collect { new Random().nextInt(200) }\n].toDataFrame(\"rented bicycles 2000\")\n</code></pre> output<pre><code>rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-01-02  |      41  |\n2000-01-03  |      47  |\n2000-01-04  |      27  |\n2000-01-05  |      95  |\n2000-01-06  |      30  |\n2000-01-07  |     162  |\n2000-01-08  |      52  |\n2000-01-09  |     197  |\n2000-01-10  |     125  |\n2000-01-11  |      15  |\n       ...  |     ...  |\n</code></pre> <p>What if we'd like to get only those records of december 2000 ?</p> after<pre><code>def lastMonth = df[df['dates'] &gt;= LocalDate.parse('01/12/2000', 'dd/MM/yyyy')]\n</code></pre> output<pre><code>  rented bicycles 2000\n  dates     |  rented  |\n-------------------------\n2000-12-01  |     104  |\n2000-12-02  |     193  |\n2000-12-03  |     107  |\n2000-12-04  |     108  |\n2000-12-05  |     193  |\n2000-12-06  |     165  |\n2000-12-07  |      82  |\n2000-12-08  |      77  |\n2000-12-09  |     176  |\n2000-12-10  |     158  |\n       ...  |     ...  |\n2000-12-31  |     150  |\n</code></pre>"},{"location":"dataframe/dataframe/filtering/#summary","title":"Summary","text":"<p>Here you have the tables with the supported operators:</p> <p>Arithmetic</p> Left Right Operator Example Status Series Series + <code>df['a'] + df['b']</code> Yes Series Series - <code>df['a'] - df['b]</code> Yes Series Object + <code>df['a'] + 1</code> Yes Series Object - <code>df['a'] - 1</code> Yes Series Object * <code>df['a'] * 2</code> Yes Series Object / <code>df['a'] / 2</code> Yes <p>filtering operators</p> Type Operator Example Status String == <code>df['a'] == 'x'</code> Yes String != <code>df['a'] != 'x'</code> Yes String ==~ <code>df['a'] ==~ /.*/</code> Yes String in <code>df['a'] in ['x']</code> Yes Number == <code>df['a'] == 1</code> Yes Number != <code>df['a'] != 1</code> Yes Number &gt; <code>df['a'] &gt; 1</code> Yes Number &gt;= <code>df['a'] &gt;= 1</code> Yes Number &lt; <code>df['a'] &lt; 1</code> Yes Number &lt;= <code>df['a'] &lt;= 1</code> Yes LocalDate &gt; <code>df['a'] &gt; date</code> Yes LocalDate &gt;= <code>df['a'] &gt;= date</code> Yes LocalDate &lt; <code>df['a'] &lt; date</code> Yes LocalDate &lt;= <code>df['a'] &lt;= date</code> Yes"},{"location":"dataframe/dataframe/import_export/","title":"Import export","text":""},{"location":"dataframe/dataframe/import_export/#import-export","title":"Import / Export","text":""},{"location":"dataframe/dataframe/import_export/#csv","title":"CSV","text":""},{"location":"dataframe/dataframe/import_export/#reading","title":"Reading","text":"<p>You can read csv files via <code>Underdog.df().read_csv(...)</code> method. Here we are importing a csv files containing tornado incidents in the USA:</p> import csv<pre><code>DataFrame dataframe = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> output<pre><code>                               tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |      |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |      |\n1950-01-03  |  11:55:00  |     IL  |         2  |      3  |         3  |      |\n1950-01-03  |  16:00:00  |     OH  |         1  |      1  |         1  |      |\n1950-01-13  |  05:25:00  |     AR  |         1  |      3  |         1  |      |\n1950-01-25  |  19:30:00  |     MO  |         2  |      2  |         5  |      |\n1950-01-25  |  21:00:00  |     IL  |         3  |      2  |         0  |      |\n1950-01-26  |  18:00:00  |     TX  |         1  |      2  |         2  |      |\n1950-02-11  |  13:10:00  |     TX  |         2  |      2  |         0  |      |\n       ...  |       ...  |    ...  |       ...  |    ...  |       ...  |  ... |\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#separator","title":"Separator","text":"<p>By default the csv reader assumes the csv file is using comma (,) as the separator character, but you can provide a custom separator. For example the following csv file content:</p> separator<pre><code>name;age\nLorna;34\nYule;63\nTom;28\n</code></pre> <p>Can be read by using the sep argument:</p> custom separator<pre><code>def dataframe = Underdog.df().read_csv(filePath, sep: \";\")\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#duplicated-names","title":"Duplicated names","text":"<p>Sometimes you can find a csv where columns are repeated, by default if you don't specify you allow repeated columns the import process will fail. Imagine we've got the following csv:</p> csv with repeated cols<pre><code>bronze,silver,gold,summer_total,bronze,silver,gold,winter_total\n1,2,1,4,1,1,1,3\n</code></pre> <p>To allow repeated columns you should set the <code>allowDuplicatedNames</code> flag to true.</p> allow repeated cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, allowedDuplicatedNames: true)\n</code></pre> <p>Then all repeated names will be prefixed in order with a number to avoid collisions:</p> output<pre><code>                                        io_repeated_cols.csv\n bronze |  silver |  gold |  summer_total |  bronze-2  |  silver-2  |  gold-2  |  winter_total|\n-----------------------------------------------------------------------------------------------\n      1 |       2 |     1 |             4 |         1  |         1  |       1  |           3  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#missing-values","title":"Missing values","text":"<p>If a csv file contains values which should be considered as well as missing values, we can pass this information before reading the csv file.</p> csv file with missing data<pre><code>from,to,id\nN/C,Madrid,123\nMadrid,Paris,124\nParis,London,125\nLondon,NONE,126\n</code></pre> <p>Here we're considering missing data the values N/C and NONE:</p> considering missing data<pre><code>def dataframe = Underdog.df().read_csv(filePath, nanValues: ['NONE', 'N/C'])\n</code></pre> <p>That will inform the reader to consider cells containing that value as missing values:</p> output<pre><code>io_custom_missing_data.csv\n  from   |    to    |  id   |\n-----------------------------\n         |  Madrid  |  123  |\n Madrid  |   Paris  |  124  |\n  Paris  |  London  |  125  |\n London  |          |  126  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#date-format","title":"Date format","text":"<p>If your csv files have a custom date format you can provide the date pattern as a parameter. Here we have a file with a custom format:</p> custom date format<pre><code>Date,Close\n2014-12-05 00:00:00+00:00,0.5267500281333923\n2014-12-08 00:00:00+00:00,0.5199999809265137\n2014-12-09 00:00:00+00:00,0.5182499885559082\n</code></pre> <p>Passing the pattern as parameter:</p> custom date format<pre><code>def dataframe = Underdog.df().read_csv(filePath, dateFormat: \"yyyy-MM-dd HH:mm:ss+00:00\")\n</code></pre> <p>Gives the following output:</p> output<pre><code>      io_custom_date_format.csv\n   Date     |        Close         |\n-------------------------------------\n2014-12-05  |  0.5267500281333923  |\n2014-12-08  |  0.5199999809265137  |\n2014-12-09  |  0.5182499885559082  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#skip-rowsfooter","title":"Skip rows/footer","text":"<p>If you're sure that there is data you'd like to avoid parsing, like nonsense data, you can skip parsing those rows. Check the following example:</p> csv file with comments<pre><code># some information about the data\n# col1: city\n# col2: weight\ncol1,col2\nNC,0\nNC,0\nNC,0\nNC,0\nMadrid,1\nParis,2\nNC,0\nNC,0\nNC,0\nNC,0\n# another comment here\n</code></pre> <p>There are lines we don't want to consider when creating our dataframe:</p> <ul> <li>comments in the beginning of the file (lines 1-3)</li> <li>comments in the end of the file (line 15)</li> <li>rows we don't want to parse because they don't add any meaningful information (4-8 and 11-14)</li> </ul> <p>To avoid parsing any of these lines we can instruct the csv reader to skip lines in the header and/or in the footer of the file:</p> skipping rows<pre><code>def dataframe = Underdog.df()\n    .read_csv(filePath,\n        header: false,    // not using first row as header\n        skipRows: 8,      // skipping rows at the beginning of the file\n        skipFooter: 4     // skipping rows at the end of the file\n    ).renameSeries(columns: ['city', 'id']) // renaming series names with the list passed as parameter\n</code></pre> output<pre><code>io_skipping_rows.csv\n city   |  id  |\n-----------------\nMadrid  |   1  |\n Paris  |   2  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export/#max-chars-x-col","title":"Max chars x col","text":"<p>You can instruct the csv reader to avoid parsing columns with more than a number of characters.</p> limiting col chars<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxCharsPerColumn: 20)\n</code></pre> <p>Warning</p> <p>If a column exceeds the number of characters the process will throw an exception</p>"},{"location":"dataframe/dataframe/import_export/#max-cols","title":"Max cols","text":"<p>You can instruct the csv reader to avoid parsing more than a given number of columns.</p> limiting number of cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxNumberOfColumns: 2)\n</code></pre> <p>Warning</p> <p>If the number of columns exceeds the number specified the process will throw an exception</p>"},{"location":"dataframe/dataframe/import_export_csv/","title":"Import export csv","text":""},{"location":"dataframe/dataframe/import_export_csv/#csv","title":"CSV","text":""},{"location":"dataframe/dataframe/import_export_csv/#reading","title":"Reading","text":"<p>You can read csv files via <code>Underdog.df().read_csv(...)</code> method. Here we are importing a csv files containing tornado incidents in the USA:</p> import csv<pre><code>DataFrame dataframe = Underdog.df().read_csv(\"src/test/resources/data/tornadoes_1950-2014.csv\")\n</code></pre> output<pre><code>                               tornadoes_1950-2014.csv\n   Date     |    Time    |  State  |  State No  |  Scale  |  Injuries  |  ... |\n--------------------------------------------------------------------------------\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |  ... |\n1950-01-03  |  11:00:00  |     MO  |         1  |      3  |         3  |      |\n1950-01-03  |  11:10:00  |     IL  |         1  |      3  |         0  |      |\n1950-01-03  |  11:55:00  |     IL  |         2  |      3  |         3  |      |\n1950-01-03  |  16:00:00  |     OH  |         1  |      1  |         1  |      |\n1950-01-13  |  05:25:00  |     AR  |         1  |      3  |         1  |      |\n1950-01-25  |  19:30:00  |     MO  |         2  |      2  |         5  |      |\n1950-01-25  |  21:00:00  |     IL  |         3  |      2  |         0  |      |\n1950-01-26  |  18:00:00  |     TX  |         1  |      2  |         2  |      |\n1950-02-11  |  13:10:00  |     TX  |         2  |      2  |         0  |      |\n       ...  |       ...  |    ...  |       ...  |    ...  |       ...  |  ... |\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#separator","title":"Separator","text":"<p>By default the csv reader assumes the csv file is using comma (,) as the separator character, but you can provide a custom separator. For example the following csv file content:</p> separator<pre><code>name;age\nLorna;34\nYule;63\nTom;28\n</code></pre> <p>Can be read by using the sep argument:</p> custom separator<pre><code>def dataframe = Underdog.df().read_csv(filePath, sep: \";\")\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#duplicated-names","title":"Duplicated names","text":"<p>Sometimes you can find a csv where columns are repeated, by default if you don't specify you allow repeated columns the import process will fail. Imagine we've got the following csv:</p> csv with repeated cols<pre><code>bronze,silver,gold,summer_total,bronze,silver,gold,winter_total\n1,2,1,4,1,1,1,3\n</code></pre> <p>To allow repeated columns you should set the <code>allowDuplicatedNames</code> flag to true.</p> allow repeated cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, allowedDuplicatedNames: true)\n</code></pre> <p>Then all repeated names will be prefixed in order with a number to avoid collisions:</p> output<pre><code>                                        io_repeated_cols.csv\n bronze |  silver |  gold |  summer_total |  bronze-2  |  silver-2  |  gold-2  |  winter_total|\n-----------------------------------------------------------------------------------------------\n      1 |       2 |     1 |             4 |         1  |         1  |       1  |           3  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#missing-values","title":"Missing values","text":"<p>If a csv file contains values which should be considered as well as missing values, we can pass this information before reading the csv file.</p> csv file with missing data<pre><code>from,to,id\nN/C,Madrid,123\nMadrid,Paris,124\nParis,London,125\nLondon,NONE,126\n</code></pre> <p>Here we're considering missing data the values N/C and NONE:</p> considering missing data<pre><code>def dataframe = Underdog.df().read_csv(filePath, nanValues: ['NONE', 'N/C'])\n</code></pre> <p>That will inform the reader to consider cells containing that value as missing values:</p> output<pre><code>io_custom_missing_data.csv\n  from   |    to    |  id   |\n-----------------------------\n         |  Madrid  |  123  |\n Madrid  |   Paris  |  124  |\n  Paris  |  London  |  125  |\n London  |          |  126  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#date-format","title":"Date format","text":"<p>If your csv files have a custom date format you can provide the date pattern as a parameter. Here we have a file with a custom format:</p> custom date format<pre><code>Date,Close\n2014-12-05 00:00:00+00:00,0.5267500281333923\n2014-12-08 00:00:00+00:00,0.5199999809265137\n2014-12-09 00:00:00+00:00,0.5182499885559082\n</code></pre> <p>Passing the pattern as parameter:</p> custom date format<pre><code>def dataframe = Underdog.df().read_csv(filePath, dateFormat: \"yyyy-MM-dd HH:mm:ss+00:00\")\n</code></pre> <p>Gives the following output:</p> output<pre><code>      io_custom_date_format.csv\n   Date     |        Close         |\n-------------------------------------\n2014-12-05  |  0.5267500281333923  |\n2014-12-08  |  0.5199999809265137  |\n2014-12-09  |  0.5182499885559082  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#skip-rowsfooter","title":"Skip rows/footer","text":"<p>If you're sure that there is data you'd like to avoid parsing, like nonsense data, you can skip parsing those rows. Check the following example:</p> csv file with comments<pre><code># some information about the data\n# col1: city\n# col2: weight\ncol1,col2\nNC,0\nNC,0\nNC,0\nNC,0\nMadrid,1\nParis,2\nNC,0\nNC,0\nNC,0\nNC,0\n# another comment here\n</code></pre> <p>There are lines we don't want to consider when creating our dataframe:</p> <ul> <li>comments in the beginning of the file (lines 1-3)</li> <li>comments in the end of the file (line 15)</li> <li>rows we don't want to parse because they don't add any meaningful information (4-8 and 11-14)</li> </ul> <p>To avoid parsing any of these lines we can instruct the csv reader to skip lines in the header and/or in the footer of the file:</p> skipping rows<pre><code>def dataframe = Underdog.df()\n    .read_csv(filePath,\n        header: false,    // not using first row as header\n        skipRows: 8,      // skipping rows at the beginning of the file\n        skipFooter: 4     // skipping rows at the end of the file\n    ).renameSeries(columns: ['city', 'id']) // renaming series names with the list passed as parameter\n</code></pre> output<pre><code>io_skipping_rows.csv\n city   |  id  |\n-----------------\nMadrid  |   1  |\n Paris  |   2  |\n</code></pre>"},{"location":"dataframe/dataframe/import_export_csv/#max-chars-x-col","title":"Max chars x col","text":"<p>You can instruct the csv reader to avoid parsing columns with more than a number of characters.</p> limiting col chars<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxCharsPerColumn: 20)\n</code></pre> <p>Warning</p> <p>If a column exceeds the number of characters the process will throw an exception</p>"},{"location":"dataframe/dataframe/import_export_csv/#max-cols","title":"Max cols","text":"<p>You can instruct the csv reader to avoid parsing more than a given number of columns.</p> limiting number of cols<pre><code>def dataframe = Underdog.df().read_csv(filePath, maxNumberOfColumns: 2)\n</code></pre> <p>Warning</p> <p>If the number of columns exceeds the number specified the process will throw an exception</p>"},{"location":"dataframe/dataframe/mapping/","title":"Mapping","text":""},{"location":"dataframe/dataframe/mapping/#mapping","title":"Mapping","text":"<p>TODO</p>"},{"location":"dataframe/dataframe/sorting/","title":"Sorting","text":""},{"location":"dataframe/dataframe/sorting/#sorting","title":"Sorting","text":"<p>TODO</p>"},{"location":"dataframe/series/","title":"Index","text":""},{"location":"dataframe/series/#series","title":"Series","text":""},{"location":"dataframe/series/#intro","title":"Intro","text":"<p>A Series object represents a named one-dimensional array. It also supports operations and statistical methods. It also has operations to deal with missing values. You can create a Series object from different sources:</p> create<pre><code>// from a range of numbers\ndef numbers = (1..4).toSeries(\"numbers\")\n\n// from a range of letters\ndef letters = ('A'..'C').toSeries(\"letters\")\n\n// from a list\ndef stuff = [1, 2, null, 3, 4].toSeries(\"stuff\")\n</code></pre> <p>You can use operator symbols to apply simple operations over the Series object:</p> operations<pre><code>// multiplying a series by a number\ndef doubleSeries = numbers * 2\n\n// multiplying a series by another series\ndef rowProduct = numbers * stuff.dropna()\n\n// dividing a series\ndef halves = stuff / 2\n\n// using custom transformation to create a new series\ndef custom = letters(String, String) { \"letter-$it\".toString() }\n</code></pre> <p>Sometimes you may want to analyze a given Series object by using statistical methods:</p> statistics<pre><code>def mean = doubleSeries.mean()\ndef max = doubleSeries.max()\ndef min = doubleSeries.min()\ndef avg = doubleSeries.avg()\n</code></pre> <p>You can find all statistical available methods in the <code>underdog.impl.extensions.SeriesStatsExtensions</code> class.</p>"},{"location":"dataframe/series/#creating","title":"Creating","text":"<p>Series are meant to be created from collections or as a transformation from another Series.</p> <p>The only way to create a Series from a collection is invoking the extension method <code>toSeries()</code> from a list:</p> collection extension<pre><code>Series series = [1, 2, 3].toSeries(\"numbers\")\n</code></pre> <p>Most of the time we will be dealing with a Series creation inside the scope of a Dataframe. Sometimes as the result of the transformation of another series, sometimes because we would like to fill a series from a constant value.</p> <p>Lets say we have a DataFrame with some Series:</p> sample dataframe<pre><code>def numbers = Underdog.df().from([numbers: 1..10], \"numbers\")\n</code></pre> output<pre><code> numbers\n numbers  |\n-----------\n       1  |\n       2  |\n       3  |\n       4  |\n       5  |\n       6  |\n       7  |\n       8  |\n       9  |\n      10  |\n</code></pre> <p>And we want to create a new series named by_two with the result of multiplying all numbers in the numbers series:</p> new series<pre><code>numbers['by_two'] = numbers['numbers'] * 2\n</code></pre> output<pre><code>       numbers\n numbers  |  by_two  |\n----------------------\n       1  |       2  |\n       2  |       4  |\n       3  |       6  |\n       4  |       8  |\n       5  |      10  |\n       6  |      12  |\n       7  |      14  |\n       8  |      16  |\n       9  |      18  |\n      10  |      20  |\n</code></pre> <p>You can also create a new Series inside a dataframe filling all rows with the same value:</p> series from value<pre><code>numbers['one'] = 1\n</code></pre> output<pre><code>           numbers\n numbers  |  by_two  |  one  |\n------------------------------\n       1  |       2  |    1  |\n       2  |       4  |    1  |\n       3  |       6  |    1  |\n       4  |       8  |    1  |\n       5  |      10  |    1  |\n       6  |      12  |    1  |\n       7  |      14  |    1  |\n       8  |      16  |    1  |\n       9  |      18  |    1  |\n      10  |      20  |    1  |\n</code></pre>"},{"location":"dataframe/series/#statistics","title":"Statistics","text":"<p>TODO</p>"},{"location":"dataframe/series/creation/","title":"Creation","text":""},{"location":"dataframe/series/creation/#creating","title":"Creating","text":"<p>Series are meant to be created from collections or as a transformation from another Series.</p> <p>The only way to create a Series from a collection is invoking the extension method <code>toSeries()</code> from a list:</p> collection extension<pre><code>Series series = [1, 2, 3].toSeries(\"numbers\")\n</code></pre> <p>Most of the time we will be dealing with a Series creation inside the scope of a Dataframe. Sometimes as the result of the transformation of another series, sometimes because we would like to fill a series from a constant value.</p> <p>Lets say we have a DataFrame with some Series:</p> sample dataframe<pre><code>def numbers = Underdog.df().from([numbers: 1..10], \"numbers\")\n</code></pre> output<pre><code> numbers\n numbers  |\n-----------\n       1  |\n       2  |\n       3  |\n       4  |\n       5  |\n       6  |\n       7  |\n       8  |\n       9  |\n      10  |\n</code></pre> <p>And we want to create a new series named by_two with the result of multiplying all numbers in the numbers series:</p> new series<pre><code>numbers['by_two'] = numbers['numbers'] * 2\n</code></pre> output<pre><code>       numbers\n numbers  |  by_two  |\n----------------------\n       1  |       2  |\n       2  |       4  |\n       3  |       6  |\n       4  |       8  |\n       5  |      10  |\n       6  |      12  |\n       7  |      14  |\n       8  |      16  |\n       9  |      18  |\n      10  |      20  |\n</code></pre> <p>You can also create a new Series inside a dataframe filling all rows with the same value:</p> series from value<pre><code>numbers['one'] = 1\n</code></pre> output<pre><code>           numbers\n numbers  |  by_two  |  one  |\n------------------------------\n       1  |       2  |    1  |\n       2  |       4  |    1  |\n       3  |       6  |    1  |\n       4  |       8  |    1  |\n       5  |      10  |    1  |\n       6  |      12  |    1  |\n       7  |      14  |    1  |\n       8  |      16  |    1  |\n       9  |      18  |    1  |\n      10  |      20  |    1  |\n</code></pre>"},{"location":"dataframe/series/intro/","title":"Intro","text":""},{"location":"dataframe/series/intro/#intro","title":"Intro","text":"<p>A Series object represents a named one-dimensional array. It also supports operations and statistical methods. It also has operations to deal with missing values. You can create a Series object from different sources:</p> create<pre><code>// from a range of numbers\ndef numbers = (1..4).toSeries(\"numbers\")\n\n// from a range of letters\ndef letters = ('A'..'C').toSeries(\"letters\")\n\n// from a list\ndef stuff = [1, 2, null, 3, 4].toSeries(\"stuff\")\n</code></pre> <p>You can use operator symbols to apply simple operations over the Series object:</p> operations<pre><code>// multiplying a series by a number\ndef doubleSeries = numbers * 2\n\n// multiplying a series by another series\ndef rowProduct = numbers * stuff.dropna()\n\n// dividing a series\ndef halves = stuff / 2\n\n// using custom transformation to create a new series\ndef custom = letters(String, String) { \"letter-$it\".toString() }\n</code></pre> <p>Sometimes you may want to analyze a given Series object by using statistical methods:</p> statistics<pre><code>def mean = doubleSeries.mean()\ndef max = doubleSeries.max()\ndef min = doubleSeries.min()\ndef avg = doubleSeries.avg()\n</code></pre> <p>You can find all statistical available methods in the <code>underdog.impl.extensions.SeriesStatsExtensions</code> class.</p>"},{"location":"dataframe/series/statistics/","title":"Statistics","text":""},{"location":"dataframe/series/statistics/#statistics","title":"Statistics","text":"<p>TODO</p>"},{"location":"development/","title":"Development","text":"<p>TODO</p>"},{"location":"graphs/","title":"Graphs","text":"<p>Underdog's Graphs module can be used for exploring graph theory problems. It's built on top of the JGraphT java library adding a Groovy DSL and some extension methods to make certain operations easier.</p>"},{"location":"graphs/#tutorial","title":"Tutorial","text":""},{"location":"graphs/#prerequisites","title":"Prerequisites","text":""},{"location":"graphs/#dependencies","title":"Dependencies","text":"<p>The modules required to follow this tutorial are the <code>graphs</code> and <code>plots</code> modules:</p> gradle<pre><code>implementation 'com.github.grooviter:underdog-graphs:VERSION'\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <p>or if you're using Maven:</p> maven<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-graphs&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"graphs/#creating-a-graph","title":"Creating a graph","text":"<p>Creating an empty graph without vertices and edges:</p> create a graph<pre><code>import underdog.Underdog\n\ndef graph = Underdog.graphs().graph(String) {\n    // vertices and edges here\n}\n</code></pre> <p>The <code>graph(Class)</code> informs what is the type of the vertices this graph is going to have. This time vertices in this graph will be of type <code>String</code> but vertices could be potentially of any type.</p>"},{"location":"graphs/#vertices-nodes","title":"Vertices / Nodes","text":"<p>Lets create a graph with two nodes without any edge between them:</p> <p> </p> <p>There are a couple of ways of adding more vertices to a graph. One is when creating the graph:</p> adding vertices at creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    vertex(\"A\")\n    vertex(\"B\")\n}\n</code></pre> <p>You can also add more vertices after the graph has been created:</p> adding vertices after creation<pre><code>def graph = Underdog.graphs().graph(String) {}\n\ngraph.addVertex(\"A\")\ngraph.addVertex(\"B\")\n</code></pre> <p>You can add simple type vertices, but you can also add more complex objects. Imagine we can add the relationships between employees in a given company. First lets define the <code>Employee</code> class:</p> Employee<pre><code>/*\n * @Canonical implements equals and hashcode among other things.\n * These methods will help the graph to id each node in the graph.\n*/\n@groovy.transform.Canonical\nstatic class Employee {\n    String name, department\n}\n</code></pre> <p>Now we can create a Graph and add relationships between employees:</p> Adding employees<pre><code>def john = new Employee(\"John\", \"Engineering\")\ndef peter = new Employee(\"Peter\", \"Engineering\")\ndef lisa = new Employee(\"Lisa\", \"Engineering\")\n\ndef graph = Underdog.graphs().graph(Employee) {\n    vertex(john)\n    vertex(peter)\n    vertex(lisa)\n}\n</code></pre>"},{"location":"graphs/#edges","title":"Edges","text":"<p>Graphs normally are not very useful without setting edges between nodes. Lets add an edge between two nodes:</p> <p> </p> <p>We can add vertices at creation time:</p> Adding edges at creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices first\n    vertex('A')\n    vertex('B')\n\n    // then adding edges between vertices\n    edge('A', 'B')\n}\n</code></pre> <p>But it is also possible to add edges after the graph has been created:</p> Adding edges after creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices 'A', 'B', 'C', 'D'\n    ('A'..'D').each(delegate::vertex)\n}\n\n// adding edge between 'A' and 'B'\ngraph.addEdge('A', 'B')\n</code></pre> <p>You can also add several edges at once using <code>edges(...)</code></p> <p> </p> adding several edges<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices first\n    ('A'..'D').each(delegate::vertex)\n\n    // then adding several edges\n    edges(\n        'A', 'B',\n        'B', 'C',\n        'C', 'D'\n    )\n}\n</code></pre> <p>We can at any point ask about how many vertices and edges there are in the graph:</p> shape<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('A'..'D').each(delegate::vertex)\n    edge('A', 'B')\n    edge('B', 'C')\n    edge('C', 'D')\n}\n\n// getting vertices and edges count\ndef (nVertices, nEdges) = graph.shape()\n\n// or just println shape\nprintln(graph.shape())\n</code></pre> <p>which prints:</p> output<pre><code>4 vertices X 3 edges\n</code></pre>"},{"location":"graphs/#elements-of-a-graph","title":"Elements of a graph","text":"<ul> <li>vertices</li> <li>edges</li> <li>adjacent</li> <li>degree</li> </ul> <p>Lets create a graph first and then ask for its elements:</p> graph<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..10).each(delegate::vertex)\n    edges(\n        1, 2,\n        1, 3,\n        3, 4\n    )\n}\n</code></pre> <p>The lets ask for its vertices, edges, neighbors.</p> graph elements<pre><code>//  getting graph vertices\ngraph.vertices.containsAll(1..10)\n\n// getting graph edges\ngraph.edges.size() == 3\n\n// getting neighbors of a specific vertex\ngraph.neighborsOf(1) == [2, 3]\n</code></pre>"},{"location":"graphs/#removing-elements","title":"Removing elements","text":"<p>Here there are some example on how to remove vertices and edges from a given graph. When using the operator minus (<code>-</code>) the result return the graph minus the element removed whereas the <code>removeXXX(,,,)</code> functions only return a boolean value: true if the element was successfully removed, or false if it couldn't be removed from graph.</p> removing elements<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..14).each(delegate::vertex)\n    edges(\n        3, 5,\n        5, 7,\n        7, 9,\n        9, 11,\n        11, 12,\n        12, 14\n    )\n}\n\n// removing vertices using - operator\ndef graph2 = graph - [2, 4, 6, 8, 10]\ndef graph3 = graph2 - 1\n\n// removing vertices using function\ngraph3.removeVertex(3)\ngraph3.removeAllVertices([5, 7])\n\n// removing edge using - operator\ndef graph4 = graph3 - graph3.edgesOf(9).first()\n\n// removing edges\ngraph4.removeEdge(graph4.edgesOf(11).first())\n// graph4.removeAllEdges(graph4.edgesOf(12)) &lt;--- this fails: ConcurrentModificationException\ngraph4.removeAllEdges(graph4.edgesOf(12).toList()) // &lt;--- this works\n</code></pre>"},{"location":"graphs/#graph-types","title":"Graph types","text":"<p>You can create different types of graphs. There are a couple of methods you can use:</p> graph types<pre><code>// graphs\ndef g = Underdog.graphs()\n\n// undirected weighted\ndef graph1 = g.graph(String)\n\n// directed weighted\ndef graph2 = g.digraph(String)\n\n// directed weighted pseudo graph\ndef graph3 = g.multidigraph(String)\n\n// weighted pseudo graph\ndef graph4 = g.multigraph(String)\n</code></pre> <p>Tip</p> <p>Because graphs in underdog are using JGraphT underneath you can always create an instance of any type of graph  directly using JGraphT api.</p>"},{"location":"graphs/#what-to-use-as-vertices","title":"What to use as vertices","text":"<p>You can use almost anything as a vertex. The only mandatory condition is that the graph must be able to distinguish between vertices. For that your vertex should implement both equals and hashcode methods. In Groovy you can use the <code>@Canonical</code> annotation to get that.</p>"},{"location":"graphs/#analyzing-graphs","title":"Analyzing graphs","text":"<p>The structure of the graph can be analyzed by using various functions.</p> graph<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('a'..'f').each(delegate::vertex)\n\n    edges(\n        'a', 'b',\n        'a', 'c',\n        'a', 'd',\n        'b', 'e',\n    )\n}\n</code></pre> <p>What is the shape of the graph ?</p> shape<pre><code>def (nVertices, nEdges) = graph.shape()\n</code></pre> <p>What is the clustering of the graph ?</p> clustering of the graph<pre><code>def graphClustering = graph.clusteringGlobal()\n</code></pre> <p>What is the clustering avg ?</p> clustering average<pre><code>def graphAvg = graph.clusteringAvg()\n</code></pre> <p>And what about the clustering of a given vertex ?</p> clustering of a vertex<pre><code>def vertexClustering = graph.clusteringOf('a')\n</code></pre> <p>What is the vertex with max degree ?</p> max degree<pre><code>String maxDegreeVertex = graph.maxDegree()\n</code></pre> <p>Lets say we want to sort vertices by degree in descending order:</p> sort vertices by degree (desc)<pre><code>def sortedVertices = graph.vertices.sort { -graph.degreeOf(it) }\n</code></pre>"},{"location":"graphs/#traversal","title":"Traversal","text":"<p>To traverse a graph you can directly access the vertices or edges sets and use normal Groovy/Java mechanisms. In the following example we are looking in all vertices to find all vertices having the number two as a neighbor</p> collections (vertices)<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..10).each(delegate::vertex)\n    edges(\n        1, 3,\n        1, 5,\n        1, 7,\n        1, 9,\n        1, 2  // &lt;--- looking here\n    )\n}\n\ndef answer1 = graph.vertices.findAll {\n    graph.neighborsOf(it).any { it == 2 }\n}\n</code></pre> output<pre><code>1\n</code></pre> <p>Now in the next example we are exploring boss-employee relationships and we'd like to find all the bosses names:</p> collections (edges)<pre><code>def anna = new Person(\"Anna\", 24)\ndef chris = new Person(\"Chris\", 26)\ndef paul = new Person(\"Paul\", 30)\ndef john = new Person(\"John\", 28)\n\ndef employees = Underdog.graphs().graph(Person) {\n    // adding people\n    [anna, chris, paul, john].each(delegate::vertex)\n    edge(chris, paul, \"boss\")\n    edge(anna, john, \"boss\")\n    edge(chris, anna, \"mentor\")\n}\n\ndef bossesNames = employees.edges                        // search in all edges where...\n    .findAll { it.relation == 'boss' }            // boss-employee relationship\n    .collect { employees.verticesOf(it)[0].name } // get only the boss name\n</code></pre> <p>Sometimes the graph could be bigger and more complex and we could benefit from using breadthFirst (*) or depthFirst (**) algorithms:</p> depthFirst<pre><code>def anna = new Person(\"Anna\", 24)\ndef chris = new Person(\"Chris\", 26)\ndef paul = new Person(\"Paul\", 30)\ndef john = new Person(\"John\", 28)\n\ndef dates = Underdog.graphs().graph(Person){\n    // adding people\n    [anna, chris, paul, john].each(delegate::vertex)\n\n    // adding dates\n    edges(\n        anna, paul,\n        anna, chris,\n        chris, paul,\n        anna, john\n    )\n}\n\ndef answer = dates.'**'.find { Person person -&gt;\n    // less than 30 years\n    person.age &lt; 30 &amp;&amp;\n    // have dated John\n    dates.neighborsOf(person).any { p -&gt; p.name == \"John\"}\n}\n</code></pre>"},{"location":"graphs/#introspection","title":"Introspection","text":"<p>You can ask the tree for different attributes such as the degrees:</p> max degree<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('A'..'C').each {\n        vertex(it)\n    }\n\n    edge('A', 'B')\n    edge('A', 'C')\n}\n\ndef node = graph.maxDegree()\n</code></pre> <p>Imagine a more complex example where our nodes are beans. Here we have a class representing a person:</p> Person<pre><code>class Person {\n    String name\n    Integer age\n\n    // this is used for destructuring eg:\n    // def (name, age) = somePerson\n    //       ^     ^\n    //       |     |\n    //       0     1\n    Object getAt(Integer index) {\n        return [name, age][index]\n    }\n}\n</code></pre> <p>This class implements the method <code>getAt(int)</code> which allows to extract the object attribute values via  Groovy destructuring. That would become handy later on.</p> <p>Note</p> <p>You can learn more about Groovy destructuring in the Groovy docs</p> <p>We create the graph:</p> Graph using complex objects<pre><code>def john = new Person(name: \"John\", age: 23)\ndef elsa = new Person (name: \"Elsa\", age: 32)\ndef raul = new Person(name: \"Raul\", age: 28)\n\ndef graph = Underdog.graphs().digraph(Person) {\n    vertex(john)\n    vertex(elsa)\n    vertex(raul)\n\n    edge(john, elsa)\n    edge(john, raul)\n}\n</code></pre> <p>Then look for the person with more relationship and extract that person name and age:</p> Person with more relationships<pre><code>def (name, age) = graph.maxDegree()\n</code></pre> <p>Do you remember we implemented destructuring for the Person class ? Here the <code>maxDegree()</code> function returns an instance of type Person, therefore we can access the properties name and age using destructuring knowing that the property in index 0 is the name and the property in index 1 is age.</p>"},{"location":"graphs/#distances","title":"Distances","text":"<p>Sometimes comes handy to know what is the shortest path from one node to another. It could be anything: How many people do I have to meet to meet a specific person ? How many cities do I have to visit before getting to a specific location ? ...etc</p> <p>Here's a very naive example with cities. The following graph contains cities and the edges represent how many kilometers there are between them.</p> <p>We'd like to use the graph as a route planner to get from point A to B. First we need to create our graph:</p> cities distances graph<pre><code>def distances = Underdog.graphs().graph(String) {\n    [\"Madrid\", \"Guadalajara\", \"Cuenca\", \"Zaragoza\", \"Teruel\", \"Castellon\"].each(delegate::vertex)\n    edge(\"Madrid\", \"Guadalajara\", weight: 66.4)\n    edge(\"Madrid\", \"Salamanca\", weight: 210)\n    edge(\"Guadalajara\", \"Zaragoza\", weight: 256.9)\n    edge(\"Zaragoza\", \"Cuenca\", weight: 290.2)\n    edge(\"Cuenca\", \"Teruel\", weight: 147.9)\n    edge(\"Teruel\", \"Castellon\", weight: 144.2)\n}\n</code></pre> <p> </p> <p>See how the weight of the edges represent the km between then. Then I'd like to know how many kilometers I'm going to drive if I'd like to go from Teruel to Madrid:</p> kms<pre><code>def kmsDriven = distances\n    .shortestPathEdges(\"Teruel\", \"Madrid\")\n    .sum { it.weight }\n</code></pre> <p>I've taken the edges and I've added up all weights to get the whole trip in kms:</p> output<pre><code>761.4\n</code></pre> <p>To get the city names I'm asking for the shortest path getting the vertices this time:</p> cities visited<pre><code>def citiesVisited = distances.shortestPathVertices(\"Teruel\", \"Madrid\")\n</code></pre> output<pre><code>[\"Teruel\", \"Cuenca\", \"Zaragoza\", \"Guadalajara\", \"Madrid\"]\n</code></pre> <p> </p> <p>If you are not sure whether you are interested in vertices or edges, or maybe you are interested in both, just use <code>shortestPath</code>:</p> shortestPath<pre><code>def shortestPath = distances.shortestPath(\"Teruel\", \"Madrid\")\n</code></pre> <p>And access the resulting object:</p> shortestPath attributes<pre><code>// kms\nshortestPath.weight == 761.4\n\n// steps (edges)\nshortestPath.length == 4\nshortestPath.vertexList == [\"Teruel\", \"Cuenca\", \"Zaragoza\", \"Guadalajara\", \"Madrid\"] // cities (vertices)\n</code></pre>"},{"location":"graphs/#operators","title":"Operators","text":"<p>You can apply arithmetic operations over graphs. Here for example you can apply a union operation over two graphs using the <code>+</code> operator:</p> merging graphs<pre><code>def names1 = Underdog.graphs().graph(String) {\n    [\"John\", \"Lisa\", \"Robert\"].each(delegate::vertex)\n}\n\ndef names2 = Underdog.graphs().graph(String) {\n    [\"Anna\", \"Vesper\", \"Tania\"].each(delegate::vertex)\n}\n\ndef names3 = names1 + names2\n</code></pre> <p>You can check how the result has all the vertices from the previous merged graphs:</p> output<pre><code>assert names3.vertices == [\"John\", \"Lisa\", \"Robert\", \"Anna\", \"Vesper\", \"Tania\"] as Set\n</code></pre>"},{"location":"graphs/distances/","title":"Distances","text":""},{"location":"graphs/distances/#distances","title":"Distances","text":"<p>Sometimes comes handy to know what is the shortest path from one node to another. It could be anything: How many people do I have to meet to meet a specific person ? How many cities do I have to visit before getting to a specific location ? ...etc</p> <p>Here's a very naive example with cities. The following graph contains cities and the edges represent how many kilometers there are between them.</p> <p>We'd like to use the graph as a route planner to get from point A to B. First we need to create our graph:</p> cities distances graph<pre><code>def distances = Underdog.graphs().graph(String) {\n    [\"Madrid\", \"Guadalajara\", \"Cuenca\", \"Zaragoza\", \"Teruel\", \"Castellon\"].each(delegate::vertex)\n    edge(\"Madrid\", \"Guadalajara\", weight: 66.4)\n    edge(\"Madrid\", \"Salamanca\", weight: 210)\n    edge(\"Guadalajara\", \"Zaragoza\", weight: 256.9)\n    edge(\"Zaragoza\", \"Cuenca\", weight: 290.2)\n    edge(\"Cuenca\", \"Teruel\", weight: 147.9)\n    edge(\"Teruel\", \"Castellon\", weight: 144.2)\n}\n</code></pre> <p> </p> <p>See how the weight of the edges represent the km between then. Then I'd like to know how many kilometers I'm going to drive if I'd like to go from Teruel to Madrid:</p> kms<pre><code>def kmsDriven = distances\n    .shortestPathEdges(\"Teruel\", \"Madrid\")\n    .sum { it.weight }\n</code></pre> <p>I've taken the edges and I've added up all weights to get the whole trip in kms:</p> output<pre><code>761.4\n</code></pre> <p>To get the city names I'm asking for the shortest path getting the vertices this time:</p> cities visited<pre><code>def citiesVisited = distances.shortestPathVertices(\"Teruel\", \"Madrid\")\n</code></pre> output<pre><code>[\"Teruel\", \"Cuenca\", \"Zaragoza\", \"Guadalajara\", \"Madrid\"]\n</code></pre> <p> </p> <p>If you are not sure whether you are interested in vertices or edges, or maybe you are interested in both, just use <code>shortestPath</code>:</p> shortestPath<pre><code>def shortestPath = distances.shortestPath(\"Teruel\", \"Madrid\")\n</code></pre> <p>And access the resulting object:</p> shortestPath attributes<pre><code>// kms\nshortestPath.weight == 761.4\n\n// steps (edges)\nshortestPath.length == 4\nshortestPath.vertexList == [\"Teruel\", \"Cuenca\", \"Zaragoza\", \"Guadalajara\", \"Madrid\"] // cities (vertices)\n</code></pre>"},{"location":"graphs/introspection/","title":"Introspection","text":""},{"location":"graphs/introspection/#introspection","title":"Introspection","text":"<p>You can ask the tree for different attributes such as the degrees:</p> max degree<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('A'..'C').each {\n        vertex(it)\n    }\n\n    edge('A', 'B')\n    edge('A', 'C')\n}\n\ndef node = graph.maxDegree()\n</code></pre> <p>Imagine a more complex example where our nodes are beans. Here we have a class representing a person:</p> Person<pre><code>class Person {\n    String name\n    Integer age\n\n    // this is used for destructuring eg:\n    // def (name, age) = somePerson\n    //       ^     ^\n    //       |     |\n    //       0     1\n    Object getAt(Integer index) {\n        return [name, age][index]\n    }\n}\n</code></pre> <p>This class implements the method <code>getAt(int)</code> which allows to extract the object attribute values via  Groovy destructuring. That would become handy later on.</p> <p>Note</p> <p>You can learn more about Groovy destructuring in the Groovy docs</p> <p>We create the graph:</p> Graph using complex objects<pre><code>def john = new Person(name: \"John\", age: 23)\ndef elsa = new Person (name: \"Elsa\", age: 32)\ndef raul = new Person(name: \"Raul\", age: 28)\n\ndef graph = Underdog.graphs().digraph(Person) {\n    vertex(john)\n    vertex(elsa)\n    vertex(raul)\n\n    edge(john, elsa)\n    edge(john, raul)\n}\n</code></pre> <p>Then look for the person with more relationship and extract that person name and age:</p> Person with more relationships<pre><code>def (name, age) = graph.maxDegree()\n</code></pre> <p>Do you remember we implemented destructuring for the Person class ? Here the <code>maxDegree()</code> function returns an instance of type Person, therefore we can access the properties name and age using destructuring knowing that the property in index 0 is the name and the property in index 1 is age.</p>"},{"location":"graphs/operators/","title":"Operators","text":""},{"location":"graphs/operators/#operators","title":"Operators","text":"<p>You can apply arithmetic operations over graphs. Here for example you can apply a union operation over two graphs using the <code>+</code> operator:</p> merging graphs<pre><code>def names1 = Underdog.graphs().graph(String) {\n    [\"John\", \"Lisa\", \"Robert\"].each(delegate::vertex)\n}\n\ndef names2 = Underdog.graphs().graph(String) {\n    [\"Anna\", \"Vesper\", \"Tania\"].each(delegate::vertex)\n}\n\ndef names3 = names1 + names2\n</code></pre> <p>You can check how the result has all the vertices from the previous merged graphs:</p> output<pre><code>assert names3.vertices == [\"John\", \"Lisa\", \"Robert\", \"Anna\", \"Vesper\", \"Tania\"] as Set\n</code></pre>"},{"location":"graphs/traversal/","title":"Traversal","text":""},{"location":"graphs/traversal/#traversal","title":"Traversal","text":"<p>To traverse a graph you can directly access the vertices or edges sets and use normal Groovy/Java mechanisms. In the following example we are looking in all vertices to find all vertices having the number two as a neighbor</p> collections (vertices)<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..10).each(delegate::vertex)\n    edges(\n        1, 3,\n        1, 5,\n        1, 7,\n        1, 9,\n        1, 2  // &lt;--- looking here\n    )\n}\n\ndef answer1 = graph.vertices.findAll {\n    graph.neighborsOf(it).any { it == 2 }\n}\n</code></pre> output<pre><code>1\n</code></pre> <p>Now in the next example we are exploring boss-employee relationships and we'd like to find all the bosses names:</p> collections (edges)<pre><code>def anna = new Person(\"Anna\", 24)\ndef chris = new Person(\"Chris\", 26)\ndef paul = new Person(\"Paul\", 30)\ndef john = new Person(\"John\", 28)\n\ndef employees = Underdog.graphs().graph(Person) {\n    // adding people\n    [anna, chris, paul, john].each(delegate::vertex)\n    edge(chris, paul, \"boss\")\n    edge(anna, john, \"boss\")\n    edge(chris, anna, \"mentor\")\n}\n\ndef bossesNames = employees.edges                        // search in all edges where...\n    .findAll { it.relation == 'boss' }            // boss-employee relationship\n    .collect { employees.verticesOf(it)[0].name } // get only the boss name\n</code></pre> <p>Sometimes the graph could be bigger and more complex and we could benefit from using breadthFirst (*) or depthFirst (**) algorithms:</p> depthFirst<pre><code>def anna = new Person(\"Anna\", 24)\ndef chris = new Person(\"Chris\", 26)\ndef paul = new Person(\"Paul\", 30)\ndef john = new Person(\"John\", 28)\n\ndef dates = Underdog.graphs().graph(Person){\n    // adding people\n    [anna, chris, paul, john].each(delegate::vertex)\n\n    // adding dates\n    edges(\n        anna, paul,\n        anna, chris,\n        chris, paul,\n        anna, john\n    )\n}\n\ndef answer = dates.'**'.find { Person person -&gt;\n    // less than 30 years\n    person.age &lt; 30 &amp;&amp;\n    // have dated John\n    dates.neighborsOf(person).any { p -&gt; p.name == \"John\"}\n}\n</code></pre>"},{"location":"graphs/tutorial/","title":"Tutorial","text":""},{"location":"graphs/tutorial/#tutorial","title":"Tutorial","text":""},{"location":"graphs/tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"graphs/tutorial/#dependencies","title":"Dependencies","text":"<p>The modules required to follow this tutorial are the <code>graphs</code> and <code>plots</code> modules:</p> gradle<pre><code>implementation 'com.github.grooviter:underdog-graphs:VERSION'\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <p>or if you're using Maven:</p> maven<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-graphs&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"graphs/tutorial/#creating-a-graph","title":"Creating a graph","text":"<p>Creating an empty graph without vertices and edges:</p> create a graph<pre><code>import underdog.Underdog\n\ndef graph = Underdog.graphs().graph(String) {\n    // vertices and edges here\n}\n</code></pre> <p>The <code>graph(Class)</code> informs what is the type of the vertices this graph is going to have. This time vertices in this graph will be of type <code>String</code> but vertices could be potentially of any type.</p>"},{"location":"graphs/tutorial/#vertices-nodes","title":"Vertices / Nodes","text":"<p>Lets create a graph with two nodes without any edge between them:</p> <p> </p> <p>There are a couple of ways of adding more vertices to a graph. One is when creating the graph:</p> adding vertices at creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    vertex(\"A\")\n    vertex(\"B\")\n}\n</code></pre> <p>You can also add more vertices after the graph has been created:</p> adding vertices after creation<pre><code>def graph = Underdog.graphs().graph(String) {}\n\ngraph.addVertex(\"A\")\ngraph.addVertex(\"B\")\n</code></pre> <p>You can add simple type vertices, but you can also add more complex objects. Imagine we can add the relationships between employees in a given company. First lets define the <code>Employee</code> class:</p> Employee<pre><code>/*\n * @Canonical implements equals and hashcode among other things.\n * These methods will help the graph to id each node in the graph.\n*/\n@groovy.transform.Canonical\nstatic class Employee {\n    String name, department\n}\n</code></pre> <p>Now we can create a Graph and add relationships between employees:</p> Adding employees<pre><code>def john = new Employee(\"John\", \"Engineering\")\ndef peter = new Employee(\"Peter\", \"Engineering\")\ndef lisa = new Employee(\"Lisa\", \"Engineering\")\n\ndef graph = Underdog.graphs().graph(Employee) {\n    vertex(john)\n    vertex(peter)\n    vertex(lisa)\n}\n</code></pre>"},{"location":"graphs/tutorial/#edges","title":"Edges","text":"<p>Graphs normally are not very useful without setting edges between nodes. Lets add an edge between two nodes:</p> <p> </p> <p>We can add vertices at creation time:</p> Adding edges at creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices first\n    vertex('A')\n    vertex('B')\n\n    // then adding edges between vertices\n    edge('A', 'B')\n}\n</code></pre> <p>But it is also possible to add edges after the graph has been created:</p> Adding edges after creation time<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices 'A', 'B', 'C', 'D'\n    ('A'..'D').each(delegate::vertex)\n}\n\n// adding edge between 'A' and 'B'\ngraph.addEdge('A', 'B')\n</code></pre> <p>You can also add several edges at once using <code>edges(...)</code></p> <p> </p> adding several edges<pre><code>def graph = Underdog.graphs().graph(String) {\n    // adding vertices first\n    ('A'..'D').each(delegate::vertex)\n\n    // then adding several edges\n    edges(\n        'A', 'B',\n        'B', 'C',\n        'C', 'D'\n    )\n}\n</code></pre> <p>We can at any point ask about how many vertices and edges there are in the graph:</p> shape<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('A'..'D').each(delegate::vertex)\n    edge('A', 'B')\n    edge('B', 'C')\n    edge('C', 'D')\n}\n\n// getting vertices and edges count\ndef (nVertices, nEdges) = graph.shape()\n\n// or just println shape\nprintln(graph.shape())\n</code></pre> <p>which prints:</p> output<pre><code>4 vertices X 3 edges\n</code></pre>"},{"location":"graphs/tutorial/#elements-of-a-graph","title":"Elements of a graph","text":"<ul> <li>vertices</li> <li>edges</li> <li>adjacent</li> <li>degree</li> </ul> <p>Lets create a graph first and then ask for its elements:</p> graph<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..10).each(delegate::vertex)\n    edges(\n        1, 2,\n        1, 3,\n        3, 4\n    )\n}\n</code></pre> <p>The lets ask for its vertices, edges, neighbors.</p> graph elements<pre><code>//  getting graph vertices\ngraph.vertices.containsAll(1..10)\n\n// getting graph edges\ngraph.edges.size() == 3\n\n// getting neighbors of a specific vertex\ngraph.neighborsOf(1) == [2, 3]\n</code></pre>"},{"location":"graphs/tutorial/#removing-elements","title":"Removing elements","text":"<p>Here there are some example on how to remove vertices and edges from a given graph. When using the operator minus (<code>-</code>) the result return the graph minus the element removed whereas the <code>removeXXX(,,,)</code> functions only return a boolean value: true if the element was successfully removed, or false if it couldn't be removed from graph.</p> removing elements<pre><code>def graph = Underdog.graphs().graph(Integer) {\n    (1..14).each(delegate::vertex)\n    edges(\n        3, 5,\n        5, 7,\n        7, 9,\n        9, 11,\n        11, 12,\n        12, 14\n    )\n}\n\n// removing vertices using - operator\ndef graph2 = graph - [2, 4, 6, 8, 10]\ndef graph3 = graph2 - 1\n\n// removing vertices using function\ngraph3.removeVertex(3)\ngraph3.removeAllVertices([5, 7])\n\n// removing edge using - operator\ndef graph4 = graph3 - graph3.edgesOf(9).first()\n\n// removing edges\ngraph4.removeEdge(graph4.edgesOf(11).first())\n// graph4.removeAllEdges(graph4.edgesOf(12)) &lt;--- this fails: ConcurrentModificationException\ngraph4.removeAllEdges(graph4.edgesOf(12).toList()) // &lt;--- this works\n</code></pre>"},{"location":"graphs/tutorial/#graph-types","title":"Graph types","text":"<p>You can create different types of graphs. There are a couple of methods you can use:</p> graph types<pre><code>// graphs\ndef g = Underdog.graphs()\n\n// undirected weighted\ndef graph1 = g.graph(String)\n\n// directed weighted\ndef graph2 = g.digraph(String)\n\n// directed weighted pseudo graph\ndef graph3 = g.multidigraph(String)\n\n// weighted pseudo graph\ndef graph4 = g.multigraph(String)\n</code></pre> <p>Tip</p> <p>Because graphs in underdog are using JGraphT underneath you can always create an instance of any type of graph  directly using JGraphT api.</p>"},{"location":"graphs/tutorial/#what-to-use-as-vertices","title":"What to use as vertices","text":"<p>You can use almost anything as a vertex. The only mandatory condition is that the graph must be able to distinguish between vertices. For that your vertex should implement both equals and hashcode methods. In Groovy you can use the <code>@Canonical</code> annotation to get that.</p>"},{"location":"graphs/tutorial/#analyzing-graphs","title":"Analyzing graphs","text":"<p>The structure of the graph can be analyzed by using various functions.</p> graph<pre><code>def graph = Underdog.graphs().graph(String) {\n    ('a'..'f').each(delegate::vertex)\n\n    edges(\n        'a', 'b',\n        'a', 'c',\n        'a', 'd',\n        'b', 'e',\n    )\n}\n</code></pre> <p>What is the shape of the graph ?</p> shape<pre><code>def (nVertices, nEdges) = graph.shape()\n</code></pre> <p>What is the clustering of the graph ?</p> clustering of the graph<pre><code>def graphClustering = graph.clusteringGlobal()\n</code></pre> <p>What is the clustering avg ?</p> clustering average<pre><code>def graphAvg = graph.clusteringAvg()\n</code></pre> <p>And what about the clustering of a given vertex ?</p> clustering of a vertex<pre><code>def vertexClustering = graph.clusteringOf('a')\n</code></pre> <p>What is the vertex with max degree ?</p> max degree<pre><code>String maxDegreeVertex = graph.maxDegree()\n</code></pre> <p>Lets say we want to sort vertices by degree in descending order:</p> sort vertices by degree (desc)<pre><code>def sortedVertices = graph.vertices.sort { -graph.degreeOf(it) }\n</code></pre>"},{"location":"ml/","title":"ML","text":"<p>Underdog's ML module can be used to explore machine learning problems. It's built on top of  the SMILE machine learning library adding a Groovy DSL and some extension methods to make certain operations  easier.</p>"},{"location":"ml/#tutorial","title":"Tutorial","text":"<p>Info</p> <p>The current tutorial follows the Tablesaw Moneyball tutorial but using Underdog's dataframe and ml modules.</p>"},{"location":"ml/#prerequisites","title":"Prerequisites","text":""},{"location":"ml/#dependencies","title":"Dependencies","text":"<p>The modules required to follow this tutorial are the <code>ml</code> and <code>plots</code> modules:</p> gradle<pre><code>implementation 'com.github.grooviter:underdog-ml:VERSION'\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <p>or if you're using Maven:</p> maven<pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n        &lt;artifactId&gt;underdog-ml&lt;/artifactId&gt;\n        &lt;version&gt;VERSION&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n        &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n        &lt;version&gt;VERSION&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"ml/#data","title":"Data","text":"<p>TODO</p>"},{"location":"ml/#introduction","title":"Introduction","text":"<p>In baseball, you make the playoffs by winning more games than your rivals, but you can\u2019t control the number of games your rivals win. How should you proceed? The A\u2019s needed to find controllable variables that affected their likelihood of making the playoffs.</p> <p>Specifically, they wanted to know how to spend their salary dollars to produce the most wins. Statistics like \"Batting Average\" are available for individual players so if you knew Batting Average had the greatest impact, you can trade for players with high batting averages, and thus improve your odds of success.</p> <p>To connect player stats to making the playoffs, they systematically decomposed their high-level goal. They started by asking how many wins they\u2019d need to make the playoffs. They decided that 95 wins would give them a strong chance. Here\u2019s how we might check that assumption using Underdog.</p> <p>The tutorial also tries to follow the iterative process:</p> <pre><code>flowchart LR\n    subgraph representation\n    a[Data Analysis]--&gt;b[Algorithm selection]\n    end\n    subgraph evaluation\n    b--&gt;c[Model Training]\n    c--&gt;d[Model Testing]\n    d-- ITERATION --&gt;a\n    end\n</code></pre>"},{"location":"ml/#analyzing-data","title":"Analyzing data","text":"<p>To connect player stats to making the playoffs, they systematically decomposed their high-level goal. They started by asking how many wins they\u2019d need to make the playoffs. They decided that 95 wins would give them a strong chance. Here\u2019s how we might check that assumption using Underdog. First lets load the data:</p> loading data<pre><code>def data = Underdog.df().read_csv(\"src/test/resources/data/baseball.csv\")\n</code></pre> <p>Lets take only data before 2002:</p> filtering<pre><code>data = data[data[\"year\"] &lt; 2002]\n</code></pre> <p>We can check the assumption visually by plotting wins per year in a way that separates the teams who make the playoffs from those who don\u2019t. This code produces the chart below:</p> playoffs<pre><code>def figure = Underdog\n    .plots()\n    .scatter(\n        data['W'],\n        data['year'],\n        group: data['playoffs'],\n        title: 'Regular seasons wins by year')\n\nfigure.show()\n</code></pre> <p>The Series <code>data['playoffs']</code> represents whether the team made it to the playoffs (1) or it didn't (0).</p>"},{"location":"ml/#preparing-data","title":"Preparing data","text":"<p>Unfortunately, you can\u2019t directly control the number of games you win. We need to go deeper. At the next level, we hypothesize that the number of wins can be predicted by the number of Runs Scored during the season, combined with the number of Runs Allowed.</p> <p>To check this assumption we compute Run Difference (RD) as Runs Scored (RS) - Runs Allowed (RA)</p> difference<pre><code>data['RD'] = data['RS'] - data['RA']\n</code></pre> <p>Now lets see if Run Difference is correlated with Wins. We use a scatter plot again:</p> correlation<pre><code>def figure = Underdog\n    .plots()\n    .scatter(\n        data['RD'],\n        data['W'],\n        title: 'Run difference vs Wins')\n\nfigure.show()\n</code></pre>"},{"location":"ml/#model-training","title":"Model training","text":"<p>Let\u2019s create our first predictive model using linear regression, with runDifference as the sole explanatory variable.  Here we use Ordinary Least Squares (OLS) regression model.</p> <p>Tip</p> <p>To know more about Ordinary Least Squares you can check out its definition in Wikipedia</p> Ordinary least square (OLS)<pre><code>// extracting features (X) and labels (y)\ndef X = data['RD'] as double[][]\ndef y = data['W'] as double[]\n\n// splitting between train and test datasets to avoid over fitting\ndef (xTrain, xTest, yTrain, yTest) = Underdog.ml().utils.trainTestSplit(X, y)\n\n// training the model\ndef winsModel = Underdog.ml().regression.ols(xTrain, yTrain)\n</code></pre> <p>If we print our \u201cwinsModel\u201d, it produces the output below:</p> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-14.4535     -2.5195      0.2255      2.9513     11.5651\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept          80.9257     0.1838   440.3014     0.0000 ***\nX0                  0.1038     0.0019    54.6362     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.9032 on 449 degrees of freedom\nMultiple R-squared: 0.8693,    Adjusted R-squared: 0.8690\nF-statistic: 2985.1152 on 2 and 449 DF,  p-value: 1.757e-200\n</code></pre> <p>If you\u2019re new to regression, here are some take-aways from the output:</p> <ul> <li>The R-squared of .88 can be interpreted to mean that roughly 88% of the variance in Wins can be explained by the Run Difference variable. The rest is determined by some combination of other variables and pure chance.</li> <li>The estimate for the Intercept is the average wins independent of Run Difference. In baseball, we have a 162 game season so we expect this value to be about 81, as it is.</li> <li>The estimate for the RD variable of .1, suggests that an increase of 10 in Run Difference, should produce about 1 additional win over the course of the season.</li> </ul> <p>Of course, this model is not simply descriptive. We can use it to make predictions. In the code below, we predict how many games we will win if we score 135 more runs than our opponents.  To do this, we pass an array of doubles, one for each explanatory variable in our model, to the predict() method. In this case, there\u2019s just one variable: run difference.</p> using prediction<pre><code>def prediction = winsModel.predict([135] as double[])\n</code></pre> output<pre><code>94.93591869149651\n</code></pre> <p>We\u2019d expect almost 95 wins when we outscore opponents by 135 runs.</p>"},{"location":"ml/#scoring-with-test-data","title":"Scoring with test data","text":"<p>If we want to check how the model is performing overall we can pick the testing datasets we kept aside from the model training and use them to get a measure on how well the model is predicting a new case.</p> <p>In this example we are using the R2 square metric.  In regression, the R2 score is a statistical measure of how well the regression predictions approximate the real data points.</p> <p>We're passing to the model the testing features (xTest) and comparing the model predictions with the test labels we have (yTest):</p> r2score<pre><code>// generating predictions for the test features\ndef predictions = winsModel.predict(xTest)\n\n// comparing predictions with the actual truth for those features\ndef r2score = Underdog.ml().metrics.r2Score(yTest, predictions)\n</code></pre> <p>Getting a result of:</p> output<pre><code>0.8895307770897616\n</code></pre>"},{"location":"ml/#modeling-runs-scored","title":"Modeling Runs Scored","text":"<p>It\u2019s time to go deeper again and see how we can model Runs Scored and Runs Allowed. The approach the A\u2019s took was to model Runs Scored using team On-base percent (OBP) and team Slugging Average (SLG). In Underdog, we write:</p> <pre><code>def X = data['OBP', 'SLG'] as double[][]\ndef y = data['RS'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef runsScored = ml.regression.ols(xTrain, yTrain)\n</code></pre> <p>Once again the first parameter takes a Underdog column containing the values we want to predict (Runs scored). The next two parameters take the explanatory variables OBP and SLG.</p> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-67.7289    -18.0586     -1.5988     16.8863     68.9436\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept        -846.1069    29.6301   -28.5556     0.0000 ***\nX0               2937.5751   141.7312    20.7264     0.0000 ***\nX1               1524.8355    65.4379    23.3020     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.4627 on 448 degrees of freedom\nMultiple R-squared: 0.9204,    Adjusted R-squared: 0.9201\nF-statistic: 2590.2693 on 3 and 448 DF,  p-value: 6.276e-247\n</code></pre> <p>Again we have a model with excellent explanatory power with an R-squared of 92. Now we\u2019ll check the model visually to see if it violates any assumptions. Our residuals should be normally distributed. We can use a histogram to verify:</p> histogram<pre><code>def histogram = Underdog\n    .plots()\n    .histogram(residuals.toList(), title: 'Runs Scored from OBP and SLG')\n\nhistogram.show()\n</code></pre> <p>It looks great.  It\u2019s also important to plot the predicted (or \u201cfitted\u201d) values against the residuals. We want to see if the model fits some values better than others, which will influence whether we can trust its predictions or not. Ideally, we want to see a cloud of random dots around zero on the y axis.</p> <p>Our Scatter class can create this plot directly from the model:</p> fitted vs residuals<pre><code>def modelResiduals = runsScored.residuals().toList()\ndef modelFitted = runsScored.fittedValues().toList()\n\ndef fittedVsResiduals = Underdog\n    .plots()\n    .scatter(modelFitted, modelResiduals,\n        title: \"Runs Scored from OBP and SLG\",\n        xLabel: \"Fitted\",\n        yLabel: \"Residuals\")\n\nfittedVsResiduals.show()\n</code></pre> <p>Again, the plot looks good.</p> <p>Let\u2019s review.  We\u2019ve created a model of baseball that predicts entry into the playoffs based on batting stats, with the influence of the variables as:</p> <pre><code>graph LR\n  A[SLG &amp; OBP] --&gt; B[Runs Scored];\n  B --&gt; C[Run Difference];\n  C --&gt; D[Regular Season Wins];</code></pre>"},{"location":"ml/#modeling-runs-allowed","title":"Modeling Runs Allowed","text":"<p>Of course, we haven\u2019t modeled the Runs Allowed side of Run Difference. We could use pitching and field stats to do this, but the A\u2019s cleverly used the same two variables (SLG and OBP), but now looked at how their opponent\u2019s performed against the A\u2019s. We could do the same as these data are encoded in the dataset as OOBP and OSLG.</p> SLG &amp; OBP<pre><code>def X = data['OOBP', 'OSLG'].dropna() as double[][]\ndef y = data['RA'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef runsAllowed = ml.regression.ols(xTrain, yTrain)\n</code></pre> print<pre><code>println(runsAllowed)\n</code></pre> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-82.1479     -8.9954      0.7291     15.7773     46.4004\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept        -822.7172    97.7824    -8.4138     0.0000 ***\nX0               2844.9158   524.5965     5.4231     0.0000 ***\nX1               1532.0857   286.8341     5.3414     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27.7020 on 42 degrees of freedom\nMultiple R-squared: 0.8985,    Adjusted R-squared: 0.8936\nF-statistic: 185.8286 on 3 and 42 DF,  p-value: 1.377e-21\n</code></pre> <p>This model also looks good, but you\u2019d want to look at the plots again, and do other checking as well. Checking the predictive variables for collinearity is always good.</p> <p>Finally, we can tie this all together and see how well wins is predicted when we consider both offensive and defensive stats.</p> regression<pre><code>def X = data[\"OOBP\", \"OBP\", \"OSLG\", \"SLG\"].dropna() as double[][]\ndef y = data['W'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef winsFinal = ml.regression.ols(xTrain, yTrain)\n</code></pre>"},{"location":"ml/#the-as-in-2001","title":"The A's in 2001","text":"<p>For fun, I decided to see what the model predicts for the 2001 A\u2019s. First, I got the independent variables for the A\u2019s in that year.</p> A's in 2001<pre><code>def asIn2001 = data[\n    data['team'] == 'OAK' &amp;\n    data['year'] == 2001].loc[__, [\"year\", \"OOBP\", \"OBP\", \"OSLG\", \"SLG\"]]\n</code></pre> output<pre><code>                 baseball.csv\nYear  |  OOBP   |   OBP   |  OSLG  |   SLG   |\n-----------------------------------------------\n2001  |  0.308  |  0.345  |  0.38  |  0.439  |\n</code></pre> <p>Now we get the prediction:</p> A's in 2001<pre><code>double[][] values = asIn2001.loc[__, [\"OOBP\", \"OBP\", \"OSLG\", \"SLG\"]] as double[][]\ndouble[] value = winsFinal.predict(values);\n</code></pre> output<pre><code>102.22837899017894\n</code></pre> <p>The model predicted that the 2001 A\u2019s would win 102 games given their slugging and On-Base stats. They won 103.</p>"},{"location":"ml/#extensions","title":"Extensions","text":"<p>TODO</p>"},{"location":"ml/extensions/","title":"Extensions","text":""},{"location":"ml/extensions/#extensions","title":"Extensions","text":"<p>TODO</p>"},{"location":"ml/tutorial/","title":"Tutorial","text":""},{"location":"ml/tutorial/#tutorial","title":"Tutorial","text":"<p>Info</p> <p>The current tutorial follows the Tablesaw Moneyball tutorial but using Underdog's dataframe and ml modules.</p>"},{"location":"ml/tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"ml/tutorial/#dependencies","title":"Dependencies","text":"<p>The modules required to follow this tutorial are the <code>ml</code> and <code>plots</code> modules:</p> gradle<pre><code>implementation 'com.github.grooviter:underdog-ml:VERSION'\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <p>or if you're using Maven:</p> maven<pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n        &lt;artifactId&gt;underdog-ml&lt;/artifactId&gt;\n        &lt;version&gt;VERSION&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n        &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n        &lt;version&gt;VERSION&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"ml/tutorial/#data","title":"Data","text":"<p>TODO</p>"},{"location":"ml/tutorial/#introduction","title":"Introduction","text":"<p>In baseball, you make the playoffs by winning more games than your rivals, but you can\u2019t control the number of games your rivals win. How should you proceed? The A\u2019s needed to find controllable variables that affected their likelihood of making the playoffs.</p> <p>Specifically, they wanted to know how to spend their salary dollars to produce the most wins. Statistics like \"Batting Average\" are available for individual players so if you knew Batting Average had the greatest impact, you can trade for players with high batting averages, and thus improve your odds of success.</p> <p>To connect player stats to making the playoffs, they systematically decomposed their high-level goal. They started by asking how many wins they\u2019d need to make the playoffs. They decided that 95 wins would give them a strong chance. Here\u2019s how we might check that assumption using Underdog.</p> <p>The tutorial also tries to follow the iterative process:</p> <pre><code>flowchart LR\n    subgraph representation\n    a[Data Analysis]--&gt;b[Algorithm selection]\n    end\n    subgraph evaluation\n    b--&gt;c[Model Training]\n    c--&gt;d[Model Testing]\n    d-- ITERATION --&gt;a\n    end\n</code></pre>"},{"location":"ml/tutorial/#analyzing-data","title":"Analyzing data","text":"<p>To connect player stats to making the playoffs, they systematically decomposed their high-level goal. They started by asking how many wins they\u2019d need to make the playoffs. They decided that 95 wins would give them a strong chance. Here\u2019s how we might check that assumption using Underdog. First lets load the data:</p> loading data<pre><code>def data = Underdog.df().read_csv(\"src/test/resources/data/baseball.csv\")\n</code></pre> <p>Lets take only data before 2002:</p> filtering<pre><code>data = data[data[\"year\"] &lt; 2002]\n</code></pre> <p>We can check the assumption visually by plotting wins per year in a way that separates the teams who make the playoffs from those who don\u2019t. This code produces the chart below:</p> playoffs<pre><code>def figure = Underdog\n    .plots()\n    .scatter(\n        data['W'],\n        data['year'],\n        group: data['playoffs'],\n        title: 'Regular seasons wins by year')\n\nfigure.show()\n</code></pre> <p>The Series <code>data['playoffs']</code> represents whether the team made it to the playoffs (1) or it didn't (0).</p>"},{"location":"ml/tutorial/#preparing-data","title":"Preparing data","text":"<p>Unfortunately, you can\u2019t directly control the number of games you win. We need to go deeper. At the next level, we hypothesize that the number of wins can be predicted by the number of Runs Scored during the season, combined with the number of Runs Allowed.</p> <p>To check this assumption we compute Run Difference (RD) as Runs Scored (RS) - Runs Allowed (RA)</p> difference<pre><code>data['RD'] = data['RS'] - data['RA']\n</code></pre> <p>Now lets see if Run Difference is correlated with Wins. We use a scatter plot again:</p> correlation<pre><code>def figure = Underdog\n    .plots()\n    .scatter(\n        data['RD'],\n        data['W'],\n        title: 'Run difference vs Wins')\n\nfigure.show()\n</code></pre>"},{"location":"ml/tutorial/#model-training","title":"Model training","text":"<p>Let\u2019s create our first predictive model using linear regression, with runDifference as the sole explanatory variable.  Here we use Ordinary Least Squares (OLS) regression model.</p> <p>Tip</p> <p>To know more about Ordinary Least Squares you can check out its definition in Wikipedia</p> Ordinary least square (OLS)<pre><code>// extracting features (X) and labels (y)\ndef X = data['RD'] as double[][]\ndef y = data['W'] as double[]\n\n// splitting between train and test datasets to avoid over fitting\ndef (xTrain, xTest, yTrain, yTest) = Underdog.ml().utils.trainTestSplit(X, y)\n\n// training the model\ndef winsModel = Underdog.ml().regression.ols(xTrain, yTrain)\n</code></pre> <p>If we print our \u201cwinsModel\u201d, it produces the output below:</p> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-14.4535     -2.5195      0.2255      2.9513     11.5651\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept          80.9257     0.1838   440.3014     0.0000 ***\nX0                  0.1038     0.0019    54.6362     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.9032 on 449 degrees of freedom\nMultiple R-squared: 0.8693,    Adjusted R-squared: 0.8690\nF-statistic: 2985.1152 on 2 and 449 DF,  p-value: 1.757e-200\n</code></pre> <p>If you\u2019re new to regression, here are some take-aways from the output:</p> <ul> <li>The R-squared of .88 can be interpreted to mean that roughly 88% of the variance in Wins can be explained by the Run Difference variable. The rest is determined by some combination of other variables and pure chance.</li> <li>The estimate for the Intercept is the average wins independent of Run Difference. In baseball, we have a 162 game season so we expect this value to be about 81, as it is.</li> <li>The estimate for the RD variable of .1, suggests that an increase of 10 in Run Difference, should produce about 1 additional win over the course of the season.</li> </ul> <p>Of course, this model is not simply descriptive. We can use it to make predictions. In the code below, we predict how many games we will win if we score 135 more runs than our opponents.  To do this, we pass an array of doubles, one for each explanatory variable in our model, to the predict() method. In this case, there\u2019s just one variable: run difference.</p> using prediction<pre><code>def prediction = winsModel.predict([135] as double[])\n</code></pre> output<pre><code>94.93591869149651\n</code></pre> <p>We\u2019d expect almost 95 wins when we outscore opponents by 135 runs.</p>"},{"location":"ml/tutorial/#scoring-with-test-data","title":"Scoring with test data","text":"<p>If we want to check how the model is performing overall we can pick the testing datasets we kept aside from the model training and use them to get a measure on how well the model is predicting a new case.</p> <p>In this example we are using the R2 square metric.  In regression, the R2 score is a statistical measure of how well the regression predictions approximate the real data points.</p> <p>We're passing to the model the testing features (xTest) and comparing the model predictions with the test labels we have (yTest):</p> r2score<pre><code>// generating predictions for the test features\ndef predictions = winsModel.predict(xTest)\n\n// comparing predictions with the actual truth for those features\ndef r2score = Underdog.ml().metrics.r2Score(yTest, predictions)\n</code></pre> <p>Getting a result of:</p> output<pre><code>0.8895307770897616\n</code></pre>"},{"location":"ml/tutorial/#modeling-runs-scored","title":"Modeling Runs Scored","text":"<p>It\u2019s time to go deeper again and see how we can model Runs Scored and Runs Allowed. The approach the A\u2019s took was to model Runs Scored using team On-base percent (OBP) and team Slugging Average (SLG). In Underdog, we write:</p> <pre><code>def X = data['OBP', 'SLG'] as double[][]\ndef y = data['RS'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef runsScored = ml.regression.ols(xTrain, yTrain)\n</code></pre> <p>Once again the first parameter takes a Underdog column containing the values we want to predict (Runs scored). The next two parameters take the explanatory variables OBP and SLG.</p> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-67.7289    -18.0586     -1.5988     16.8863     68.9436\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept        -846.1069    29.6301   -28.5556     0.0000 ***\nX0               2937.5751   141.7312    20.7264     0.0000 ***\nX1               1524.8355    65.4379    23.3020     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.4627 on 448 degrees of freedom\nMultiple R-squared: 0.9204,    Adjusted R-squared: 0.9201\nF-statistic: 2590.2693 on 3 and 448 DF,  p-value: 6.276e-247\n</code></pre> <p>Again we have a model with excellent explanatory power with an R-squared of 92. Now we\u2019ll check the model visually to see if it violates any assumptions. Our residuals should be normally distributed. We can use a histogram to verify:</p> histogram<pre><code>def histogram = Underdog\n    .plots()\n    .histogram(residuals.toList(), title: 'Runs Scored from OBP and SLG')\n\nhistogram.show()\n</code></pre> <p>It looks great.  It\u2019s also important to plot the predicted (or \u201cfitted\u201d) values against the residuals. We want to see if the model fits some values better than others, which will influence whether we can trust its predictions or not. Ideally, we want to see a cloud of random dots around zero on the y axis.</p> <p>Our Scatter class can create this plot directly from the model:</p> fitted vs residuals<pre><code>def modelResiduals = runsScored.residuals().toList()\ndef modelFitted = runsScored.fittedValues().toList()\n\ndef fittedVsResiduals = Underdog\n    .plots()\n    .scatter(modelFitted, modelResiduals,\n        title: \"Runs Scored from OBP and SLG\",\n        xLabel: \"Fitted\",\n        yLabel: \"Residuals\")\n\nfittedVsResiduals.show()\n</code></pre> <p>Again, the plot looks good.</p> <p>Let\u2019s review.  We\u2019ve created a model of baseball that predicts entry into the playoffs based on batting stats, with the influence of the variables as:</p> <pre><code>graph LR\n  A[SLG &amp; OBP] --&gt; B[Runs Scored];\n  B --&gt; C[Run Difference];\n  C --&gt; D[Regular Season Wins];</code></pre>"},{"location":"ml/tutorial/#modeling-runs-allowed","title":"Modeling Runs Allowed","text":"<p>Of course, we haven\u2019t modeled the Runs Allowed side of Run Difference. We could use pitching and field stats to do this, but the A\u2019s cleverly used the same two variables (SLG and OBP), but now looked at how their opponent\u2019s performed against the A\u2019s. We could do the same as these data are encoded in the dataset as OOBP and OSLG.</p> SLG &amp; OBP<pre><code>def X = data['OOBP', 'OSLG'].dropna() as double[][]\ndef y = data['RA'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef runsAllowed = ml.regression.ols(xTrain, yTrain)\n</code></pre> print<pre><code>println(runsAllowed)\n</code></pre> output<pre><code>Residuals:\nMin          1Q      Median          3Q         Max\n-82.1479     -8.9954      0.7291     15.7773     46.4004\n\nCoefficients:\nEstimate Std. Error    t value   Pr(&gt;|t|)\nIntercept        -822.7172    97.7824    -8.4138     0.0000 ***\nX0               2844.9158   524.5965     5.4231     0.0000 ***\nX1               1532.0857   286.8341     5.3414     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27.7020 on 42 degrees of freedom\nMultiple R-squared: 0.8985,    Adjusted R-squared: 0.8936\nF-statistic: 185.8286 on 3 and 42 DF,  p-value: 1.377e-21\n</code></pre> <p>This model also looks good, but you\u2019d want to look at the plots again, and do other checking as well. Checking the predictive variables for collinearity is always good.</p> <p>Finally, we can tie this all together and see how well wins is predicted when we consider both offensive and defensive stats.</p> regression<pre><code>def X = data[\"OOBP\", \"OBP\", \"OSLG\", \"SLG\"].dropna() as double[][]\ndef y = data['W'] as double[]\n\ndef ml = Underdog.ml()\ndef (xTrain, xTest, yTrain, yTest) = ml.utils.trainTestSplit(X, y)\ndef winsFinal = ml.regression.ols(xTrain, yTrain)\n</code></pre>"},{"location":"ml/tutorial/#the-as-in-2001","title":"The A's in 2001","text":"<p>For fun, I decided to see what the model predicts for the 2001 A\u2019s. First, I got the independent variables for the A\u2019s in that year.</p> A's in 2001<pre><code>def asIn2001 = data[\n    data['team'] == 'OAK' &amp;\n    data['year'] == 2001].loc[__, [\"year\", \"OOBP\", \"OBP\", \"OSLG\", \"SLG\"]]\n</code></pre> output<pre><code>                 baseball.csv\nYear  |  OOBP   |   OBP   |  OSLG  |   SLG   |\n-----------------------------------------------\n2001  |  0.308  |  0.345  |  0.38  |  0.439  |\n</code></pre> <p>Now we get the prediction:</p> A's in 2001<pre><code>double[][] values = asIn2001.loc[__, [\"OOBP\", \"OBP\", \"OSLG\", \"SLG\"]] as double[][]\ndouble[] value = winsFinal.predict(values);\n</code></pre> output<pre><code>102.22837899017894\n</code></pre> <p>The model predicted that the 2001 A\u2019s would win 102 games given their slugging and On-Base stats. They won 103.</p>"},{"location":"plots/","title":"Plots","text":"<p>Underdog's Plots module renders different types of charts and adds integration with rest of Underdog's modules.</p>"},{"location":"plots/#introduction","title":"Introduction","text":""},{"location":"plots/#echarts-dsl","title":"Echarts DSL","text":"<p>Underdog plots uses the Apache Echarts under the hood, so the idea is to try to be able to render whatever is possible in Echarts. To accomplish that this project creates a Groovy DSL mimicking the Echarts Option object. You can access the DSL when customizing the chart.</p> <p>At the moment the support of the Echarts Option object is limited but we aim to improve that overtime.</p>"},{"location":"plots/#basic-properties","title":"Basic properties","text":"<p>For every chart we must provide methods containing the following properties:</p> <ul> <li>data entry as list of numbers</li> <li>data entry as Series</li> <li>chart title</li> <li>chart subtitle</li> </ul> <p>Apart from that every plot has a <code>customize(Closure)</code> method to be able to customize the chart following Echarts documentation using a Groovy DSL.</p> <p>When the method is receiving data as List instances:</p> <ul> <li>X coordinate label (by default is X)</li> <li>Y coordinate label (by default is Y)</li> </ul> <p>In methods receiving Series objects the name of the X and Y coordinate will be taken from the Series' name passed as parameter.</p> <p>In general we'd like to be able to use the plots library for anyone, that's why any chart should be able to receive its data from a plain java list.</p> <p>Apart from those mandatory properties, there could be extra properties added to specific charts depending on how practical these properties are when dealing which that type of charts.</p>"},{"location":"plots/#return-options","title":"Return Options","text":"<p>All plotting methods return a memento.plots.charts.Options instance which represents the Echarts Options object.</p>"},{"location":"plots/#customizing-chart","title":"Customizing chart","text":"<p>All default methods provide a limited setup of the chart via the mandatory attributes we saw previously.To access the full Groovy Echarts DSL we can always access the Options#customize(Closure) method rendering the chart calling the Options#show().</p>"},{"location":"plots/#line","title":"Line","text":""},{"location":"plots/#simple","title":"Simple","text":"<p>Here is a simple line chart:</p> simple line<pre><code>def line = Plots.plots()\n    .line(\n        // You can use a **range or a list** for X axis\n        2000..2010,\n        // You can use a **range or a list** for the Y axis\n        [10, 15, 18, 3, 5, 9, 10, 11, 12, 10],\n        // Optional attributes\n        title: \"Wins of Team A\",\n        subtitle: \"Between years 2000 - 2010\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    )\n\nline.show()\n</code></pre> <p> </p> <p>There are methods adapted for using Underdog's Series objects:</p> Series<pre><code>// load data\ndef df = Underdog.df().read_csv(baseballPath)\n\n// filter &amp; aggregate &amp; sort\ndf = df[df['Team'] == 'BOS']\n    .agg(W: 'sum')\n    .by('year')\n    .sort_values(by: 'year')\n\n// show\ndef plot = Underdog.plots()\n    .line(\n        // using `year` series for X axis\n        df['year'],\n        // renaming series to `Wins X Years` and using it for Y axis\n        df['Sum [W]'].rename('Wins X Years'),\n        title: \"Wins of 'BOS' team over time\")\n\nplot.show()\n</code></pre> <p></p> <p>In any method using Underdog's Series there is no attribute for changing the xLabel or the yLabel as it takes the Series' name. To change the label you can rename the Series' name as mentioned in the example.</p>"},{"location":"plots/#n-lines","title":"N-lines","text":"lines using collections<pre><code>Map&lt;String, List&lt;Number&gt;&gt; data = [\n    // A list of lists of 2 elements [[x1, y1], [x2, y2],..., [xn, yn]]\n    A: [[2000, 13],[2001, 5], [2002, 7], [2003, 10], [2004,6]],\n    B: [[2000, 5], [2001, 6], [2002, 7], [2003, 8], [2004, 9]],\n    // Using [listX, listY]transpose()` == [[x1, y1], [x2, y2],..., [xn, yn]]\n    C: [2000..2004, 3..7].transpose()\n]\n\ndef plot = Underdog.plots()\n    .lines(\n        data,\n        title: \"Progress of Teams A, B, C\",\n        subtitle: \"Between years 2000 - 2010\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/#customize","title":"Customize","text":"<p>As in any chart once we've created our Options object and before calling <code>show()</code> we can use the <code>Options#customize()</code> method to customize the chart using Groovy's Echart DSL.</p> Customize<pre><code>def plot = Underdog.plots()\n    .lines(\n        dataFrame,\n        title: \"Team comparison (BOS, ATL, CIN)\",\n        subtitle: \"Years 2000-2004\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    ).customize {\n        // Adding legend in the top right corner\n        legend {\n            top(\"10%\")\n            right('15%')\n            show(true)\n        }\n        // Adding tooltip of type `axis`\n        tooltip {\n            trigger('axis')\n        }\n    }\n\nplot.show()\n</code></pre> <p>In this occasion we are adding the chart legend and positioning it to the top-right side of the chart.</p> <p> </p>"},{"location":"plots/#bar","title":"Bar","text":""},{"location":"plots/#simple_1","title":"Simple","text":"simple bar<pre><code>def plot = Underdog.plots()\n    .bar(\n        1..12,\n        [10, 12, 18, 3, 0, 20, 10, 12, 18, 3, 0, 10],\n        title: \"Using bars\",\n        xLabel: \"Months\",\n        yLabel: \"Indicator\"\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/#histogram","title":"Histogram","text":"simple histogram<pre><code>// generating data\ndef random = new Random()\ndef distribution = (0..1_000).collect { random.nextGaussian(0, 50) }\n\n// plot\ndef plot = Underdog\n    .plots()\n    .histogram(\n        distribution,\n        title: \"Distribution\",\n        subtitle: \"mean: 0 / stddev: 50\",\n        bins: 10\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/#scatter","title":"Scatter","text":""},{"location":"plots/#simple_2","title":"Simple","text":"simple<pre><code>// numbers from 0 to 99\n// You can use a \"range or a list\" for X axis\ndef xs = 0..&lt;100\n\n// 100 random numbers\n// You can use a \"range or a list\" for the Y axis\ndef ys = (0..&lt;100).collect { new Random().nextInt(100) }\n\n// plot\ndef plot = Underdog.plots()\n    .scatter(\n        xs,\n        ys,\n        title: \"Random Numbers\") // Optional attributes\n\nplot.show()\n</code></pre> <p>Here's the same example but using Underdog's series for X and Y axes. Given the dataframe instance <code>df</code> and series <code>xs</code> and <code>ys</code>:</p> simple series<pre><code>Plots.plots()\n        .scatter(\n            // using a series for x axis and renaming it to X\n            df['xs'].rename('X'),\n            // using another series for y axis and renaming it to Y\n            df['ys'].rename('Y'),\n            title: \"Random Numbers\")\n        .show()\n</code></pre>"},{"location":"plots/#graphs","title":"Graphs","text":""},{"location":"plots/#simple_3","title":"Simple","text":"<p>Here's a simple graph representation just showing 2 edges connecting 3 vertices. We only have to pass an instance of a graph to the <code>graph(...)</code> function:</p> <p> </p> simple graph<pre><code>// create instance of Graph\ndef friends = Graphs.graph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\n// show plot\ndef plot = Underdog.plots().graph(friends)\n\nplot.show()\n</code></pre>"},{"location":"plots/#directed-graph","title":"Directed graph","text":"<p>If the graph we're passing to the <code>plots().graph(...)</code> function is a directed graph, the direction of the edges will show up:</p> <p> </p> directed graph<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Underdog.plots().graph(friends)\nplot.show()\n</code></pre>"},{"location":"plots/#edge-labels","title":"Edge labels","text":"<p>If we want to show the edges labels we can do so by setting the <code>showEdgeLabel</code> parameter to <code>true</code>:</p> <p> </p> edge labels<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Plots.plots().graph(friends, showEdgeLabel: true)\nplot.show()\n</code></pre>"},{"location":"plots/#showing-paths","title":"Showing paths","text":"<p>Sometimes we may want highlight a given path between vertices. We can use the parameter <code>paths</code> which receives a list of paths to highlight any number of paths:</p> <p> </p> show paths<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef friendship = friends.shortestPath('Robert', 'Troy')\n\ndef plot = Underdog.plots().graph(\n    friends,\n    paths: [friendship],\n    showEdgeLabel: true)\n\nplot.show()\n</code></pre>"},{"location":"plots/#graph-domain","title":"Graph domain","text":"<p>To go one step further and play with vertices sizes and colors, we can use the graph chart domain classes. These classes are:</p> <ul> <li><code>underdog.plots.charts.Graph.Node</code>: represents a node graphically (size, color, label)</li> <li><code>underdog.plots.charts.Graph.Edge</code>: represents an edge graphically (width, color, label)</li> </ul> <p> </p> domain classes<pre><code>List&lt;Graph.Node&gt; nodes = [\n    new Graph.Node(id: \"robert\", name: \"Robert\", symbolSize: 75),\n    new Graph.Node(id: \"thelma\", name: \"Thelma\", symbolSize: 40),\n    new Graph.Node(id: \"troy\", name: \"Troy\", symbolSize: 40)\n]\n\nList&lt;Graph.Edge&gt; edges = [\n    new Graph.Edge(\n        source: \"robert\",\n        target: \"thelma\",\n        color: \"green\",\n        width: 2,\n        value: \"bff\"),\n    new Graph.Edge(\n        source: \"robert\",\n        target: \"troy\",\n        color: \"red\",\n        width: 10,\n        value: \"friend\")\n]\n\ndef plot = Underdog.plots().graph(\n    nodes,\n    edges,\n    showEdgeLabel: true)\n\nplot.show()\n</code></pre>"},{"location":"plots/#customize_1","title":"Customize","text":"<p>As any of the charts in Underdog, we can use the <code>customize(...)</code> method of any chart to customize the chart by using the Groovy Echarts DSL:</p> customization<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Underdog.plots()\n    .graph(friends, showEdgeLabel: true)\n    .customize {\n        title {\n            text \"New title\"\n            subtext(\"New subtitle\")\n            top(\"bottom\")\n            left(\"right\")\n        }\n    }\n\nplot.show()\n</code></pre>"},{"location":"plots/#pie","title":"Pie","text":"<p>Wikipedia</p> <p>According to Wikipedia a pie chart (or a circle chart) is a circular statistical graphic which is divided into slices to illustrate numerical proportion.</p> <p>In a pie chart, the arc length of each slice (and consequently its central angle and area) is proportional to the quantity it represents.</p>"},{"location":"plots/#simple_4","title":"Simple","text":"<p>To create a minimal representation of a Pie we must provide at least a collection of the labels of each partition, and another collection with the values of each partition:</p> building pie<pre><code>def plot = Underdog\n    .plots()\n    .pie(\n\n        ('A'..'D'), // slice labels\n        [9,5,6,4]   // slice values\n    )\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/#color-mapping","title":"Color mapping","text":"<p>In some situations the color of each partition is really meaningful. For example, it would be strange to represent a  group of race teams and represent the team Ferrari (which historically is red) with other color than red. In order to map the colors to each partition we can provide a map of entries of type <code>partitionLabel: color</code>  to the parameter <code>colorMap</code>:</p> color mapping<pre><code>// Colors matching the labels\ndef COLORS = [\n    \"Red Bull\": \"#101864\",\n    \"Ferrari\": \"#b03641\",\n    \"Mclaren\": \"#d26f30\",\n    \"Mercedes\": \"#505c62\"\n]\n\ndef plot = Underdog.plots()\n    .pie(\n        // Labels\n        [\"Red Bull\", \"Ferrari\", \"Mclaren\", \"Mercedes\"],\n        // Values\n        [9,5,6,4],\n        // Passing color mappings\n        colorMap: COLORS,\n        title: \"Top 4 Teams F1(tm) 2024 season\",\n        subtitle: \"Total number of driver victories per team\"\n    )\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/#dataframe","title":"Dataframe","text":"<p>In order to use an Underdog's dataframe we have to make sure that the name of the series should match  the names: <code>names</code>, <code>values</code>, <code>colors</code>. </p> dataframe<pre><code>// source map\ndef df = [\n    names: ('A'..'D'),\n    values: (10..40).by(10),\n    colors: ['red', 'pink', 'yellow', 'lightblue']\n].toDataFrame(\"dataframe\")\n\n// passing dataframe to pie plot and show it\ndef plot = Underdog.plots().pie(df)\n\nplot.show()\n</code></pre> <p> </p> <p>You can also use Underdog's Series following the same rules:</p> series<pre><code>// given a dataframe\ndef df = [\n        A: ('D'..'G'),\n        B: (110..140).by(10),\n        C: ['orange', 'gray', 'lightgray', 'blue']\n].toDataFrame(\"dataframe\")\n\n// we can pass series\ndef plot = Underdog\n    .plots()\n    .pie(\n        // using series \"A\" and renaming it to \"names\"\n        df['A'].rename(\"names\"),\n        // using series \"B\" and renaming it to \"values\"\n        df['B'].rename(\"values\"),\n        // using series \"C\" and renaming it to \"colors\"\n        df['C'].rename(\"colors\")\n    )\n\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/#radar","title":"Radar","text":"<p>According to Wikipedia A radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point</p>"},{"location":"plots/#simple_5","title":"Simple","text":"simple<pre><code>def plot = Underdog\n    .plots()\n    .radar(\n        [\"power\", \"consumption\", \"price\"], // Name of the categories\n        [200, 10, 100000],                 // Maximum values for each category\n        [150, 5, 54_350]                   // Actual value for each category\n    )\nplot.show()\n</code></pre>"},{"location":"plots/#extensions","title":"Extensions","text":""},{"location":"plots/#dataframe_1","title":"Dataframe","text":"dataframe extensions<pre><code>Underdog.df()\n    // Dataframe created\n    .from(X: 10..&lt;20, Y: [1, 3, 9, 3, 19, 10, 11, 4, 14, 20], \"dataframe name\")\n    // Plots extensions for dataframe add plots methods such as `scatter()`\n    .scatter()\n    .show()\n</code></pre>"},{"location":"plots/#graphs_1","title":"Graphs","text":"<p>TODO</p>"},{"location":"plots/bar/","title":"Bar","text":""},{"location":"plots/bar/#bar","title":"Bar","text":""},{"location":"plots/bar/#simple","title":"Simple","text":"simple bar<pre><code>def plot = Underdog.plots()\n    .bar(\n        1..12,\n        [10, 12, 18, 3, 0, 20, 10, 12, 18, 3, 0, 10],\n        title: \"Using bars\",\n        xLabel: \"Months\",\n        yLabel: \"Indicator\"\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/bar/#histogram","title":"Histogram","text":"simple histogram<pre><code>// generating data\ndef random = new Random()\ndef distribution = (0..1_000).collect { random.nextGaussian(0, 50) }\n\n// plot\ndef plot = Underdog\n    .plots()\n    .histogram(\n        distribution,\n        title: \"Distribution\",\n        subtitle: \"mean: 0 / stddev: 50\",\n        bins: 10\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/extensions/","title":"Extensions","text":""},{"location":"plots/extensions/#extensions","title":"Extensions","text":""},{"location":"plots/extensions/#dataframe","title":"Dataframe","text":"dataframe extensions<pre><code>Underdog.df()\n    // Dataframe created\n    .from(X: 10..&lt;20, Y: [1, 3, 9, 3, 19, 10, 11, 4, 14, 20], \"dataframe name\")\n    // Plots extensions for dataframe add plots methods such as `scatter()`\n    .scatter()\n    .show()\n</code></pre>"},{"location":"plots/extensions/#graphs","title":"Graphs","text":"<p>TODO</p>"},{"location":"plots/graphs/","title":"Graphs","text":""},{"location":"plots/graphs/#graphs","title":"Graphs","text":""},{"location":"plots/graphs/#simple","title":"Simple","text":"<p>Here's a simple graph representation just showing 2 edges connecting 3 vertices. We only have to pass an instance of a graph to the <code>graph(...)</code> function:</p> <p> </p> simple graph<pre><code>// create instance of Graph\ndef friends = Graphs.graph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\n// show plot\ndef plot = Underdog.plots().graph(friends)\n\nplot.show()\n</code></pre>"},{"location":"plots/graphs/#directed-graph","title":"Directed graph","text":"<p>If the graph we're passing to the <code>plots().graph(...)</code> function is a directed graph, the direction of the edges will show up:</p> <p> </p> directed graph<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Underdog.plots().graph(friends)\nplot.show()\n</code></pre>"},{"location":"plots/graphs/#edge-labels","title":"Edge labels","text":"<p>If we want to show the edges labels we can do so by setting the <code>showEdgeLabel</code> parameter to <code>true</code>:</p> <p> </p> edge labels<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Plots.plots().graph(friends, showEdgeLabel: true)\nplot.show()\n</code></pre>"},{"location":"plots/graphs/#showing-paths","title":"Showing paths","text":"<p>Sometimes we may want highlight a given path between vertices. We can use the parameter <code>paths</code> which receives a list of paths to highlight any number of paths:</p> <p> </p> show paths<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef friendship = friends.shortestPath('Robert', 'Troy')\n\ndef plot = Underdog.plots().graph(\n    friends,\n    paths: [friendship],\n    showEdgeLabel: true)\n\nplot.show()\n</code></pre>"},{"location":"plots/graphs/#graph-domain","title":"Graph domain","text":"<p>To go one step further and play with vertices sizes and colors, we can use the graph chart domain classes. These classes are:</p> <ul> <li><code>underdog.plots.charts.Graph.Node</code>: represents a node graphically (size, color, label)</li> <li><code>underdog.plots.charts.Graph.Edge</code>: represents an edge graphically (width, color, label)</li> </ul> <p> </p> domain classes<pre><code>List&lt;Graph.Node&gt; nodes = [\n    new Graph.Node(id: \"robert\", name: \"Robert\", symbolSize: 75),\n    new Graph.Node(id: \"thelma\", name: \"Thelma\", symbolSize: 40),\n    new Graph.Node(id: \"troy\", name: \"Troy\", symbolSize: 40)\n]\n\nList&lt;Graph.Edge&gt; edges = [\n    new Graph.Edge(\n        source: \"robert\",\n        target: \"thelma\",\n        color: \"green\",\n        width: 2,\n        value: \"bff\"),\n    new Graph.Edge(\n        source: \"robert\",\n        target: \"troy\",\n        color: \"red\",\n        width: 10,\n        value: \"friend\")\n]\n\ndef plot = Underdog.plots().graph(\n    nodes,\n    edges,\n    showEdgeLabel: true)\n\nplot.show()\n</code></pre>"},{"location":"plots/graphs/#customize","title":"Customize","text":"<p>As any of the charts in Underdog, we can use the <code>customize(...)</code> method of any chart to customize the chart by using the Groovy Echarts DSL:</p> customization<pre><code>def friends = Graphs.digraph(String) {\n    edge('Robert', 'Thelma', relation: 'friend')\n    edge('Robert', 'Troy', relation: 'friend')\n}\n\ndef plot = Underdog.plots()\n    .graph(friends, showEdgeLabel: true)\n    .customize {\n        title {\n            text \"New title\"\n            subtext(\"New subtitle\")\n            top(\"bottom\")\n            left(\"right\")\n        }\n    }\n\nplot.show()\n</code></pre>"},{"location":"plots/introduction/","title":"Introduction","text":""},{"location":"plots/introduction/#introduction","title":"Introduction","text":""},{"location":"plots/introduction/#echarts-dsl","title":"Echarts DSL","text":"<p>Underdog plots uses the Apache Echarts under the hood, so the idea is to try to be able to render whatever is possible in Echarts. To accomplish that this project creates a Groovy DSL mimicking the Echarts Option object. You can access the DSL when customizing the chart.</p> <p>At the moment the support of the Echarts Option object is limited but we aim to improve that overtime.</p>"},{"location":"plots/introduction/#basic-properties","title":"Basic properties","text":"<p>For every chart we must provide methods containing the following properties:</p> <ul> <li>data entry as list of numbers</li> <li>data entry as Series</li> <li>chart title</li> <li>chart subtitle</li> </ul> <p>Apart from that every plot has a <code>customize(Closure)</code> method to be able to customize the chart following Echarts documentation using a Groovy DSL.</p> <p>When the method is receiving data as List instances:</p> <ul> <li>X coordinate label (by default is X)</li> <li>Y coordinate label (by default is Y)</li> </ul> <p>In methods receiving Series objects the name of the X and Y coordinate will be taken from the Series' name passed as parameter.</p> <p>In general we'd like to be able to use the plots library for anyone, that's why any chart should be able to receive its data from a plain java list.</p> <p>Apart from those mandatory properties, there could be extra properties added to specific charts depending on how practical these properties are when dealing which that type of charts.</p>"},{"location":"plots/introduction/#return-options","title":"Return Options","text":"<p>All plotting methods return a memento.plots.charts.Options instance which represents the Echarts Options object.</p>"},{"location":"plots/introduction/#customizing-chart","title":"Customizing chart","text":"<p>All default methods provide a limited setup of the chart via the mandatory attributes we saw previously.To access the full Groovy Echarts DSL we can always access the Options#customize(Closure) method rendering the chart calling the Options#show().</p>"},{"location":"plots/line/","title":"Line","text":""},{"location":"plots/line/#line","title":"Line","text":""},{"location":"plots/line/#simple","title":"Simple","text":"<p>Here is a simple line chart:</p> simple line<pre><code>def line = Plots.plots()\n    .line(\n        // You can use a **range or a list** for X axis\n        2000..2010,\n        // You can use a **range or a list** for the Y axis\n        [10, 15, 18, 3, 5, 9, 10, 11, 12, 10],\n        // Optional attributes\n        title: \"Wins of Team A\",\n        subtitle: \"Between years 2000 - 2010\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    )\n\nline.show()\n</code></pre> <p> </p> <p>There are methods adapted for using Underdog's Series objects:</p> Series<pre><code>// load data\ndef df = Underdog.df().read_csv(baseballPath)\n\n// filter &amp; aggregate &amp; sort\ndf = df[df['Team'] == 'BOS']\n    .agg(W: 'sum')\n    .by('year')\n    .sort_values(by: 'year')\n\n// show\ndef plot = Underdog.plots()\n    .line(\n        // using `year` series for X axis\n        df['year'],\n        // renaming series to `Wins X Years` and using it for Y axis\n        df['Sum [W]'].rename('Wins X Years'),\n        title: \"Wins of 'BOS' team over time\")\n\nplot.show()\n</code></pre> <p></p> <p>In any method using Underdog's Series there is no attribute for changing the xLabel or the yLabel as it takes the Series' name. To change the label you can rename the Series' name as mentioned in the example.</p>"},{"location":"plots/line/#n-lines","title":"N-lines","text":"lines using collections<pre><code>Map&lt;String, List&lt;Number&gt;&gt; data = [\n    // A list of lists of 2 elements [[x1, y1], [x2, y2],..., [xn, yn]]\n    A: [[2000, 13],[2001, 5], [2002, 7], [2003, 10], [2004,6]],\n    B: [[2000, 5], [2001, 6], [2002, 7], [2003, 8], [2004, 9]],\n    // Using [listX, listY]transpose()` == [[x1, y1], [x2, y2],..., [xn, yn]]\n    C: [2000..2004, 3..7].transpose()\n]\n\ndef plot = Underdog.plots()\n    .lines(\n        data,\n        title: \"Progress of Teams A, B, C\",\n        subtitle: \"Between years 2000 - 2010\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    )\n\nplot.show()\n</code></pre>"},{"location":"plots/line/#customize","title":"Customize","text":"<p>As in any chart once we've created our Options object and before calling <code>show()</code> we can use the <code>Options#customize()</code> method to customize the chart using Groovy's Echart DSL.</p> Customize<pre><code>def plot = Underdog.plots()\n    .lines(\n        dataFrame,\n        title: \"Team comparison (BOS, ATL, CIN)\",\n        subtitle: \"Years 2000-2004\",\n        xLabel: \"Years\",\n        yLabel: \"Wins\"\n    ).customize {\n        // Adding legend in the top right corner\n        legend {\n            top(\"10%\")\n            right('15%')\n            show(true)\n        }\n        // Adding tooltip of type `axis`\n        tooltip {\n            trigger('axis')\n        }\n    }\n\nplot.show()\n</code></pre> <p>In this occasion we are adding the chart legend and positioning it to the top-right side of the chart.</p> <p> </p>"},{"location":"plots/pie/","title":"Pie","text":""},{"location":"plots/pie/#pie","title":"Pie","text":"<p>Wikipedia</p> <p>According to Wikipedia a pie chart (or a circle chart) is a circular statistical graphic which is divided into slices to illustrate numerical proportion.</p> <p>In a pie chart, the arc length of each slice (and consequently its central angle and area) is proportional to the quantity it represents.</p>"},{"location":"plots/pie/#simple","title":"Simple","text":"<p>To create a minimal representation of a Pie we must provide at least a collection of the labels of each partition, and another collection with the values of each partition:</p> building pie<pre><code>def plot = Underdog\n    .plots()\n    .pie(\n\n        ('A'..'D'), // slice labels\n        [9,5,6,4]   // slice values\n    )\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/pie/#color-mapping","title":"Color mapping","text":"<p>In some situations the color of each partition is really meaningful. For example, it would be strange to represent a  group of race teams and represent the team Ferrari (which historically is red) with other color than red. In order to map the colors to each partition we can provide a map of entries of type <code>partitionLabel: color</code>  to the parameter <code>colorMap</code>:</p> color mapping<pre><code>// Colors matching the labels\ndef COLORS = [\n    \"Red Bull\": \"#101864\",\n    \"Ferrari\": \"#b03641\",\n    \"Mclaren\": \"#d26f30\",\n    \"Mercedes\": \"#505c62\"\n]\n\ndef plot = Underdog.plots()\n    .pie(\n        // Labels\n        [\"Red Bull\", \"Ferrari\", \"Mclaren\", \"Mercedes\"],\n        // Values\n        [9,5,6,4],\n        // Passing color mappings\n        colorMap: COLORS,\n        title: \"Top 4 Teams F1(tm) 2024 season\",\n        subtitle: \"Total number of driver victories per team\"\n    )\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/pie/#dataframe","title":"Dataframe","text":"<p>In order to use an Underdog's dataframe we have to make sure that the name of the series should match  the names: <code>names</code>, <code>values</code>, <code>colors</code>. </p> dataframe<pre><code>// source map\ndef df = [\n    names: ('A'..'D'),\n    values: (10..40).by(10),\n    colors: ['red', 'pink', 'yellow', 'lightblue']\n].toDataFrame(\"dataframe\")\n\n// passing dataframe to pie plot and show it\ndef plot = Underdog.plots().pie(df)\n\nplot.show()\n</code></pre> <p> </p> <p>You can also use Underdog's Series following the same rules:</p> series<pre><code>// given a dataframe\ndef df = [\n        A: ('D'..'G'),\n        B: (110..140).by(10),\n        C: ['orange', 'gray', 'lightgray', 'blue']\n].toDataFrame(\"dataframe\")\n\n// we can pass series\ndef plot = Underdog\n    .plots()\n    .pie(\n        // using series \"A\" and renaming it to \"names\"\n        df['A'].rename(\"names\"),\n        // using series \"B\" and renaming it to \"values\"\n        df['B'].rename(\"values\"),\n        // using series \"C\" and renaming it to \"colors\"\n        df['C'].rename(\"colors\")\n    )\n\nplot.show()\n</code></pre> <p> </p>"},{"location":"plots/radar/","title":"Radar","text":""},{"location":"plots/radar/#radar","title":"Radar","text":"<p>According to Wikipedia A radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point</p>"},{"location":"plots/radar/#simple","title":"Simple","text":"simple<pre><code>def plot = Underdog\n    .plots()\n    .radar(\n        [\"power\", \"consumption\", \"price\"], // Name of the categories\n        [200, 10, 100000],                 // Maximum values for each category\n        [150, 5, 54_350]                   // Actual value for each category\n    )\nplot.show()\n</code></pre>"},{"location":"plots/scatter/","title":"Scatter","text":""},{"location":"plots/scatter/#scatter","title":"Scatter","text":""},{"location":"plots/scatter/#simple","title":"Simple","text":"simple<pre><code>// numbers from 0 to 99\n// You can use a \"range or a list\" for X axis\ndef xs = 0..&lt;100\n\n// 100 random numbers\n// You can use a \"range or a list\" for the Y axis\ndef ys = (0..&lt;100).collect { new Random().nextInt(100) }\n\n// plot\ndef plot = Underdog.plots()\n    .scatter(\n        xs,\n        ys,\n        title: \"Random Numbers\") // Optional attributes\n\nplot.show()\n</code></pre> <p>Here's the same example but using Underdog's series for X and Y axes. Given the dataframe instance <code>df</code> and series <code>xs</code> and <code>ys</code>:</p> simple series<pre><code>Plots.plots()\n        .scatter(\n            // using a series for x axis and renaming it to X\n            df['xs'].rename('X'),\n            // using another series for y axis and renaming it to Y\n            df['ys'].rename('Y'),\n            title: \"Random Numbers\")\n        .show()\n</code></pre>"},{"location":"ta/","title":"Technical Analysis","text":"<p>Underdog's TA module provides a series of tools for technical analysis. It's built on top of  the TA4j java library adding a Groovy DSL and some extension methods to make  certain operations easier.</p>"},{"location":"ta/#tutorial","title":"Tutorial","text":"<p>Info</p> <p>This getting started section mimics the getting started section steps from Ta4j wiki but using underdog-ta module. You can compare both entries to see the differences.</p>"},{"location":"ta/#prerequisites","title":"Prerequisites","text":""},{"location":"ta/#dependencies","title":"Dependencies","text":"<p>To be able to follow the tutorial you should add the following modules to your gradle project:</p> gradle<pre><code>implementation 'com.github.grooviter:underdog-ta:VERSION'\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <p>or in maven:</p> maven<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-ta&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"ta/#data","title":"Data","text":"<p>TODO</p>"},{"location":"ta/#barseries-vs-dataframe","title":"BarSeries vs DataFrame","text":"<p>It's important to start defining some concepts.</p> <ul> <li> <p>BarSeries: Ta4j's BarSeries is like a dataframe containing series (columns) such as open price, lower price, close price, and volume data.</p> </li> <li> <p>DataFrame: Underdog's dataframe which is composed of different Series (columns), Each Series or column is like an array of objects.</p> </li> </ul> <p>As this module integrates Ta4j with Underdog there will be methods which converts from BarSeries to DataFrame and vice versa.</p>"},{"location":"ta/#loading-data","title":"Loading data","text":"<p>In this example we are using Underdog to load stock quotes from a csv file and create a DataFrame:</p> stock quotes from csv<pre><code>// The file path where the csv file can be found\ndef filePath = \"src/test/resources/data/ta/stock_quotes_10_years.csv\"\n\n// The format of the dates included in the file\ndef dateFormat = \"yyyy-MM-dd HH:mm:ss+00:00\"\n\ndef quotes = Underdog.df().read_csv(filePath, dateFormat: dateFormat)\n</code></pre> <p>Warning</p> <p>If dates are not treated as dates the csv reader will consider them as strings. That will cause you problems when converting an Underdog's DataFrame to Ta4j's BarSeries.</p> <p>Which outputs something like the following (the prices and volume are truncated here to make it look good).</p> output<pre><code>                                stock_quotes_10_years.csv\n           Date             |  Adj Close  |  Close |  High  |  Low  |  Open  |   Volume |\n-----------------------------------------------------------------------------------------\n 2014-12-05 00:00:00+00:00  |   0.50      |  0.52  |  0.52  |  0.52 |  0.52  |  165680  |\n</code></pre> <p>In order to successfully convert the dataframe to a bar series the name of the series (columns) should match to the expected ones which are: DATE, CLOSE, HIGH, LOW, OPEN, VOLUME.</p> renaming<pre><code>quotes = quotes\n    .drop(\"Adj Close\") // Removing the \"Adj Close\" series\n    .renameSeries(fn: String::toUpperCase) // Renaming all remaining columns to upper case to match\n</code></pre> output<pre><code>                          stock_quotes_10_years.csv\n           DATE             |  CLOSE |  HIGH  |  LOW  |  OPEN  |  VOLUME  |\n---------------------------------------------------------------------------\n 2014-12-05 00:00:00+00:00  |  0.52  |  0.52  |  0.52 |  0.52  |  165680  |\n</code></pre>"},{"location":"ta/#dataframe-to-barseries","title":"DataFrame to BarSeries","text":"<p>Because we need to convert the dataframe to a BarSeries in order to create technical analysis rules:</p> to bar series<pre><code>def quotesBarSeries = quotes.toBarSeries()\n</code></pre> <p>Now we can operate with the bar series.</p>"},{"location":"ta/#indicators","title":"Indicators","text":"<p>Now we can start creating some indicators and metrics. This time we are getting metrics based on the closing price indicator:</p> indicators<pre><code>// Using the close price indicator as root indicator...\ndef closePrice = quotesBarSeries.closePriceIndicator\n// Getting the simple moving average (SMA) of the close price over the last 5 bars\nSMAIndicator shortSma = closePrice.sma(5)\n\n// Here is the 5-bars-SMA value at the 42nd index\nprintln(\"5-bars-SMA value at the 42nd index: \" + shortSma.getValue(42).doubleValue())\n\n// Getting a longer SMA (e.g. over the 30 last bars)\nSMAIndicator longSma = closePrice.sma(30)\n</code></pre> output<pre><code>5-bars-SMA value at the 42nd index: 0.503899997472763\n</code></pre>"},{"location":"ta/#building-a-trading-strategy","title":"Building a trading strategy","text":"<p>Now that we've got a couple of indicators ready lets build a strategy. Strategies are made of two trading rules: one for entry (buying), the other for exit (selling).</p> strategy base<pre><code>// Buying rules\n// We want to buy:\n//  - if the 5-bars SMA crosses over 30-bars SMA\n//  - or if the price goes below a defined price (e.g $800.00)\nRule buyingRule = shortSma.xUp(longSma)\n    .or(closePrice.xDown(120))\n\n// Selling rules\n// We want to sell:\n//  - if the 5-bars SMA crosses under 30-bars SMA\n//  - or if the price loses more than 3%\n//  - or if the price earns more than 2%\nRule sellingRule = shortSma.xDown(longSma)\n    .or(closePrice.stopLoss(3))\n    .or(closePrice.stopGain(2))\n\n// Create the strategy\nStrategy strategy = new BaseStrategy(buyingRule, sellingRule)\n</code></pre> <p>We can use Groovy's syntax to refactor the rules a little bit:</p> strategy using operators<pre><code>buyingRule = shortSma.xUp(longSma)\n    | closePrice.xDown(120)\n\nsellingRule = shortSma.xDown(longSma)\n    | closePrice.stopLoss(3)\n    | closePrice.stopGain(2)\n</code></pre> <p>In Groovy | and &amp; represent calls to methods or and and of any object (any object having those methods implemented). So if your object has these methods, you can substitute your method call by the operators.</p>"},{"location":"ta/#backtesting","title":"Backtesting","text":"<p>What is backtesting ? According to Investopedia:</p> <p>\"Backtesting is the general method for seeing how well a strategy or model would have done after the fact. It assesses the viability of a trading strategy by discovering how it would play out using historical data. If backtesting works, traders and analysts may have the confidence to employ it going forward\" -- Investopedia, https://www.investopedia.com/terms/b/backtesting.asp</p> <p>The backtest step is pretty simple:</p> backtesting<pre><code>// Running our juicy trading strategy...\nBarSeriesManager seriesManager = new BarSeriesManager(quotesBarSeries)\nTradingRecord tradingRecord = seriesManager.run(strategy)\nprintln(\"Number of positions (trades) for our strategy: \" + tradingRecord.getPositionCount())\n</code></pre> output<pre><code>Number of positions (trades) for our strategy: 57\n</code></pre> <p>For many scenarios we can run a base strategy backtesting by just executing the run method in the BarSeries object and pass directly the entry (buying) rule and the exit (selling) rule:</p> backtesting<pre><code>tradingRecord = quotesBarSeries.run(buyingRule, sellingRule)\n</code></pre> <p>We can see this visually. Follow-up showing only trades from 2024-04-01:</p> plotting trades<pre><code>// getting only stocks from 2024-04-01\nquotes = quotes[quotes['DATE'] &gt;= LocalDate.parse('2024-04-01')]\n\n// getting Underdog's series for x and y coordinates\ndef xs = quotes['DATE'](LocalDate, String) { it.format(\"dd/MM/yyyy\") }\ndef ys = quotes['CLOSE']\n\n// building a line plot\ndef plot = Underdog.plots()\n    .line(\n        xs.rename(\"Dates\"),\n        ys.rename(\"Closing Price\"),\n        title: \"Trades from 2024-01-01\",\n        subtitle: \"Using Underdog's TA and Ta4j\")\n\n// showing trades over stock quotes\ntradingRecord.trades.each {trade -&gt;\n    String x = quotesBarSeries.getBar(trade.index).endTime.format('dd/MM/yyyy')\n    double y = trade.value.doubleValue()\n    String t = trade.type.name()[0] // first letter of type name ('B' for buy, 'S' for sell)\n    plot.addAnnotation(x, y, text: t, color: t == 'B' ? 'green' : '#dd7474')\n}\n\n// showing the plot\nplot.show()\n</code></pre> <p> </p> <p>We can also visualize winning vs losing positions.</p> winning vs losing<pre><code>// creating a function to map every position to a map containing date and profit\ndef positionToDataFrameEntry = { Position pos -&gt;\n    return [\n        date: quotesBarSeries.getBar(pos.exit.index).endTime.toLocalDate(),\n        profit: pos.grossProfit.doubleValue()\n    ]\n}\n\n// getting gross profit and date of every position\ndef wins = tradingRecord\n    .positions\n    .collect(positionToDataFrameEntry)\n    .toDataFrame(\"trade values\")\n\n// mark every position as winners/losers\nwins['winner'] = wins['profit'](Double, String) { it &gt; 0 ? \"winners\" : \"losers\" }\n\n// grouping by winners/losers and get the count\ndef byMonth = wins\n    .agg(profit: 'count')\n    .by('winner')\n    .renameSeries(mapper: [\"count [profit]\": \"value\", winner: \"name\"])\n\n// getting the min/max date for the subtitle of the plot\ndef minDate = wins['date'].min(LocalDate).format(\"dd/MM/yyyy\")\ndef maxDate = wins['date'].max(LocalDate).format(\"dd/MM/yyyy\")\n\n// building the plot\ndef piePlot = Underdog\n    .plots()\n    .pie(\n        byMonth['name'].toList(),\n        byMonth['value'].toList(),\n        title: \"Winners vs Losers positions\",\n        subtitle: \"From ${minDate} to ${maxDate}\"\n    )\n\n// showing the plot\npiePlot.show()\n</code></pre> <p> </p>"},{"location":"ta/#analyzing-our-results","title":"Analyzing our results","text":"<p>Here is how we can analyze the results of our backtest:</p> analysis<pre><code>// Getting the winning positions ratio\nAnalysisCriterion winningPositionsRatio = new PositionsRatioCriterion(AnalysisCriterion.PositionFilter.PROFIT)\ndouble winningPositionRatioValue = winningPositionsRatio.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Winning positions ratio: \" + winningPositionRatioValue)\n\n// Getting a risk-reward ratio\nAnalysisCriterion romad = new ReturnOverMaxDrawdownCriterion()\ndouble nomadValue = romad.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Return over Max Drawdown: \" + nomadValue)\n\n// Total return of our strategy vs total return of a buy-and-hold strategy\nAnalysisCriterion vsBuyAndHold = new VersusEnterAndHoldCriterion(new ReturnCriterion())\ndouble vsBuyAndHoldValue = vsBuyAndHold.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Our return vs buy-and-hold return: \" + vsBuyAndHoldValue)\n</code></pre> output<pre><code>Winning positions ratio: 0.54385964912280701754385964912281\nReturn over Max Drawdown: 3.4519153297405649777237484258582\nOur return vs buy-and-hold return: 0.0040904249296843023287166775087440\n</code></pre> <p>Showing these metrics in a chart:</p> radar<pre><code>def winningRatioPlot = Underdog.plots()\n    .radar(\n        // names of metrics\n        ['winning ratio', 'return over drawdown', 'return vs buy-and-hold'],\n        // maximum possible value of each metric\n        [1, 100, 1],\n        // values from metrics\n        [winningPositionRatioValue, nomadValue, vsBuyAndHoldValue],\n        title: \"Metrics comparison\",\n        subtitle: \"Winning Ratio / Risk Reward Ratio / Return vs Buy-And-Hold\"\n    )\nwinningRatioPlot.show()\n</code></pre> <p>Which displays:</p> <p> </p>"},{"location":"ta/tutorial/","title":"Tutorial","text":""},{"location":"ta/tutorial/#tutorial","title":"Tutorial","text":"<p>Info</p> <p>This getting started section mimics the getting started section steps from Ta4j wiki but using underdog-ta module. You can compare both entries to see the differences.</p>"},{"location":"ta/tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"ta/tutorial/#dependencies","title":"Dependencies","text":"<p>To be able to follow the tutorial you should add the following modules to your gradle project:</p> gradle<pre><code>implementation 'com.github.grooviter:underdog-ta:VERSION'\nimplementation 'com.github.grooviter:underdog-plots:VERSION'\n</code></pre> <p>or in maven:</p> maven<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-ta&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.github.grooviter&lt;/groupId&gt;\n    &lt;artifactId&gt;underdog-plots&lt;/artifactId&gt;\n    &lt;version&gt;VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"ta/tutorial/#data","title":"Data","text":"<p>TODO</p>"},{"location":"ta/tutorial/#barseries-vs-dataframe","title":"BarSeries vs DataFrame","text":"<p>It's important to start defining some concepts.</p> <ul> <li> <p>BarSeries: Ta4j's BarSeries is like a dataframe containing series (columns) such as open price, lower price, close price, and volume data.</p> </li> <li> <p>DataFrame: Underdog's dataframe which is composed of different Series (columns), Each Series or column is like an array of objects.</p> </li> </ul> <p>As this module integrates Ta4j with Underdog there will be methods which converts from BarSeries to DataFrame and vice versa.</p>"},{"location":"ta/tutorial/#loading-data","title":"Loading data","text":"<p>In this example we are using Underdog to load stock quotes from a csv file and create a DataFrame:</p> stock quotes from csv<pre><code>// The file path where the csv file can be found\ndef filePath = \"src/test/resources/data/ta/stock_quotes_10_years.csv\"\n\n// The format of the dates included in the file\ndef dateFormat = \"yyyy-MM-dd HH:mm:ss+00:00\"\n\ndef quotes = Underdog.df().read_csv(filePath, dateFormat: dateFormat)\n</code></pre> <p>Warning</p> <p>If dates are not treated as dates the csv reader will consider them as strings. That will cause you problems when converting an Underdog's DataFrame to Ta4j's BarSeries.</p> <p>Which outputs something like the following (the prices and volume are truncated here to make it look good).</p> output<pre><code>                                stock_quotes_10_years.csv\n           Date             |  Adj Close  |  Close |  High  |  Low  |  Open  |   Volume |\n-----------------------------------------------------------------------------------------\n 2014-12-05 00:00:00+00:00  |   0.50      |  0.52  |  0.52  |  0.52 |  0.52  |  165680  |\n</code></pre> <p>In order to successfully convert the dataframe to a bar series the name of the series (columns) should match to the expected ones which are: DATE, CLOSE, HIGH, LOW, OPEN, VOLUME.</p> renaming<pre><code>quotes = quotes\n    .drop(\"Adj Close\") // Removing the \"Adj Close\" series\n    .renameSeries(fn: String::toUpperCase) // Renaming all remaining columns to upper case to match\n</code></pre> output<pre><code>                          stock_quotes_10_years.csv\n           DATE             |  CLOSE |  HIGH  |  LOW  |  OPEN  |  VOLUME  |\n---------------------------------------------------------------------------\n 2014-12-05 00:00:00+00:00  |  0.52  |  0.52  |  0.52 |  0.52  |  165680  |\n</code></pre>"},{"location":"ta/tutorial/#dataframe-to-barseries","title":"DataFrame to BarSeries","text":"<p>Because we need to convert the dataframe to a BarSeries in order to create technical analysis rules:</p> to bar series<pre><code>def quotesBarSeries = quotes.toBarSeries()\n</code></pre> <p>Now we can operate with the bar series.</p>"},{"location":"ta/tutorial/#indicators","title":"Indicators","text":"<p>Now we can start creating some indicators and metrics. This time we are getting metrics based on the closing price indicator:</p> indicators<pre><code>// Using the close price indicator as root indicator...\ndef closePrice = quotesBarSeries.closePriceIndicator\n// Getting the simple moving average (SMA) of the close price over the last 5 bars\nSMAIndicator shortSma = closePrice.sma(5)\n\n// Here is the 5-bars-SMA value at the 42nd index\nprintln(\"5-bars-SMA value at the 42nd index: \" + shortSma.getValue(42).doubleValue())\n\n// Getting a longer SMA (e.g. over the 30 last bars)\nSMAIndicator longSma = closePrice.sma(30)\n</code></pre> output<pre><code>5-bars-SMA value at the 42nd index: 0.503899997472763\n</code></pre>"},{"location":"ta/tutorial/#building-a-trading-strategy","title":"Building a trading strategy","text":"<p>Now that we've got a couple of indicators ready lets build a strategy. Strategies are made of two trading rules: one for entry (buying), the other for exit (selling).</p> strategy base<pre><code>// Buying rules\n// We want to buy:\n//  - if the 5-bars SMA crosses over 30-bars SMA\n//  - or if the price goes below a defined price (e.g $800.00)\nRule buyingRule = shortSma.xUp(longSma)\n    .or(closePrice.xDown(120))\n\n// Selling rules\n// We want to sell:\n//  - if the 5-bars SMA crosses under 30-bars SMA\n//  - or if the price loses more than 3%\n//  - or if the price earns more than 2%\nRule sellingRule = shortSma.xDown(longSma)\n    .or(closePrice.stopLoss(3))\n    .or(closePrice.stopGain(2))\n\n// Create the strategy\nStrategy strategy = new BaseStrategy(buyingRule, sellingRule)\n</code></pre> <p>We can use Groovy's syntax to refactor the rules a little bit:</p> strategy using operators<pre><code>buyingRule = shortSma.xUp(longSma)\n    | closePrice.xDown(120)\n\nsellingRule = shortSma.xDown(longSma)\n    | closePrice.stopLoss(3)\n    | closePrice.stopGain(2)\n</code></pre> <p>In Groovy | and &amp; represent calls to methods or and and of any object (any object having those methods implemented). So if your object has these methods, you can substitute your method call by the operators.</p>"},{"location":"ta/tutorial/#backtesting","title":"Backtesting","text":"<p>What is backtesting ? According to Investopedia:</p> <p>\"Backtesting is the general method for seeing how well a strategy or model would have done after the fact. It assesses the viability of a trading strategy by discovering how it would play out using historical data. If backtesting works, traders and analysts may have the confidence to employ it going forward\" -- Investopedia, https://www.investopedia.com/terms/b/backtesting.asp</p> <p>The backtest step is pretty simple:</p> backtesting<pre><code>// Running our juicy trading strategy...\nBarSeriesManager seriesManager = new BarSeriesManager(quotesBarSeries)\nTradingRecord tradingRecord = seriesManager.run(strategy)\nprintln(\"Number of positions (trades) for our strategy: \" + tradingRecord.getPositionCount())\n</code></pre> output<pre><code>Number of positions (trades) for our strategy: 57\n</code></pre> <p>For many scenarios we can run a base strategy backtesting by just executing the run method in the BarSeries object and pass directly the entry (buying) rule and the exit (selling) rule:</p> backtesting<pre><code>tradingRecord = quotesBarSeries.run(buyingRule, sellingRule)\n</code></pre> <p>We can see this visually. Follow-up showing only trades from 2024-04-01:</p> plotting trades<pre><code>// getting only stocks from 2024-04-01\nquotes = quotes[quotes['DATE'] &gt;= LocalDate.parse('2024-04-01')]\n\n// getting Underdog's series for x and y coordinates\ndef xs = quotes['DATE'](LocalDate, String) { it.format(\"dd/MM/yyyy\") }\ndef ys = quotes['CLOSE']\n\n// building a line plot\ndef plot = Underdog.plots()\n    .line(\n        xs.rename(\"Dates\"),\n        ys.rename(\"Closing Price\"),\n        title: \"Trades from 2024-01-01\",\n        subtitle: \"Using Underdog's TA and Ta4j\")\n\n// showing trades over stock quotes\ntradingRecord.trades.each {trade -&gt;\n    String x = quotesBarSeries.getBar(trade.index).endTime.format('dd/MM/yyyy')\n    double y = trade.value.doubleValue()\n    String t = trade.type.name()[0] // first letter of type name ('B' for buy, 'S' for sell)\n    plot.addAnnotation(x, y, text: t, color: t == 'B' ? 'green' : '#dd7474')\n}\n\n// showing the plot\nplot.show()\n</code></pre> <p> </p> <p>We can also visualize winning vs losing positions.</p> winning vs losing<pre><code>// creating a function to map every position to a map containing date and profit\ndef positionToDataFrameEntry = { Position pos -&gt;\n    return [\n        date: quotesBarSeries.getBar(pos.exit.index).endTime.toLocalDate(),\n        profit: pos.grossProfit.doubleValue()\n    ]\n}\n\n// getting gross profit and date of every position\ndef wins = tradingRecord\n    .positions\n    .collect(positionToDataFrameEntry)\n    .toDataFrame(\"trade values\")\n\n// mark every position as winners/losers\nwins['winner'] = wins['profit'](Double, String) { it &gt; 0 ? \"winners\" : \"losers\" }\n\n// grouping by winners/losers and get the count\ndef byMonth = wins\n    .agg(profit: 'count')\n    .by('winner')\n    .renameSeries(mapper: [\"count [profit]\": \"value\", winner: \"name\"])\n\n// getting the min/max date for the subtitle of the plot\ndef minDate = wins['date'].min(LocalDate).format(\"dd/MM/yyyy\")\ndef maxDate = wins['date'].max(LocalDate).format(\"dd/MM/yyyy\")\n\n// building the plot\ndef piePlot = Underdog\n    .plots()\n    .pie(\n        byMonth['name'].toList(),\n        byMonth['value'].toList(),\n        title: \"Winners vs Losers positions\",\n        subtitle: \"From ${minDate} to ${maxDate}\"\n    )\n\n// showing the plot\npiePlot.show()\n</code></pre> <p> </p>"},{"location":"ta/tutorial/#analyzing-our-results","title":"Analyzing our results","text":"<p>Here is how we can analyze the results of our backtest:</p> analysis<pre><code>// Getting the winning positions ratio\nAnalysisCriterion winningPositionsRatio = new PositionsRatioCriterion(AnalysisCriterion.PositionFilter.PROFIT)\ndouble winningPositionRatioValue = winningPositionsRatio.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Winning positions ratio: \" + winningPositionRatioValue)\n\n// Getting a risk-reward ratio\nAnalysisCriterion romad = new ReturnOverMaxDrawdownCriterion()\ndouble nomadValue = romad.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Return over Max Drawdown: \" + nomadValue)\n\n// Total return of our strategy vs total return of a buy-and-hold strategy\nAnalysisCriterion vsBuyAndHold = new VersusEnterAndHoldCriterion(new ReturnCriterion())\ndouble vsBuyAndHoldValue = vsBuyAndHold.calculate(quotesBarSeries, tradingRecord).doubleValue()\nprintln(\"Our return vs buy-and-hold return: \" + vsBuyAndHoldValue)\n</code></pre> output<pre><code>Winning positions ratio: 0.54385964912280701754385964912281\nReturn over Max Drawdown: 3.4519153297405649777237484258582\nOur return vs buy-and-hold return: 0.0040904249296843023287166775087440\n</code></pre> <p>Showing these metrics in a chart:</p> radar<pre><code>def winningRatioPlot = Underdog.plots()\n    .radar(\n        // names of metrics\n        ['winning ratio', 'return over drawdown', 'return vs buy-and-hold'],\n        // maximum possible value of each metric\n        [1, 100, 1],\n        // values from metrics\n        [winningPositionRatioValue, nomadValue, vsBuyAndHoldValue],\n        title: \"Metrics comparison\",\n        subtitle: \"Winning Ratio / Risk Reward Ratio / Return vs Buy-And-Hold\"\n    )\nwinningRatioPlot.show()\n</code></pre> <p>Which displays:</p> <p> </p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""}]}